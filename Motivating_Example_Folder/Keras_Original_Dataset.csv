SO Post URL,ML API Name,Level 1 (Central Contract Category),Level 2,Level 3 (Hybrid Patterns),Leaf Contract Category,Root Cause,Effect,ML Library,Contract Violation Location,Detection Technique,Reasons for not labelling,Stack Overflow Tags,Question Score,Answer Creation Date,Answer Score,acceptedResponderReputation,acceptedResponderID,Questioner Reputation,Questioner ID,weightedScoreResponder,weightedScoreQuestioner,,,Question,Answer,PostCreationDate,Answer Creation Date,QuestionComments,AnswerComments
https://stackoverflow.com/questions/47599436,keras.models.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Train,Static,0,<python><tensorflow><machine-learning><keras><neural-network>,8,43070.77014,43075.77014,8,161,9040452,161,9040452,8,8,https://stackoverflow.com/questions/47599436,"I am trying to make a simple proof-of-concept where I can see the probabilities of different classes for a given prediction.

However, everything I try seems to only output the predicted class, even though I am using a softmax activation. I am new to machine learning, so I'm not sure if I am making a simple mistake or if this is a feature not available in Keras. 

I'm using Keras + TensorFlow. I have adapted one of the basic examples given by Keras for classifying the MNIST dataset.

My code below is exactly the same as the example, except for a few (commented) extra lines that exports the model to a local file.

'''Trains a simple deep NN on the MNIST dataset.
Gets to 98.40% test accuracy after 20 epochs
(there is *a lot* of margin for parameter tuning).
2 seconds per epoch on a K520 GPU.
'''

from __future__ import print_function

import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop

import h5py # added import because it is required for model.save
model_filepath = 'test_model.h5' # added filepath config

batch_size = 128
num_classes = 10
epochs = 20

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
          optimizer=RMSprop(),
          metrics=['accuracy'])

history = model.fit(x_train, y_train,
                batch_size=batch_size,
                epochs=epochs,
                verbose=1,
                validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

model.save(model_filepath) # added saving model
print('Model saved') # added log


Then the second part of this is a simple script that should import the model, predict the class for some given data, and print out the probabilities for each class. (I am using the same mnist class included with the Keras codebase to make an example as simple as possible).

import keras
from keras.datasets import mnist
from keras.models import Sequential
import keras.backend as K

import numpy

# loading model saved locally in test_model.h5
model_filepath = 'test_model.h5'
prev_model = keras.models.load_model(model_filepath)

# these lines are copied from the example for loading MNIST data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(60000, 784)

# for this example, I am only taking the first 10 images
x_slice = x_train[slice(1, 11, 1)]

# making the prediction
prediction = prev_model.predict(x_slice)

# logging each on a separate line
for single_prediction in prediction:
    print(single_prediction)


If I run the first script to export the model, then the second script to classify some examples, I get the following output:

[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]
[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]
[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]
[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]
[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]


This is great for seeing which class each is predicted to be, but what if I want to see the relative probabilities of each class for each example? I am looking for something more like this:

[ 0.94 0.01 0.02 0. 0. 0.01 0. 0.01 0.01 0.]
[ 0. 0. 0. 0. 0.51 0. 0. 0. 0.49 0.]
...


In other words, I need to know how sure each prediction is, not just the prediction itself. I thought seeing the relative probabilities was a part of using a softmax activation in the model, but I can't seem to find anything in the Keras documentation that would give me probabilities instead of the predicted answer. Am I making some kind of silly mistake, or is this feature not available?","So it turns out that the problem was I was not fully normalizing the data in the prediction script.

My prediction script should have had the following lines:

# these lines are copied from the example for loading MNIST data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(60000, 784)
x_train = x_train.astype('float32') # this line was missing
x_train /= 255 # this line was missing too


Because the data was not cast to float, and divided by 255 (so it would be between 0 and 1), it was just showing up as 1s and 0s.",11-09-2015 13:52,11-09-2015 14:05,"What is your Keras version?
                
                
– desertnaut
                
                
                    Commented
                    Dec 1, 2017 at 21:12
                
            
        
    
    
        
            
            
        
        
            
                
                I'm using Keras version 2.0.9
                
                
– user9040452
                
                
                    Commented
                    Dec 1, 2017 at 21:42
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                cannot reproduce your issue; predict in my Keras 2.0.9 returns probabilities, as it should do
                
                
– desertnaut
                
                
                    Commented
                    Dec 1, 2017 at 21:46
                
            
        
    
    
        
            
            
        
        
            
                
                Hmmm - would you by any chance be using Theano or CNTK instead of TensorFlow? Maybe it is a bug in TensorFlow?
                
                
– user9040452
                
                
                    Commented
                    Dec 1, 2017 at 21:50
                
            
        
    
    
        
            
            
        
        
            
                
                No, TensorFlow... Maybe it is a rounding detail when you print in Python 3 (I m using Python 2)?
                
                
– desertnaut
                
                
                    Commented
                    Dec 1, 2017 at 21:52
                
            
        
    

            
	    

        
                    Add a comment
                 |","I ran into this same issue where I seemed to be getting classes instead of probabilities. I was using training generators for training which normalized my data automatically, but I was not normalizing my data when doing a custom inference call. Thanks for the help!
                
                
– dzubke
                
                
                    Commented
                    Jun 11, 2021 at 17:04
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/48082655,keras.models.model.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><tensorflow><keras><loss-function>,14,43103.74583,43103.76042,12,7147,5140223,7863,743775,1.419369319,1.5,https://stackoverflow.com/questions/48082655,"I'm trying to create a simple weighted loss function. 

Say, I have input dimensions 100 * 5, and output dimensions also 100 * 5. I also have a weight matrix of the same dimension.

Something like the following:

import numpy as np
train_X = np.random.randn(100, 5)
train_Y = np.random.randn(100, 5)*0.01 + train_X

weights = np.random.randn(*train_X.shape)


Defining the custom loss function

def custom_loss_1(y_true, y_pred):
    return K.mean(K.abs(y_true-y_pred)*weights)


Defining the model

from keras.layers import Dense, Input
from keras import Model
import keras.backend as K

input_layer = Input(shape=(5,))
out = Dense(5)(input_layer)
model = Model(input_layer, out)


Testing with existing metrics works fine

model.compile('adam','mean_absolute_error')
model.fit(train_X, train_Y, epochs=1)


Testing with our custom loss function doesn't work

model.compile('adam',custom_loss_1)
model.fit(train_X, train_Y, epochs=10)


It gives the following stack trace:

InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5]
 [[Node: loss_9/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_9/dense_8_loss/Abs, loss_9/dense_8_loss/mul/y)]]


Where is the number 32 coming from?

Testing a loss function with weights as Keras tensors

def custom_loss_2(y_true, y_pred):
    return K.mean(K.abs(y_true-y_pred)*K.ones_like(y_true))


This function seems to do the work. So, probably suggests that a Keras tensor as a weight matrix would work. So, I created another version of the loss function.

Loss function try 3

from functools import partial

def custom_loss_3(y_true, y_pred, weights):
    return K.mean(K.abs(y_true-y_pred)*K.variable(weights, dtype=y_true.dtype))

cl3 = partial(custom_loss_3, weights=weights)  


Fitting data using cl3 gives the same error as above.

InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5]
     [[Node: loss_11/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_11/dense_8_loss/Abs, loss_11/dense_8_loss/Variable/read)]]


I wonder what I'm missing! I could have used the notion of sample_weight in Keras; but then I'd have to reshape my inputs to a 3d vector. 

I thought that this custom loss function should really have been trivial.","In model.fit the batch size is 32 by default, that's where this number is coming from. Here's what's happening:


In custom_loss_1 the tensor K.abs(y_true-y_pred) has shape (batch_size=32, 5), while the numpy array weights has shape (100, 5). This is an invalid multiplication, since the dimensions don't agree and broadcasting can't be applied.
In custom_loss_2 this problem doesn't exist because you're multiplying 2 tensors with the same shape (batch_size=32, 5).
In custom_loss_3 the problem is the same as in custom_loss_1, because converting weights into a Keras variable doesn't change their shape.




UPDATE: It seems you want to give a different weight to each element in each training sample, so the weights array should have shape (100, 5) indeed.
In this case, I would input your weights' array into your model and then use this tensor within the loss function:

import numpy as np
from keras.layers import Dense, Input
from keras import Model
import keras.backend as K
from functools import partial


def custom_loss_4(y_true, y_pred, weights):
    return K.mean(K.abs(y_true - y_pred) * weights)


train_X = np.random.randn(100, 5)
train_Y = np.random.randn(100, 5) * 0.01 + train_X
weights = np.random.randn(*train_X.shape)

input_layer = Input(shape=(5,))
weights_tensor = Input(shape=(5,))
out = Dense(5)(input_layer)
cl4 = partial(custom_loss_4, weights=weights_tensor)
model = Model([input_layer, weights_tensor], out)
model.compile('adam', cl4)
model.fit(x=[train_X, weights], y=train_Y, epochs=10)",11-09-2015 19:32,11-09-2015 19:32,Add a comment,"1
Thanks. So, model.compile('adam',custom_loss_1) model.fit(train_X, train_Y, epochs=10, batch_size=len(train_Y)) works. Would it be possible to correctly use the corresponding batch from the weight matrix? – 
Nipun Batra
 CommentedJan 3, 2018 at 18:19
I'm not entirely sure about the purpose of this weighted loss. Do you want to have a different weight for each training example, or rather a different weight for each ""class""? – 
rvinas
 CommentedJan 3, 2018 at 18:22
I'm trying to use a different weight for each ""class"". However, my outputs are real-valued variables. A lot of them are zeros. A. very few of them are say, greater than 10. So, it's a case of imbalance, which I'm trying to cover using weighing. Something like - give lower weight to 0s and higher weight to anything more than zero. Without weighting, I end up predicting zeros! – 
Nipun Batra
 CommentedJan 3, 2018 at 18:26
In that case, you should only have a different weight for each class (i.e. only 5 weights, not 100*5 weights). Your code will work if you define weights as: weights = np.random.randn(5) – 
rvinas
 CommentedJan 3, 2018 at 18:29
Actually, the notion of 100*5 weights comes from the fact that different samples (the 100 dimension) can have different amount of zero and non-zero values. Think of it in this way that given a 100*5 matrix with a lot of zeros, I want to give higher weight to non-zeros, which can occur anywhere in the matrix. – 
Nipun Batra
 CommentedJan 3, 2018 at 18:37
Ok. Please correct me if I'm wrong: if train_Y[i, :] is an array with 5 zeros, then you want to give sample i a low weight, right? – 
rvinas
 CommentedJan 3, 2018 at 18:44 
1
Let us continue this discussion in chat. – 
Nipun Batra
 CommentedJan 3, 2018 at 18:45
See also: stackoverflow.com/a/50127646/1447257 custom losses taking multiple inputs can be added with Model.add_loss – 
user1447257
 CommentedNov 11, 2018 at 23:12 
1
Thanks for the code. But I got this error: i.postimg.cc/6qwP5Bsr/error.png . I also asked the same question: stackoverflow.com/questions/74425890/…. I need to find the solution. I don't understand why a person commented and denied it. – 
Aref Hemati
 CommentedNov 15, 2022 at 20:25
1
This error: TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=""created by layer 'tf.cast_3'""), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as tf.cond, tf.function, gradient tapes, or tf.map_fn. Keras Functional model construction only supports TF API calls that do support dispatching, such as tf.math.add or tf.reshape. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by ... – 
Aref Hemati
 CommentedNov 15, 2022 at 20:29
1
...You can work around this limitation by putting the operation in a custom Keras layer call and calling that layer on this symbolic input/output. – 
Aref Hemati
 CommentedNov 15, 2022 at 20:30
Add a comment"
https://stackoverflow.com/questions/48303166,keras.wrappers.scikit_learn.KerasClassifier,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Evaluation,Static,0,<python><tensorflow><neural-network><deep-learning><keras>,8,43117.59236,43117.60486,8,40978,712995,762,7449890,2.726763298,0,https://stackoverflow.com/questions/48303166,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = pd.read_csv(""Churn_Modelling.csv"")
X = dataset.iloc[:,3:13].values
Y = dataset.iloc[:,13:].values

from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler

enc1=LabelEncoder()
enc2=LabelEncoder()
X[:,1] = enc1.fit_transform(X[:,1])
X[:,2] = enc2.fit_transform(X[:,2])

one = OneHotEncoder(categorical_features=[1])
X=one.fit_transform(X).toarray()

X = X[:,1:]

from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,random_state=0,test_size=0.2)

scale = StandardScaler()
scale.fit_transform(Xtrain)
scale.transform(Xtest)

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score 
from keras.models import Sequential
from keras.layers import Dense

def func1():
    net = Sequential()
    net.add(Dense(input_dim=11,units=6,activation=""relu"",kernel_initializer='uniform'))
    net.add(Dense(units=6,activation=""relu"",kernel_initializer='uniform'))
    net.add(Dense(units=1,activation=""sigmoid"",kernel_initializer='uniform'))
    net.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')

    return net

classfier = KerasClassifier(build_fn=func1(),batch_size=10, epochs=100)
cross = cross_val_score(estimator=classfier, X=Xtrain, y=Ytrain, cv=10 , n_jobs=-1)


Throws the error:

Traceback (most recent call last):

  File ""<ipython-input-7-e80e82960eb9>"", line 1, in <module>
    cross = cross_val_score(estimator=classfier, X=Xtrain, y=Ytrain, cv=10 , n_jobs=-1)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 342, in cross_val_score
    pre_dispatch=pre_dispatch)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 206, in cross_validate
    for train, test in cv.split(X, y, groups))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 620, in dispatch_one_batch
    tasks = BatchedCalls(itertools.islice(iterator, batch_size))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 127, in __init__
    self.items = list(iterator_slice)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 206, in <genexpr>
    for train, test in cv.split(X, y, groups))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\base.py"", line 62, in clone
    new_object_params[name] = clone(param, safe=False)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\base.py"", line 53, in clone
    return copy.deepcopy(estimator)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 169, in deepcopy
    rv = reductor(4)

TypeError: can't pickle _thread.lock objects


How do I solve this?","Change this line:

classfier = KerasClassifier(build_fn=func1, batch_size=10, epochs=100, verbose=0)


Note that func1 is not called. From the documentation:


  build_fn: callable function or class instance
  
  The build_fn should construct, compile and return a Keras model, which
     will then be used to fit/predict. One of the following
     three values could be passed to build_fn:
  
  
  A function
  An instance of a class that implements the __call__ method
  None. This means you implement a class that inherits from either
  KerasClassifier or KerasRegressor. The __call__ method of the
  present class will then be treated as the default build_fn.",11-10-2015 03:56,11-10-2015 17:09,"Add a comment
","1
Oooohhhh..My bad.... Totally forgot to check that parameter.... Thanks man – 
Joish
 CommentedJan 18, 2018 at 8:49 
1
you have saved my day too! – 
pari
 CommentedJan 4, 2021 at 22:07
Add a comment"
https://stackoverflow.com/questions/48547688,keras.models.model.target_model.set_weights,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Unknown,Keras,Model Construction,Static,0,<python-3.x><tensorflow><machine-learning><neural-network><keras>,10,43131.71389,43131.91736,30,28691,5974433,1044,3977276,3.18125,5.5,https://stackoverflow.com/questions/48547688,"Using Keras from Tensorflow 1.4.1, how does one copy weights from one model to another?

As some background, I'm trying to implement a deep-q network (DQN) for Atari games following the DQN publication by DeepMind.  My understanding is that the implementation uses two networks, Q and Q'.  The weights of Q are trained using gradient descent, and then the weights are copied periodically to Q'.

Here's how I build Q and Q':

ACT_SIZE   = 4
LEARN_RATE = 0.0025
OBS_SIZE   = 128

def buildModel():
  model = tf.keras.models.Sequential()

  model.add(tf.keras.layers.Lambda(lambda x: x / 255.0, input_shape=OBS_SIZE))
  model.add(tf.keras.layers.Dense(128, activation=""relu""))
  model.add(tf.keras.layers.Dense(128, activation=""relu""))
  model.add(tf.keras.layers.Dense(ACT_SIZE, activation=""linear""))
  opt = tf.keras.optimizers.RMSprop(lr=LEARN_RATE)

  model.compile(loss=""mean_squared_error"", optimizer=opt)

  return model


I call that twice to get Q and Q'.

I have an updateTargetModel method below that is my attempt at copying weights.  The code runs fine, but my overall DQN implementation is failing.  I'm really just trying to verify if this is a valid way of copying weights from one network to another.

def updateTargetModel(model, targetModel):
  modelWeights       = model.trainable_weights
  targetModelWeights = targetModel.trainable_weights

  for i in range(len(targetModelWeights)):
    targetModelWeights[i].assign(modelWeights[i])


There's another question here that discusses saving and loading weights to and from disk (Tensorflow Copy Weights Issue), but there's no accepted answer.  There is also a question about loading weights from individual layers (Copying weights from one Conv2D layer to another), but I'm wanting to copy the entire model's weights.","Actually what you've done is much more than simply copying weights. You made these two models identical all the time. Every time you update one model - the second one is also updated - as both models have the same weights variables. 

If you want to just copy weights - the simplest way is by this command:

target_model.set_weights(model.get_weights())",11-10-2015 06:17,11-10-2015 07:28,"Keras FAQ covers saving and loading model weights. You can save/load all weights or you can go by layer as well: keras.io/getting-started/faq/#how-can-i-save-a-keras-model – 
Manngo
 CommentedJan 31, 2018 at 17:57
Thank you Manngo. I have reviewed saving and loading models, and mentioned as much at the end of my question via the first inked question. My question, however, is regarding copying weights directly from one model to another without the intermediary file. – 
benbotto
 CommentedJan 31, 2018 at 19:14
Add a comment","Thanks Marcin, that's what I was worried about. So the way I was doing it the weights in Q' basically reference those in Q? Do I need to recompile the target model or anything else after copying using your method? – 
benbotto
 CommentedJan 31, 2018 at 22:51
Your welcome :) I'm glad I could help. Good luck with your project. – 
Marcin Mo?ejko
 CommentedJan 31, 2018 at 22:57
4
What is the difference with clone_model? github.com/keras-team/keras/issues/1765#issuecomment-324018225 – 
mrgloom
 CommentedMay 22, 2018 at 21:05
16
Just want to remind anyone reading this thread: the clone_model function, unlike what its name suggests, does NOT copy the weight – 
DiveIntoML
 CommentedFeb 27, 2020 at 22:43
1
@mrgloom, the referenced function creates a fresh model with the same architecture. This includes creating fresh weights. – 
emil
 CommentedDec 15, 2021 at 14:32
Add a comment"
https://stackoverflow.com/questions/48846332," keras.models.Model,keras.models.model.predict",Hybrid,F,F,AMO(Level-2),Missing Options,Crash,Keras,Prediction,Static,0,<python><tensorflow><neural-network><keras>,8,43148.91458,43149.85347,11,8698,7137636,390,3233501,2.501501501,0,https://stackoverflow.com/questions/48846332,"I train a model A and try to use the output of the intermediate layer with the name=""layer_x"" as an additional input for model B.

I tried to use the output of the intermediate layer like on the Keras doc 
https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer.

Model A:

inputs = Input(shape=(100,))
dnn = Dense(1024, activation='relu')(inputs)
dnn = Dense(128, activation='relu', name=""layer_x"")(dnn)
dnn = Dense(1024, activation='relu')(dnn)
output = Dense(10, activation='softmax')(dnn)


Model B:

input_1 = Input(shape=(200,))
input_2 = Input(shape=(100,)) # input for model A

# loading model A
model_a = keras.models.load_model(path_to_saved_model_a)

intermediate_layer_model = Model(inputs=model_a.input, 
                                 outputs=model_a.get_layer(""layer_x"").output)

intermediate_output = intermediate_layer_model.predict(data)

merge_layer = concatenate([input_1, intermediate_output])
dnn_layer = Dense(512, activation=""relu"")(merge_layer)
output = Dense(5, activation=""sigmoid"")(dnn_layer)
model = keras.models.Model(inputs=[input_1, input_2], outputs=output)


When I debug I get an error on this line:

intermediate_layer_model = Model(inputs=model_a.input, 
                                 outputs=model_a.get_layer(""layer_x"").output)

File "".."", line 89, in set_model
  outputs=self.neural_net_asc.model.get_layer(""layer_x"").output)
File ""C:\WinPython\python-3.5.3.amd64\lib\site-packages\keras\legacy\interfaces.py"", line 87, in wrapper
  return func(*args, **kwargs)
File ""C:\WinPython\python-3.5.3.amd64\lib\site-packages\keras\engine\topology.py"", line 1592, in __init__
  mask = node.output_masks[tensor_index]
AttributeError: 'Node' object has no attribute 'output_masks'


I can access the tensor with get_layer(""layer_x"").output and the output_mask is None. Do I have to set manually an output mask and how do I set up this output mask if needed?","There are two things that you seem to be doing wrong :

intermediate_output = intermediate_layer_model.predict(data)


when you do .predict(), you are actually passing data through the graph and asking what will be the result. When you do that, intermediate_output will be a numpy array and not a layer as you would like it to be.

Secondly, you don't need to recreate a new intermediate model. You can directly use the part of model_a that interest you.

Here is a code that ""compiles"" for me :

from keras.layers import Input, Dense, concatenate
from keras.models import Model

inputs = Input(shape=(100,))
dnn = Dense(1024, activation='relu')(inputs)
dnn = Dense(128, activation='relu', name=""layer_x"")(dnn)
dnn = Dense(1024, activation='relu')(dnn)
output = Dense(10, activation='softmax')(dnn)

model_a = Model(inputs=inputs, outputs=output)

# You don't need to recreate an input for the model_a, 
# it already has one and you can reuse it
input_b = Input(shape=(200,))

# Here you get the layer that interests you from model_a, 
# it is still linked to its input layer, you just need to remember it for later
intermediate_from_a = model_a.get_layer(""layer_x"").output

# Since intermediate_from_a is a layer, you can concatenate it with the other input
merge_layer = concatenate([input_b, intermediate_from_a])
dnn_layer = Dense(512, activation=""relu"")(merge_layer)
output_b = Dense(5, activation=""sigmoid"")(dnn_layer)
# Here you remember that one input is input_b and the other one is from model_a
model_b = Model(inputs=[input_b, model_a.input], outputs=output_b)


I hope this is what you wanted to do.

Please tell me if something isn't clear :-)",11-10-2015 15:19,11-10-2015 15:41,Add a comment,"1
hey I finally found my mistake I used tensorflow.python.keras api and here I imported from keras.models and the 2 different keras versions of tensorflow and pure keras could not work together :) but thank you for your answer but also I do not use the predict method now – 
KyleReemoN-
 CommentedFeb 18, 2018 at 21:46 
@Nassim which is the proper way of train such a ""chain-of-models""? – 
Pierluigi
 CommentedSep 26, 2018 at 18:48 
It really depends on the case... The natural way seems to be 1) train model A, 2) freeze common layers, 3) train model B with frozen layers from model A – 
Nassim Ben
 CommentedSep 26, 2018 at 19:06
@NassimBen I see, I was hoping in some keras (tensorflow) magic to do the training one shot... – 
Pierluigi
 CommentedSep 27, 2018 at 13:38
That kind of model is really application specific... the way you train it really depends on the reasons why those two models are connected... so no magic to my knowledge :D sorry – 
Nassim Ben
 CommentedSep 27, 2018 at 13:41
Add a comment"
https://stackoverflow.com/questions/49546922," keras.models.model.get_weights,keras.models.model.load_model",Hybrid,SL,"F,IC-1",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,Unknown,Keras,Load,Static,0,<python><tensorflow><deep-learning><keras>,15,43188.08889,43188.46111,25,1522,3994228,389,6217326,1.318681319,0,https://stackoverflow.com/questions/49546922,"The code that I have (that I can't change) uses the Resnet with my_input_tensor as the input_tensor.

model1 = keras.applications.resnet50.ResNet50(input_tensor=my_input_tensor, weights='imagenet')


Investigating the source code, ResNet50 function creates a new keras Input Layer with my_input_tensor and then create the rest of the model. This is the behavior that I want to copy with my own model. I load my model from h5 file.

model2 = keras.models.load_model('my_model.h5')


Since this model already has an Input Layer, I want to replace it with a new Input Layer defined with my_input_tensor.

How can I replace an input layer?","When you saved your model using:

old_model.save('my_model.h5')


it will save following:


The architecture of the model, allowing to create the model.
The weights of the model.
The training configuration of the model (loss, optimizer).
The state of the optimizer, allowing training to resume from where you left before.


So then, when you load the model:

res50_model = load_model('my_model.h5')


you should get the same model back, you can verify the same using:

res50_model.summary()
res50_model.get_weights()


Now you can, pop the input layer and add your own using:

res50_model.layers.pop(0)
res50_model.summary()


add new input layer: 

newInput = Input(batch_shape=(0,299,299,3))    # let us say this new InputLayer
newOutputs = res50_model(newInput)
newModel = Model(newInput, newOutputs)

newModel.summary()
res50_model.summary()",11-10-2015 22:07,4/22/2016 4:23,"have u tried the funtional api – 
user239457
 CommentedMar 29, 2018 at 3:26
I haven't. I looked up the documentation. Maybe model2(my_input_tensor) can be somehow used? – 
zcadqe
 CommentedMar 29, 2018 at 4:01
If the model is not sequential (as i can see its resnet50) the solution is below, if it is, you can use model.add(). – 
Milind Deore
 CommentedMar 29, 2018 at 11:08
Add a comment","4
Since I have input tensor that I want to use, I ended using. new_input_layer = keras.layers.Input(tensor=my_input_tensor) and followed your suggestion. It works! Thanks. – 
zcadqe
 CommentedMar 30, 2018 at 1:52
6
It seems odd that after the model.layers.pop(0) call that model.input still shows original input layer. I was trying the posted solution with VGG16 model. – 
user3731622
 CommentedMar 15, 2019 at 19:35
1
THIS WILL NOT WORK, if we create a resnet18, then we remove the first layer (pop) then we create a model with another input and then the resnet, what happens is that it creates a model with one input layer and the second a ""model"" layer, which is disconnected from the other. Plus, this also denies the possibility of concatenating/merging with other models (this is mainly why I am complaining). – 
dberga
 CommentedDec 17, 2019 at 12:15 
2
model.layers.pop(0) won't change anything. github.com/tensorflow/tensorflow/issues/22479 – 
Jason
 CommentedSep 6, 2020 at 6:38 
Add a comment"
https://stackoverflow.com/questions/49834380," keras.backend.function,keras.backend.gradients",AMO,F,F,F,Missing Required State-specific Method Order,Crash,Keras,Model Evaluation,Static,0,<python><tensorflow><neural-network><deep-learning><keras>,9,43204.73819,43204.78194,11,41498,349130,153,8815081,0.820833961,0,https://stackoverflow.com/questions/49834380,"I have CNN models trained using Keras with Tensorflow backend.
And I want to visualize my CNN filters with this tutorial: https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html

from keras import backend as K
from keras.models import load_model
import numpy as np

model = load_model('my_cnn_model.h5')
input_img = np.load('my_picture.npy')

# get the symbolic outputs of each ""key"" layer (we gave them unique names).
layer_dict = dict([(layer.name, layer) for layer in model.layers])

layer_name = 'block5_conv3'
filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer

# build a loss function that maximizes the activation
# of the nth filter of the layer considered
layer_output = layer_dict[layer_name].output
loss = K.mean(layer_output[:, :, :, filter_index])

# compute the gradient of the input picture wrt this loss
grads = K.gradients(loss, input_img)[0]

# normalization trick: we normalize the gradient
grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)

# this function returns the loss and grads given the input picture
iterate = K.function([input_img], [loss, grads])


However, when the code execute to this line:

grads = K.gradients(loss, input_img)[0]

I found it returns nothing but None object, so the program fail to progress after that.

I search for some solution. Some people say theinput_img should be tensorflow's Tensor type:
https://github.com/keras-team/keras/issues/5455

But when I tried to convert the img to Tensor, the problem is still exist.

I tried the solution in the link above, but still fail.

There is also someone say that this problem exists because your CNN model is not differentiable.
https://github.com/keras-team/keras/issues/8478

But my model use only the activate function of ReLU and Sigmoid(at output layer).
Is this problem really caused by nondifferentiable problem?

Can anyone help me? Thank you very much!","If you have a Model instance, then to take the gradient of the loss with respect to the input, you should do:

grads = K.gradients(loss, model.input)[0]


model.input contains the symbolic tensor that represents the input to the model. Using a plain numpy array makes no sense because TensorFlow then has no idea how this connects to the computational graph, and returns None as the gradient.

Then you should also rewrite the iterate function as:

iterate = K.function([model.input], [loss, grads])",11-10-2015 23:43,11-11-2015 05:30,"Could you provide your code and the complete error? Also, the only NoneType object that exists is None.
                
                
– wizzwizz4
                
                
                    Commented
                    Apr 14, 2018 at 17:44
                
            
        
    
    
        
            
            
        
        
            
                
                @wizzwizz4 It returns “None” indeed! I pasted the code from the tutorial right now. The error message from tensorflow is so long, but I know that it is because the next function which use “grads” as parameter fails to deal with “None” variable. I think the gradients of the neural network is no possible to be None, right?
                
                
– Jexus
                
                
                    Commented
                    Apr 14, 2018 at 18:19
                
            
        
    
    
        
            
            
        
        
            
                
                Well, it doesn't make all that much sense for it to be None... But we can't help you unless you show us the code, the specific error message and what inputs you put in to create the problem. Bonus points if you can simplify the program and the problem still occurs.
                
                
– wizzwizz4
                
                
                    Commented
                    Apr 14, 2018 at 18:28
                
            
        
    
    
        
            
            
        
        
            
                
                What is input_img exactly? Can you show its definition?
                
                
– Dr. Snoopy
                
                
                    Commented
                    Apr 14, 2018 at 18:34
                
            
        
    
    
        
            
            
        
        
            
                
                @MatiasValdenegro input_img is a numpy array with shape=(48, 48), dtype = float it is a gray scale image, with every value range from 0 to 255.
                
                
– Jexus
                
                
                    Commented
                    Apr 14, 2018 at 18:38
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 3 more comments","Thank you so much! I know where the problem is.
                
                
– Jexus
                
                
                    Commented
                    Apr 14, 2018 at 18:50
                
            
        
    
    
        
            
            
        
        
            
                
                @Matias Valdenegro I'm facing a similar issue where I'm getting a NoneType, but in my case my model.input is the input to my fine tuned network, so I believe it's connected to the graph.  Do you have any suggestions?: stackoverflow.com/questions/50310063/…
                
                
– Ryan Chase
                
                
                    Commented
                    May 12, 2018 at 19:52
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                @Jexus - were you able to get your code to work?  How did you alter your code to get it to work?
                
                
– Ryan Chase
                
                
                    Commented
                    May 12, 2018 at 19:52
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                It would be very nice to complement this answer with how to use the function. We need sess = K.get_session() and results = sess.run(iterate, feed_dict={model.input: numpyBatchWithData}).
                
                
– Daniel Möller
                
                
                    Commented
                    Mar 7, 2019 at 1:18
                
            
        
    
    
        
            
            
        
        
            
                
                I think @DanielMöller shows how to use this function. I was calculating the gradient of the output of the network with respect to the input, and couldn't understand how to get this function to work k.gradients(_, _). Thnx @DanielMöller!
                
                
– KareemJ
                
                
                    Commented
                    Oct 10, 2019 at 11:05
                
            
        
    

            
	    

        
                    
                 | 
            Show 1 more comment"
https://stackoverflow.com/questions/51100508," keras.backend.sqrt,keras.backend.expand_dims",Hybrid,SL,"F,IC-1",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,Crash,Keras,Model Construction,Static,0,<python><tensorflow><keras><loss-function>,6,43280.46875,43284.28264,7,11289,1531463,229,1419189,4.628205128,0,https://stackoverflow.com/questions/51100508,"I need some help with keras loss function. I have been implementing custom loss function on keras with Tensorflow backend. 

I have implemented the custom loss function in numpy but it would be great if it could be translated into keras loss function. The loss function takes dataframe and series of user id. The Euclidean distance for same user_id are positive and negative if the user_id are different. The function returns summed up scalar distance of the dataframe.

def custom_loss_numpy (encodings, user_id):
# user_id: a pandas series of users
# encodings: a pandas dataframe of encodings

    batch_dist = 0

    for i in range(len(user_id)):
         first_row = encodings.iloc[i,:].values
         first_user = user_id[i]

         for j in range(i+1, len(user_id)):
              second_user = user_id[j]
              second_row = encodings.iloc[j,:].values

        # compute distance: if the users are same then Euclidean distance is positive otherwise negative.
            if first_user == second_user:
                tmp_dist = np.linalg.norm(first_row - second_row)
            else:
                tmp_dist = -np.linalg.norm(first_row - second_row)

            batch_dist += tmp_dist

    return batch_dist


I have tried to implement into keras loss function. I extracted numpy array from y_true and y_pred tensor objects. 

def custom_loss_keras(y_true, y_pred):
    # session of my program
    sess = tf_session.TF_Session().get()

    with sess.as_default():
        array_pred = y_pred.eval()
        print(array_pred)


But I get the following error. 

tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'dense_1_input' with dtype float and shape [?,102]
 [[Node: dense_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,102], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


Any kind of help would be really appreciated.","First of all, it is not possible to ""extract numpy array from y_true and y_pred"" in Keras loss functions. You have to operate the tensors with Keras backend functions (or TF functions) to calculate the loss.

In other words, it would be better to think about a ""vectorized"" way to calculate the loss, without using if-else and loops.

Your loss function can be computed in the following steps:


Generate a matrix of pairwise Euclidean distances, between all pairs of vectors in encodings.
Generate a matrix I whose element I_ij is 1 if user_i == user_j, and -1 if user_i != user_j.
Element-wise multiply the two matrices, and sum up the elements to get the final loss.


Here's an implementation:

def custom_loss_keras(user_id, encodings):
    # calculate pairwise Euclidean distance matrix
    pairwise_diff = K.expand_dims(encodings, 0) - K.expand_dims(encodings, 1)
    pairwise_squared_distance = K.sum(K.square(pairwise_diff), axis=-1)

    # add a small number before taking K.sqrt for numerical safety
    # (K.sqrt(0) sometimes becomes nan)
    pairwise_distance = K.sqrt(pairwise_squared_distance + K.epsilon())

    # this will be a pairwise matrix of True and False, with shape (batch_size, batch_size)
    pairwise_equal = K.equal(K.expand_dims(user_id, 0), K.expand_dims(user_id, 1))

    # convert True and False to 1 and -1
    pos_neg = K.cast(pairwise_equal, K.floatx()) * 2 - 1

    # divide by 2 to match the output of `custom_loss_numpy`, but it's not really necessary
    return K.sum(pairwise_distance * pos_neg, axis=-1) / 2


I've assumed that user_id are integers in the code above. The trick here is to use K.expand_dims for implementing pairwise operations. It's probably a bit difficult to understand at a first glance, but it's quite useful.

It should give about the same loss value as custom_loss_numpy (there will be a little bit difference because of K.epsilon()):

encodings = np.random.rand(32, 10)
user_id = np.random.randint(10, size=32)

print(K.eval(custom_loss_keras(K.variable(user_id), K.variable(encodings))).sum())
-478.4245

print(custom_loss_numpy(pd.DataFrame(encodings), pd.Series(user_id)))
-478.42953553795815




I've made a mistake in the loss function.

When this function is used in training, since Keras automatically changes y_true to be at least 2D, the argument user_id is no longer a 1D tensor. The shape of it will be (batch_size, 1).

In order to use this function, the extra axis must be removed:

def custom_loss_keras(user_id, encodings):
    pairwise_diff = K.expand_dims(encodings, 0) - K.expand_dims(encodings, 1)
    pairwise_squared_distance = K.sum(K.square(pairwise_diff), axis=-1)
    pairwise_distance = K.sqrt(pairwise_squared_distance + K.epsilon())

    user_id = K.squeeze(user_id, axis=1)  # remove the axis added by Keras
    pairwise_equal = K.equal(K.expand_dims(user_id, 0), K.expand_dims(user_id, 1))

    pos_neg = K.cast(pairwise_equal, K.floatx()) * 2 - 1
    return K.sum(pairwise_distance * pos_neg, axis=-1) / 2",11-11-2015 08:15,11/13/2015 17:40,Add a comment,"Add a comment
"
https://stackoverflow.com/questions/51109639,keras.layers.CuDNNLSTM,SAM,BET,IC-1,IC-1,Unacceptable Input Value,BP,Keras,Model Construction,Runtime Checking,0,<python><tensorflow><keras>,6,43280.89514,43280.91528,9,887,5745211,173,9458627,1.285714286,0,https://stackoverflow.com/questions/51109639,"Whenever I try out LSTM models on Keras, it seems that the model is impossible to train due to long training time.

For instance, a model like this takes 80 seconds per step to train.:

def create_model(self):
        inputs = {}
        inputs['input'] = []
        lstm = []
        placeholder = {}
        for tf, v in self.env.timeframes.items():
            inputs[tf] = Input(shape = v['shape'], name = tf)
            lstm.append(LSTM(8)(inputs[tf]))
            inputs['input'].append(inputs[tf])
        account = Input(shape = (3,), name = 'account')
        account_ = Dense(8, activation = 'relu')(account)
        dt = Input(shape = (7,), name = 'dt')
        dt_ = Dense(16, activation = 'relu')(dt)
        inputs['input'].extend([account, dt])

        data = Concatenate(axis = 1)(lstm)
        data = Dense(128, activation = 'relu')(data)
        y = Concatenate(axis = 1)([data, account, dt])
        y = Dense(256, activation = 'relu')(y)
        y = Dense(64, activation = 'relu')(y)
        y = Dense(16, activation = 'relu')(y)
        output = Dense(3, activation = 'linear')(y)

        model = Model(inputs = inputs['input'], outputs = output)
        model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mae'])
        return model


Whereas model which has LSTM substituded with Flatten + Dense like this:

def create_model(self):
        inputs = {}
        inputs['input'] = []
        lstm = []
        placeholder = {}
        for tf, v in self.env.timeframes.items():
            inputs[tf] = Input(shape = v['shape'], name = tf)
            #lstm.append(LSTM(8)(inputs[tf]))
            placeholder[tf] = Flatten()(inputs[tf])
            lstm.append(Dense(32, activation = 'relu')(placeholder[tf]))
            inputs['input'].append(inputs[tf])
        account = Input(shape = (3,), name = 'account')
        account_ = Dense(8, activation = 'relu')(account)
        dt = Input(shape = (7,), name = 'dt')
        dt_ = Dense(16, activation = 'relu')(dt)
        inputs['input'].extend([account, dt])

        data = Concatenate(axis = 1)(lstm)
        data = Dense(128, activation = 'relu')(data)
        y = Concatenate(axis = 1)([data, account, dt])
        y = Dense(256, activation = 'relu')(y)
        y = Dense(64, activation = 'relu')(y)
        y = Dense(16, activation = 'relu')(y)
        output = Dense(3, activation = 'linear')(y)

        model = Model(inputs = inputs['input'], outputs = output)
        model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mae'])
        return model


takes 45-50 ms per step to train.

Is there something wrong in the model that is causing this? Or is this as fast as this model will run?

-- self.env.timeframes looks like this: dictionary with 9 items

timeframes = {
            's1': {
                'lookback': 86400,
                'word': '1 s',
                'unit': 1,
                'offset': 12
                },
            's5': {
                'lookback': 200,
                'word': '5 s',
                'unit': 5,
                'offset': 2
                },
            'm1': {
                'lookback': 100,
                'word': '1 min',
                'unit': 60,
                'offset': 0
                },
            'm5': {
                'lookback': 100,
                'word': '5 min',
                'unit': 300,
                'offset': 0
                },
            'm30': {
                'lookback': 100,
                'word': '30 min',
                'unit': 1800,
                'offset': 0
                },
            'h1': {
                'lookback': 200,
                'word': '1 h',
                'unit': 3600,
                'offset': 0
                },
            'h4': {
                'lookback': 200,
                'word': '4 h',
                'unit': 14400,
                'offset': 0
                },
            'h12': {
                'lookback': 100,
                'word': '12 h',
                'unit': 43200,
                'offset': 0
                },
            'd1': {
                'lookback': 200,
                'word': '1 d',
                'unit': 86400,
                'offset': 0
                }
            }


GPU info from prompt - 

2018-06-30 07:35:16.204320: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-06-30 07:35:16.495832: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.86
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.59GiB
2018-06-30 07:35:16.495981: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-30 07:35:16.956743: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-30 07:35:16.956827: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0
2018-06-30 07:35:16.957540: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N
2018-06-30 07:35:16.957865: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6370 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)","If you are using GPU please replace all LSTM layers with CuDNNLSTM layers. You can import it from keras.layers:

from keras.layers import  CuDNNLSTM

def create_model(self):
    inputs = {}
    inputs['input'] = []
    lstm = []
    placeholder = {}
    for tf, v in self.env.timeframes.items():
        inputs[tf] = Input(shape = v['shape'], name = tf)
        lstm.append(CuDNNLSTM(8)(inputs[tf]))
        inputs['input'].append(inputs[tf])
    account = Input(shape = (3,), name = 'account')
    account_ = Dense(8, activation = 'relu')(account)
    dt = Input(shape = (7,), name = 'dt')
    dt_ = Dense(16, activation = 'relu')(dt)
    inputs['input'].extend([account, dt])

    data = Concatenate(axis = 1)(lstm)
    data = Dense(128, activation = 'relu')(data)
    y = Concatenate(axis = 1)([data, account, dt])
    y = Dense(256, activation = 'relu')(y)
    y = Dense(64, activation = 'relu')(y)
    y = Dense(16, activation = 'relu')(y)
    output = Dense(3, activation = 'linear')(y)

    model = Model(inputs = inputs['input'], outputs = output)
    model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mae'])
    return model


Here is more information: https://keras.io/layers/recurrent/#cudnnlstm

This will significantly speed up the model =)",11-11-2015 10:01,11-11-2015 10:10,"Could you elaborate on self.env.timeframes? There seems to be a property 'shape' that is used in your code but isn't clear from your example. – 
alta
 CommentedJun 29, 2018 at 21:40 
right, shape just returns list of [rows, columns] for size of input data – 
Joseph Choi
 CommentedJun 29, 2018 at 21:47
Add a comment","I didn't know this! I will try this now and reply – 
Joseph Choi
 CommentedJun 29, 2018 at 22:06 
I am getting 2-3 seconds per step with this - significant improvement ! It seems Keras does have documentation about this - keras.io/layers/recurrent/#cudnnlstm albeit I did not come across any markers//directions to this up until now from any Keras tutorials/examples – 
Joseph Choi
 CommentedJun 29, 2018 at 22:47 
It's much faster. But you lose: masking, custom activation, dropout. – 
John Stewart
 CommentedNov 10, 2018 at 19:53
Add a comment"
https://stackoverflow.com/questions/51763983,keras.models.fit,SAM,DT,BIT,BIT,Unacceptable Input Type,Crash,Keras,Model Evaluation,Static,0,<python><tensorflow><neural-network><keras>,12,43319.67361,43319.68472,15,1745,7699859,523,6875778,1.8,0,https://stackoverflow.com/questions/51763983,"I'm training a model to predict the stock price and input data is close price. I use 45 days data to predict the 46th day's close price and a economic Indicator to be second feature, here is the model:

model = Sequential()
model.add( LSTM( 512, input_shape=(45, 2), return_sequences=True))
model.add( LSTM( 512, return_sequences=True))
model.add( (Dense(1)))
model.compile(loss='mse', optimizer='adam')
history = model.fit( X_train, y_train, batch_size = batchSize, epochs=epochs, shuffle = False)


When I run this I get the following error:


  ValueError: Error when checking target: expected dense_1 to have 3
  dimensions, but got array with shape (118, 1)


However, I print the shape of data and they are:

X_train:(118, 45, 2)
y_train:(118, 1)


I have no idea why the model is expecting a 3 dimensional output when y_train is (118, 1). Where am I wrong and what should I do?","Your second LSTM layer also returns sequences and Dense layers by default apply the kernel to every timestep also producing a sequence:

# (bs, 45, 2)
model.add( LSTM( 512, input_shape=(45, 2), return_sequences=True))
# (bs, 45, 512)
model.add( LSTM( 512, return_sequences=True))
# (bs, 45, 512)
model.add( (Dense(1)))
# (bs, 45, 1)


So your output is shape (bs, 45, 1). To solve the problem you need to set return_sequences=False in your second LSTM layer which will compress sequence:

# (bs, 45, 2)
model.add( LSTM( 512, input_shape=(45, 2), return_sequences=True))
# (bs, 45, 512)
model.add( LSTM( 512, return_sequences=False)) # SET HERE
# (bs, 512)
model.add( (Dense(1)))
# (bs, 1)


And you'll get the desired output. Note bs is the batch size.",11-11-2015 10:09,11/23/2015 1:39,"Add a comment
                 |","1
            
        
        
            
                
                After reshaping for 131 sample data using [131,32,512] for training and reshaping [131,32,8] for  corresponding labels I have received the following error: ValueError: Error when checking target: expected dense_8 to have 2 dimensions, but got array with shape (131, 32, 8)
                
                
– Nhqazi
                
                
                    Commented
                    Jun 9, 2019 at 23:46
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/53212672,keras.models.model.load_weights,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Load,Static,0,<python><tensorflow><keras><deep-learning>,15,43412.70833,43412.77292,12,374,2103431,197,7576115,11,0,https://stackoverflow.com/questions/53212672,"I have cloned human pose estimation keras model from this link human pose estimation keras 

When I try to load the model on google colab, I get the following error

code

from keras.models import load_model
model = load_model('model.h5')


error

ValueError                                Traceback (most recent call 

last)
<ipython-input-29-bdcc7d8d338b> in <module>()
      1 from keras.models import load_model
----> 2 model = load_model('model.h5')

/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py in load_model(filepath, custom_objects, compile)
    417     f = h5dict(filepath, 'r')
    418     try:
--> 419         model = _deserialize_model(f, custom_objects, compile)
    420     finally:
    421         if opened_new_file:

/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py in _deserialize_model(f, custom_objects, compile)
    219         return obj
    220 
--> 221     model_config = f['model_config']
    222     if model_config is None:
    223         raise ValueError('No model found in config.')

/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py in __getitem__(self, attr)
    300             else:
    301                 if self.read_only:
--> 302                     raise ValueError('Cannot create group in read only mode.')
    303                 val = H5Dict(self.data.create_group(attr))
    304         return val

ValueError: Cannot create group in read only mode.


Can someone please help me understand this read-only mode? How do I load this model?","I had a similar issue and solved this way

store the graph\architecture in JSON format and weights in h5 format 

import json

# lets assume `model` is main model 
model_json = model.to_json()
with open(""model_in_json.json"", ""w"") as json_file:
    json.dump(model_json, json_file)

model.save_weights(""model_weights.h5"")


then need to load model first to create graph\architecture and load_weights in model

from keras.models import load_model
from keras.models import model_from_json
import json

with open('model_in_json.json','r') as f:
    model_json = json.load(f)

model = model_from_json(model_json)
model.load_weights('model_weights.h5')",11-11-2015 16:58,11/13/2015 18:03,"I had the same error for my network and found a solution. coremltools troubleshoots – 
Dmitry Pavlenko
 CommentedJan 31, 2019 at 18:06
Add a comment","How does one 'set and define the architecture of your model'? – 
deadcode
 CommentedFeb 18, 2019 at 9:09
e.g. model = Sequential() model.add(layers.Dense(512, activation='relu', input_shape=input_shape)) – 
Philip
 CommentedOct 23, 2019 at 11:42 
Add a comment"
https://stackoverflow.com/questions/53496095,keras.layers.SimpleRNN,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Initialization,Static,0,<python><tensorflow><machine-learning><keras><rnn>,8,43431.38056,43431.40694,6,18037,2099607,241,6187009,1.64761847,0.083333333,https://stackoverflow.com/questions/53496095,"I'm a beginner in Keras and just write a toy example. It reports a TypeError. The code and error are as follows:

Code:

inputs = keras.Input(shape=(3, ))

cell = keras.layers.SimpleRNNCell(units=5, activation='softmax')
label = keras.layers.RNN(cell)(inputs)

model = keras.models.Model(inputs=inputs, outputs=label)
model.compile(optimizer='rmsprop',
              loss='mae',
              metrics=['acc'])

data = np.array([[1, 2, 3], [3, 4, 5]])
labels = np.array([1, 2])
model.fit(x=data, y=labels)


Error:

Traceback (most recent call last):
    File ""/Users/david/Documents/code/python/Tensorflow/test.py"", line 27, in <module>
        run()
    File ""/Users/david/Documents/code/python/Tensorflow/test.py"", line 21, in run
        label = keras.layers.RNN(cell)(inputs)
    File ""/Users/david/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 619, in __call__
...
    File ""/Users/david/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py"", line 473, in __call__
        scale /= max(1., (fan_in + fan_out) / 2.)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'


So how can I deal with it?","The input to a RNN layer would have a shape of (num_timesteps, num_features), i.e. each sample consists of num_timesteps timesteps where each timestep is a vector of length num_features. Further, the number of timesteps (i.e. num_timesteps) could be variable or unknown (i.e. None) but the number of features (i.e. num_features) should be fixed and specified from the beginning. Therefore, you need to change the shape of Input layer to be consistent with the RNN layer. For example:

inputs = keras.Input(shape=(None, 3))  # variable number of timesteps each with length 3
inputs = keras.Input(shape=(4, 3))     # 4 timesteps each with length 3
inputs = keras.Input(shape=(4, None))  # this is WRONG! you can't do this. Number of features must be fixed


Then, you also need to change the shape of input data (i.e. data) as well to be consistent with the input shape you have specified (i.e. it must have a shape of (num_samples, num_timesteps, num_features)).

As a side note, you could define the RNN layer more simply by using the SimpleRNN layer directly:

label = keras.layers.SimpleRNN(units=5, activation='softmax')(inputs)",11-11-2015 17:42,11/13/2015 10:56,Add a comment,"Thanks for your answer? – 
david
 CommentedNov 27, 2018 at 9:55
Add a comment"
https://stackoverflow.com/questions/39467496,keras.wrappers.scikit_learn.KerasRegressor,SAM,DT,RT,RT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><machine-learning><scikit-learn><keras>,8,42626.42569,42648.63403,12,250,2479241,123,6825808,12,0,https://stackoverflow.com/questions/39467496,"i'm learning keras these days, and i met an error when using scikit-learn API.Here are something maybe useful:  

ENVIRONMENT:  

python:3.5.2  
keras:1.0.5  
scikit-learn:0.17.1


CODE

import pandas as pd
from keras.layers import Input, Dense
from keras.models import Model
from keras.models import Sequential
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import cross_val_score
from sqlalchemy import create_engine
from sklearn.cross_validation import KFold


def read_db():
    ""get prepared data from mysql.""
    con_str = ""mysql+mysqldb://root:0000@localhost/nbse?charset=utf8""
    engine = create_engine(con_str)
    data = pd.read_sql_table('data_ml', engine)
    return data

def nn_model():
    ""create a model.""
    model = Sequential()
    model.add(Dense(output_dim=100, input_dim=105, activation='softplus'))
    model.add(Dense(output_dim=1, input_dim=100, activation='softplus'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

data = read_db()
y = data.pop('PRICE').as_matrix()
x = data.as_matrix()
model = nn_model()
model = KerasRegressor(build_fn=model, nb_epoch=2)
model.fit(x,y)  #something wrong here!


ERROR

Traceback (most recent call last):
  File ""C:/Users/Administrator/PycharmProjects/forecast/gridsearch.py"", line 43, in <module>
    model.fit(x,y)
  File ""D:\Program Files\Python35\lib\site-packages\keras\wrappers\scikit_learn.py"", line 135, in fit
    **self.filter_sk_params(self.build_fn.__call__))
TypeError: __call__() missing 1 required positional argument: 'x'

Process finished with exit code 1


??the model works well without packaging with kerasRegressor, but i wanna using sk_learn's gridSearch after this, so i'm here for help. I tried but still have no idea.  

something maybe helpful:

keras.warappers.scikit_learn.py  

class BaseWrapper(object):  


    def __init__(self, build_fn=None, **sk_params):
        self.build_fn = build_fn
        self.sk_params = sk_params
        self.check_params(sk_params)  


    def fit(self, X, y, **kwargs):
        '''Construct a new model with build_fn and fit the model according
        to the given training data.
    # Arguments
        X : array-like, shape `(n_samples, n_features)`
            Training samples where n_samples in the number of samples
            and n_features is the number of features.
        y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`
            True labels for X.
        kwargs: dictionary arguments
            Legal arguments are the arguments of `Sequential.fit`

    # Returns
        history : object
            details about the training history at each epoch.
    '''

    if self.build_fn is None:
        self.model = self.__call__(**self.filter_sk_params(self.__call__))
    elif not isinstance(self.build_fn, types.FunctionType):
        self.model = self.build_fn(
            **self.filter_sk_params(self.build_fn.__call__))
    else:
        self.model = self.build_fn(**self.filter_sk_params(self.build_fn))

    loss_name = self.model.loss
    if hasattr(loss_name, '__name__'):
        loss_name = loss_name.__name__
    if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:
        y = to_categorical(y)

    fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))
    fit_args.update(kwargs)

    history = self.model.fit(X, y, **fit_args)

    return history


??error occored in this line:

    self.model = self.build_fn(
        **self.filter_sk_params(self.build_fn.__call__))


self.build_fn here is keras.models.Sequential  

models.py  

class Sequential(Model):

    def call(self, x, mask=None):
        if not self.built:
            self.build()
        return self.model.call(x, mask)


So, what's that x mean and how to fix this error?

Thanks!","xiao, I ran into the same issue! Hopefully this helps:
Background and The Issue
The documentation for Keras states that, when implementing Wrappers for scikit-learn, there are two arguments. The first is the build function, which is a ""callable function or class instance"". Specifically, it states that:

build_fn should construct, compile and return a Keras model, which will then be used to fit/predict. One of the following three values could be passed to build_fn:

A function
An instance of a class that implements the call method
None. This means you implement a class that inherits from either KerasClassifier or KerasRegressor. The call method of the present class will then be treated as the default build_fn.


In your code, you create the model, and then pass the model as the value for the argument build_fn when creating the KerasRegressor wrapper:
model = nn_model()
model = KerasRegressor(build_fn=model, nb_epoch=2)

Herein lies the issue. Rather than passing your nn_model function as the build_fn, you pass an actual instance of the Keras Sequential model. For this reason, when fit() is called, it cannot find the call method, because it is not implemented in the class you returned.
Proposed Solution
What I did to make things work is pass the function as build_fn, rather than an actual model:
data = read_db()
y = data.pop('PRICE').as_matrix()
x = data.as_matrix()
# model = nn_model() # Don't do this!
# set build_fn equal to the nn_model function
model = KerasRegressor(build_fn=nn_model, nb_epoch=2) # note that you do not call the function!
model.fit(x,y)  # fixed!

This is not the only solution (you could set build_fn to a class that implements the call method appropriately), but the one that worked for me. I hope it helps you!",11-11-2015 20:40,11-12-2015 00:26,"Add a comment
                 |","2
            
        
        
            
                
                @matsuninja Do you know how to pass parameters to the function? I would like to make a more generic nn_model function with different layer sizes and such parameters...
                
                
– Luis
                
                
                    Commented
                    Feb 26, 2017 at 16:38
                
            
        
    
    
        
            
            
        
        
            
                
                @Luis, in the example above, remember that the code inside of the nn_model function defines the model parameters (including layer sizes, etc.). My proposed solution (again, only one of a few alternatives) passes the function nn_model as the build_fn value. In order to set the model parameters, alter the code in the nn_model function; this is in fact what xiao does when defining the nn_model function. Hope that helps.
                
                
– matsuninja
                
                
                    Commented
                    Feb 27, 2017 at 17:22
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @matsuninja I solved it by setting a global variable to the number of nodes. I guess it is not possible to use an nn_model function which expects parameters in this approach...
                
                
– Luis
                
                
                    Commented
                    Feb 28, 2017 at 15:50
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/40393629,keras.wrappers.scikit_learn.KerasClassifier,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><function><scikit-learn><keras><callable>,18,42677.16458,42680.97708,16,1734,4120005,42488,67405,7.279411765,0,https://stackoverflow.com/questions/40393629,"I have the following code, using Keras Scikit-Learn Wrapper, which work fine:

from keras.models import Sequential
from keras.layers import Dense
from sklearn import datasets
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
import numpy as np


def create_model():
    # create model
    model = Sequential()
    model.add(Dense(12, input_dim=4, init='uniform', activation='relu'))
    model.add(Dense(6, init='uniform', activation='relu'))
    model.add(Dense(1, init='uniform', activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


def main():
    """"""
    Description of main
    """"""


    iris = datasets.load_iris()
    X, y = iris.data, iris.target

    NOF_ROW, NOF_COL =  X.shape

    # evaluate using 10-fold cross validation
    seed = 7
    np.random.seed(seed)
    model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10, verbose=0)
    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
    results = cross_val_score(model, X, y, cv=kfold)

    print(results.mean())
    # 0.666666666667


if __name__ == '__main__':
    main()


The pima-indians-diabetes.data  can be downloaded here.

Now what I want to do is to pass a value NOF_COL into a parameter of create_model() function the following way

model = KerasClassifier(build_fn=create_model(input_dim=NOF_COL), nb_epoch=150, batch_size=10, verbose=0)


With the create_model() function that looks like this:

def create_model(input_dim=None):
    # create model
    model = Sequential()
    model.add(Dense(12, input_dim=input_dim, init='uniform', activation='relu'))
    model.add(Dense(6, init='uniform', activation='relu'))
    model.add(Dense(1, init='uniform', activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


But it fails giving this error:

TypeError: __call__() takes at least 2 arguments (1 given)


What's the right way to do it?","You can add an input_dim keyword argument to the KerasClassifier constructor:

model = KerasClassifier(build_fn=create_model, input_dim=5, nb_epoch=150, batch_size=10, verbose=0)",11-12-2015 00:51,11-12-2015 04:45,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/44622857,keras.wrappers.scikit_learn.KerasClassifier.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><machine-learning><scikit-learn><deep-learning><keras>,7,42905.22986,42905.25556,18,21864,3374996,170,7119128,2.526315789,0,https://stackoverflow.com/questions/44622857,"This is the code and I'm getting the error in the last line only which is y_pred = classifier.predict(X_test). The error I'm getting is AttributeError: 'KerasClassifier' object has no attribute 'model'

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets
from sklearn import preprocessing
from keras.utils import np_utils

# Importing the dataset
dataset = pd.read_csv('Data1.csv',encoding = ""cp1252"")
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_0 = LabelEncoder()
X[:, 0] = labelencoder_X_0.fit_transform(X[:, 0])
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
labelencoder_X_3 = LabelEncoder()
X[:, 3] = labelencoder_X_3.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Creating the ANN!
# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
def build_classifier():
    # Initialising the ANN
    classifier = Sequential()
    # Adding the input layer and the first hidden layer
    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))

    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    return classifier

classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 2)
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 1, n_jobs=1)
mean = accuracies.mean()
variance = accuracies.std()

# Predicting the Test set results
import sklearn
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Predicting new observations
test = pd.read_csv('test.csv',encoding = ""cp1252"")
test = test.iloc[:, 1:].values
test[:, 0] = labelencoder_X_0.transform(test[:, 0])
test[:, 1] = labelencoder_X_1.transform(test[:, 1])
test[:, 2] = labelencoder_X_2.transform(test[:, 2])
test[:, 3] = labelencoder_X_3.transform(test[:, 3])
test = onehotencoder.transform(test).toarray()
test = test[:, 1:]
new_prediction = classifier.predict_classes(sc.transform(test))
new_prediction1 = (new_prediction > 0.5)","Because you haven't fitted the classifier yet. For classifier to have the model variable available, you need to call 

classifier.fit(X_train, y_train)


Although you have used cross_val_score() over the classifier, and found out accuracies, but the main point to note here is that the cross_val_score will clone the supplied model and use them for cross-validation folds. So your original estimator classifier is untouched and untrained. 

You can see the working of cross_val_score in my other answer here

So put the above mentioned line just above y_pred = classifier.predict(X_test) line and you are all set. Hope this makes it clear.",11-12-2015 04:51,11-12-2015 05:01,Add a comment,Add a comment
https://stackoverflow.com/questions/31880720," keras.layers.Activation,keras.layers.Dense",Hybrid,SL,"F,IC-2",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,BP,Keras,Model construction,Runtime Checking,0,<python><machine-learning><keras>,16,42223.61944,42223.75208,8,2256,3760780,2300,3828416,20.5,0.166666667,https://stackoverflow.com/questions/31880720,"Motivation

To run a set of labeled vectors through Keras neural network.

Example

Looking at Keras dataset example mnist:

keras.datasets import mnist
(x_tr, y_tr), (x_te, y_te) = mnist.load_data()
print x_tr.shape


It seem to be a 3 dimensional numpy array:

(60000, 28, 28)



1st dimension is for the samples
2nd and 3rd for each sample features


Attempt

Building the labeled vectors:

X_train = numpy.array([[1] * 128] * (10 ** 4) + [[0] * 128] * (10 ** 4))
X_test = numpy.array([[1] * 128] * (10 ** 2) + [[0] * 128] * (10 ** 2))

Y_train = numpy.array([True] * (10 ** 4) + [False] * (10 ** 4))
Y_test = numpy.array([True] * (10 ** 2) + [False] * (10 ** 2))

X_train = X_train.astype(""float32"")
X_test = X_test.astype(""float32"")

Y_train = Y_train.astype(""bool"")
Y_test = Y_test.astype(""bool"")


The training code

model = Sequential()
model.add(Dense(128, 50))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(50, 50))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(50, 1))
model.add(Activation('softmax'))

rms = RMSprop()
model.compile(loss='binary_crossentropy', optimizer=rms)

model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
          show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))

score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])


Result

Test score: 13.9705320154
Test accuracy: 1.0


Why do I get such a bad result for such a simple dataset?
Is my dataset malformed?

Thanks!","A softmax over just one output node doesn't make much sense. If you change model.add(Activation('softmax')) to model.add(Activation('sigmoid')), your network performs well.

Alternatively you can also use two output nodes, where 1, 0 represents the case of True and 0, 1 represents the case of False. Then you can use a softmax layer. You just have to change your Y_train and Y_test accordingly.",11-12-2015 12:26,11-12-2015 15:44,"Add a comment
                 |","1
            
        
        
            
                
                @c0d3rman ""if you put 5, 6, 7 through softmax, you'll get each divided by their sum - 5 / 18, 6 / 18, 7 / 18"". That is not correct. see en.wikipedia.org/wiki/Softmax_function
                
                
– Jorge Leitao
                
                
                    Commented
                    Jul 5, 2017 at 20:37
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/33474424," keras.layers.model.compile, keras.models.load_weights, keras.models.predict,keras.layers.add",Hybrid,SL,"F,F",AMO(Level-2),Missing Options,Crash,Keras,Prediction,Static,0,<python><neural-network><theano><keras>,8,42310.39444,42310.40278,13,30409,127480,465,4842101,2.095238095,1,https://stackoverflow.com/questions/33474424,"I'm using the Keras library to create a neural network. I have a iPython Notebook in order to load the training data, initializing the network and ""fit"" the weights of the neural network.
Finally, I save the weights using the save_weights() method.
Code is below :

from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD
from keras.regularizers import l2
from keras.callbacks import History

[...]

input_size = data_X.shape[1]
output_size = data_Y.shape[1]
hidden_size = 100
learning_rate = 0.01
num_epochs = 100
batch_size = 75

model = Sequential()
model.add(Dense(hidden_size, input_dim=input_size, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(hidden_size))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(output_size))
model.add(Activation('tanh'))

sgd = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mse', optimizer=sgd)

model.fit(X_NN_part1, Y_NN_part1, batch_size=batch_size, nb_epoch=num_epochs, validation_data=(X_NN_part2, Y_NN_part2), callbacks=[history])

y_pred = model.predict(X_NN_part2) # works well

model.save_weights('keras_w')


Then, in another iPython Notebook, I just want to use these weights and predict some outputs values given inputs. I initialize the same neural network, and then load the weights.

# same headers
input_size = 37
output_size = 40
hidden_size = 100

model = Sequential()
model.add(Dense(hidden_size, input_dim=input_size, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(hidden_size))
model.add(Activation('tanh'))
model.add(Dropout(0.2))
model.add(Dense(output_size))
model.add(Activation('tanh'))

model.load_weights('keras_w') 
#no error until here

y_pred = model.predict(X_nn)


The problem is that apparently, the load_weights method is not enough to have a functional model. I'm getting an error :

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-17-e6d32bc0d547> in <module>()
  1 
----> 2 y_pred = model.predict(X_nn)
C:\XXXXXXX\Local\Continuum\Anaconda\lib\site-packages\keras\models.pyc in predict(self, X, batch_size, verbose)
491     def predict(self, X, batch_size=128, verbose=0):
492         X = standardize_X(X)
--> 493         return self._predict_loop(self._predict, X, batch_size, verbose)[0]
494 
495     def predict_proba(self, X, batch_size=128, verbose=1):

AttributeError: 'Sequential' object has no attribute '_predict'


Any idea?
Thanks a lot.",You need to call model.compile. This can be done either before or after the model.load_weights call but must be after the model architecture is specified and before the model.predict call.,11-12-2015 19:11,11-12-2015 19:59,"Add a comment
                 |","2
            
        
        
            
                
                since this commit, it is no longer necessary to compile before prediction
                
                
– Antoine
                
                
                    Commented
                    Aug 3, 2018 at 15:08
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Compilation is not longer needed in the current version before predict
                
                
– Md Johirul Islam
                
                
                    Commented
                    Aug 14, 2018 at 16:43
                
            
        
    
    
        
            
            
        
        
            
                
                Still having this same problem, either with or without compilation.  How to fix it?
                
                
– sh37211
                
                
                    Commented
                    Jan 21, 2021 at 2:21
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/34716454," keras.layers.Activation, keras.layers.BatchNormalization,keras.layers.Dense",AMO,F,F,F,Missing Required State-specific Method Order,BP,Keras,Model construction,Runtime Checking,0,<python><keras><neural-network><data-science><batch-normalization>,152,42380.32431,42543.94444,209,3103,4727781,5809,4984897,21.42857143,0,https://stackoverflow.com/questions/34716454,"If I want to use the BatchNormalization function in Keras, then do I need to call it once only at the beginning?

I read this documentation for it: http://keras.io/layers/normalization/

I don't see where I'm supposed to call it. Below is my code attempting to use it:

model = Sequential()
keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(64, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)


I ask because if I run the code with the second line including the batch normalization and if I run the code without the second line I get similar outputs. So either I'm not calling the function in the right place, or I guess it doesn't make that much of a difference.","As Pavel said, Batch Normalization is just another layer, so you can use it as such to create your desired network architecture.
The general use case is to use BN between the linear and non-linear layers in your network, because it normalizes the input to your activation function, so that you're centered in the linear section of the activation function (such as Sigmoid). There's a small discussion of it here
In your case above, this might look like:
# import BatchNormalization
from keras.layers.normalization import BatchNormalization

# instantiate model
model = Sequential()

# we can think of this chunk as the input layer
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))

# we can think of this chunk as the hidden layer    
model.add(Dense(64, init='uniform'))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))

# we can think of this chunk as the output layer
model.add(Dense(2, init='uniform'))
model.add(BatchNormalization())
model.add(Activation('softmax'))

# setting up the optimization of our weights 
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)

# running the fitting
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)",11/14/2015 23:58,11/15/2015 4:30,"Add a comment
                 |","29
            
        
        
            
                
                FYI apparently batch normalization works better in practice after the activation function
                
                
– Claudiu
                
                
                    Commented
                    Oct 5, 2016 at 9:39
                
            
        
    
    
        
            
                    17
            
        
        
            
                
                Hi @Claudiu, would you mind expanding on this FYI? It appears to directly contradict the answer above.
                
                
– Ben Ogorek
                
                
                    Commented
                    Oct 6, 2016 at 12:51
                
            
        
    
    
        
            
                    8
            
        
        
            
                
                @benogorek: sure thing, basically I based it entirely on the results here where placing the batch norm after the relu performed better. FWIW I haven't had success applying it one way or the other on the one net I've tried
                
                
– Claudiu
                
                
                    Commented
                    Oct 6, 2016 at 13:41
                
            
        
    
    
        
            
                    35
            
        
        
            
                
                Interesting. Just to follow up, if you continue to read on in that summary, it says that their best model [GoogLeNet128_BN_lim0606] actually has the BN layer BEFORE the ReLU.  So while BN after Activation might improve accuracy in an isolated case, when the whole model is constructed, before performed best.   Likely it's possible that placing BN after Activation could improve accuracy, but is likely problem dependent.
                
                
– Lucas Ramadan
                
                
                    Commented
                    Oct 6, 2016 at 20:55
                
                        
                            
                        
            
        
    
    
        
            
                    9
            
        
        
            
                
                @CarlThomé kind of. See this reddit comment by ReginaldIII for instance. They state: ""BN is normalizing the distribution of features coming out of a convolution, some [of] these features might be negative [and] truncated by a non-linearity like ReLU. If you normalize before activation you are including these negative values in the normalization immediately before culling them from the feature space. BN after activation will normalize the positive features without statistically biasing them with features that do not make it through to the next convolutional layer.""
                
                
– mab
                
                
                    Commented
                    Aug 29, 2017 at 8:26
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 8 more comments"
https://stackoverflow.com/questions/34717241,keras.layers.Activation,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model construction,Static,0,<python><machine-learning><neural-network><keras><data-science>,31,42380.36389,42380.53264,30,14528,179372,5809,4984897,8.75,0,https://stackoverflow.com/questions/34717241,"This is my code that works if I use other activation layers like tanh:

model = Sequential()
act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(Activation(act))
model.add(Dropout(0.15))
model.add(Dense(64, init='uniform'))
model.add(Activation('softplus'))
model.add(Dropout(0.15))
model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)


In this case, it doesn't work and says ""TypeError: 'PReLU' object is not callable"" and the error is called at the model.compile line. Why is this the case? All the non-advanced activation functions works. However, neither of the advanced activation functions, including this one, works.","The correct way to use the advanced activations like PReLU is to use it with add() method and not wrapping it using Activation class. Example:

model = Sequential()
act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(act)",11/15/2015 14:12,11/15/2015 18:34,Add a comment,"1
- If we have two dense FC layers, should we add after each of them, and If we also have dropout, what should we do? – 
fermat4214
 CommentedMar 16, 2017 at 13:59 
In the case of ReLU, it doesn't matter if you add Dropout before or after the activation (maybe performance differences only, but results will be the same). – 
Tarantula
 CommentedJun 26, 2018 at 5:52
Add a comment"
https://stackoverflow.com/questions/34717241," keras.layers.advanced_activations.PReLU,keras.layers.add",Hybrid,SL,"F,IC-1",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,Crash,Keras,Model construction,Static,0,<python><machine-learning><neural-network><keras><data-science>,31,42380.36389,42380.53264,30,14528,179372,5809,4984897,8.75,0,https://stackoverflow.com/questions/35074549,"This is my code that works if I use other activation layers like tanh:

model = Sequential()
act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(Activation(act))
model.add(Dropout(0.15))
model.add(Dense(64, init='uniform'))
model.add(Activation('softplus'))
model.add(Dropout(0.15))
model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)


In this case, it doesn't work and says ""TypeError: 'PReLU' object is not callable"" and the error is called at the model.compile line. Why is this the case? All the non-advanced activation functions works. However, neither of the advanced activation functions, including this one, works.","The correct way to use the advanced activations like PReLU is to use it with add() method and not wrapping it using Activation class. Example:

model = Sequential()
act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(act)",11/17/2015 1:22,11/17/2015 6:21,"Add a comment
                 |","1
            
        
        
            
                
                - If we have two dense FC layers, should we add after each of them, and If we also have dropout, what should we do?
                
                
– fermat4214
                
                
                    Commented
                    Mar 16, 2017 at 13:59
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                In the case of ReLU, it doesn't matter if you add Dropout before or after the activation (maybe performance differences only, but results will be the same).
                
                
– Tarantula
                
                
                    Commented
                    Jun 26, 2018 at 5:52
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/35074549," keras.models.load_weights,keras.layers.add",AMO,F,F,F,Missing Required State-specific Method Order,Crash,Keras,Load,Static,0,<python><machine-learning><keras><data-science>,75,42398.00208,42404.38403,69,1827,4116272,5809,4984897,6.484848485,0,https://stackoverflow.com/questions/35757151,"How to load a model from an HDF5 file in Keras?

What I tried:

model = Sequential()

model.add(Dense(64, input_dim=14, init='uniform'))
model.add(LeakyReLU(alpha=0.3))
model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))
model.add(Dropout(0.5))

model.add(Dense(64, init='uniform'))
model.add(LeakyReLU(alpha=0.3))
model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))
model.add(Dropout(0.5))

model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))


sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)

checkpointer = ModelCheckpoint(filepath=""/weights.hdf5"", verbose=1, save_best_only=True)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2, callbacks=[checkpointer])


The above code successfully saves the best model to a file named weights.hdf5. What I want to do is then load that model. The below code shows how I tried to do so:

model2 = Sequential()
model2.load_weights(""/Users/Desktop/SquareSpace/weights.hdf5"")


This is the error I get:

IndexError                                Traceback (most recent call last)
<ipython-input-101-ec968f9e95c5> in <module>()
      1 model2 = Sequential()
----> 2 model2.load_weights(""/Users/Desktop/SquareSpace/weights.hdf5"")

/Applications/anaconda/lib/python2.7/site-packages/keras/models.pyc in load_weights(self, filepath)
    582             g = f['layer_{}'.format(k)]
    583             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]
--> 584             self.layers[k].set_weights(weights)
    585         f.close()
    586 

IndexError: list index out of range","If you stored the complete model, not only the weights, in the HDF5 file, then it is as simple as

from keras.models import load_model
model = load_model('model.h5')",11/17/2015 3:17,11/18/2015 2:24,"Add a comment
                 |","does a model include the actual training data as well when calculating the model's memory footprint?  How could you load a model that's bigger than your available memory?
                
                
– user798719
                
                
                    Commented
                    May 12, 2017 at 8:37
                
            
        
    
    
        
            
            
        
        
            
                
                A model does NOT (explicitly) include the training data. You can't load a model which is bigger than your available memory (well, ok, it is possible but this will be quite difficult and you will need to go through that yourself... but if your model is too big to load you should (a) get more memory or (b) train a smaller model)
                
                
– Martin Thoma
                
                
                    Commented
                    May 12, 2017 at 8:47
                
            
        
    
    
        
            
            
        
        
            
                
                @MartinThoma I am using the method suggested by you. I'm trying to get one layer out of the loaded model and trying to see it's weights by:   encoder = autoencoder.layers[0] encoder.get_weights()   But I'm getting:   FailedPreconditionError: Attempting to use uninitialized value lstm_1/kernel
                
                
– shubhamsingh
                
                
                    Commented
                    Nov 13, 2017 at 12:10
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                I appreciate the compliment :-) To make a point for the accepted answer: I could imagine that storing only the weights is more robust. If keras changes, the weights could still be imported while the complete thing cannot be imported. On the other hand, one can install an old version, dump the weights and do the same as before.
                
                
– Martin Thoma
                
                
                    Commented
                    Jun 22, 2018 at 10:55
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @pr338 Please consider updating your accepted answer.
                
                
– Kris
                
                
                    Commented
                    Jun 12, 2019 at 4:31
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/36136562,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Train,Static,0,<python><keras><recurrent-neural-network>,9,42450.68542,42459.42153,8,1262,6099650,2798,5606352,7.357142857,0,https://stackoverflow.com/questions/36241731,"I'm trying to run this SimpleRNN:

model.add(SimpleRNN(init='uniform',output_dim=1,input_dim=len(pred_frame.columns)))
model.compile(loss=""mse"", optimizer=""sgd"")
model.fit(X=predictor_train, y=target_train, batch_size=len(pred_frame.index),show_accuracy=True)


The error is on model.fit, as you can see below:

File ""/Users/file.py"", line 1496, in Pred
model.fit(X=predictor_train, y=target_train, batch_size=len(pred_frame.index),show_accuracy=True)
File ""/Library/Python/2.7/site-packages/keras/models.py"", line 581, in fit
shuffle=shuffle, metrics=metrics)
File ""/Library/Python/2.7/site-packages/keras/models.py"", line 239, in _fit
outs = f(ins_batch)
File ""/Library/Python/2.7/site-packages/keras/backend/theano_backend.py"", line 365, in __call__
return self.function(*inputs)
File ""/Library/Python/2.7/site-packages/theano/compile/function_module.py"", line 513, in __call__
allow_downcast=s.allow_downcast)
File ""/Library/Python/2.7/site-packages/theano/tensor/type.py"", line 169, in filter
data.shape))
TypeError: ('Bad input argument to theano function with name ""/Library/Python/2.7/site-packages/keras/backend/theano_backend.py:362""  at index 0(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (88, 88).')


The error is telling me it's got the wrong number of dimensions, it should be 3 and it has only got 2. What are the dimensions it is referring to?","You are trying to run a RNN. This means that you want to include previous time steps in your calculation. In order to do so, you have to preprocess your data before giving it to the SimpleRNN layer.

For simplicity, let us assume that instead of 88 samples with 88 features each you have 8 samples with 4 features each. Now, when using a RNN you will have to decide on a maximum for the backpropagation (i.e. number of previous time steps that are included in the calculation). In this case, you could choose to include a maximum of 2 previous time steps. Therefore, for the calculation of the weights of the RNN you will have to provide at each time step the input of the current time step (with its 4 features) and the input of the 2 previous time steps (with 4 features each). Just like in this visualization:

sequence    sample0  sample1  sample2  sample3  sample4  sample5  sample6 sample7       
   0        |-----------------------|
   1                 |-----------------------|
   2                          |-----------------------|
   3                                   |-----------------------|
   4                                             |----------------------|
   5                                                      |----------------------|


So instead of giving a (nb_samples, nb_features) matrix as an input to the SimpleRNN, you will have to give it a (nb_sequences, nb_timesteps, nb_features) shaped input. In this example, it means that instead of giving a (8x4) input you give it a (5x3x4) input.

The keras Embedding layer might do this job but in this case you can also write a short code for it:

input = np.random.rand(8,4)
nb_timesteps = 3    # 2 (previous) + 1 (current)
nb_sequences = input.shape[0] - nb_timesteps    #8-3=5

input_3D = np.array([input[i:i+nb_timesteps] for i in range(nb_sequences)])",11/18/2015 16:57,11/18/2015 17:09,"Add a comment
                 |","Thanks for the explanation, I have a similar problem. Why the maximum steps for the backpropagation in the case you are commenting are only 2? And why the number of sequences are 5? By the way, with sequence you mean an epoch in the training?
                
                
– David
                
                
                    Commented
                    Sep 23, 2016 at 9:05
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                I randomly chose the number 2 as the amount of previous steps for the backpropagation for this example. In combination with the (also freely chosen) number of 8 samples, this leads to a total of 5 sequences. Sequence 1 includes sample 1, 2 and 3, sequence 2 includes 2, 3 and 4 and so on. See the visualization in my answer for details. One epoch is the done when the network has been trained with each sequence once. Then you begin with the first sequence again
                
                
– Lorrit
                
                
                    Commented
                    Sep 27, 2016 at 17:23
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/36241731,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Train,Static,0,<python><neural-network><keras>,10,42455.94444,42458.62083,6,2837,3427404,2798,5606352,5.820833333,0,https://stackoverflow.com/questions/36392966,"I'm new to Keras and having some trouble with shapes, specially when it comes to RNNs and LSTMs.

I'm running this code:

model.add(SimpleRNN(init='uniform',output_dim=1,input_dim=len(pred_frame.columns)))
model.compile(loss=""mse"", optimizer=""sgd"")
model.fit(X=predictor_train, y=target_train, batch_size=len(pred_frame.index),show_accuracy=True)


The variable predictor_train is a numpy array with 119 inner arrays, each one having 80 different items.

I'm having this error:

TypeError: ('Bad input argument to theano function with name ""/Library/Python/2.7/site-packages/keras/backend/theano_backend.py:362""  at index 0(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (119, 80).')


So far what I found out is that an RNN receives 3D tensor with shape of (batch_size, timesteps, dimension) and when you set input_shape the batch_size is usually omitted, and you should just provide a tuple of (timesteps, dimension). But in what part of the code should that be changed (if possible, add your code changing suggestions)? 



Extra info


  About pred_frame


type: class 'pandas.core.frame.DataFrame'

shape: (206,80)

                  Pred      Pred         Pred  ...    
Date                                                                      
1999-01-01         NaN       NaN          NaN         
1999-02-01         NaN       NaN          NaN        
1999-03-01         NaN       NaN          NaN       
1999-04-01         NaN       NaN          NaN
...
2015-11-01  288.333333 -0.044705   589.866667
2015-12-01  276.333333 -0.032157  1175.466667    
2016-01-01  282.166667  0.043900  1458.966667     
2016-02-01  248.833333 -0.082199  5018.966667   
[206 rows x 80 columns]



  About target_train


type: class 'pandas.core.series.Series'

shape: (119,)

dtype: float64

Date
2004-10-01    0.003701
2005-05-01    0.001715
2005-06-01    0.002031
2005-07-01    0.002818
...
2015-05-01   -0.007597
2015-06-01   -0.007597
2015-07-01   -0.007597
2015-08-01   -0.007597



  About predictor_train


type: 'numpy.ndarray'

shape: (119,80)

dtype: float64

[[  0.00000000e+00  -1.00000000e+00   1.03550000e-02 ...,   8.42105263e-01
    6.50000000e+01  -3.98148148e-01]
 [ -1.13600000e-02  -1.07482052e+00  -9.25333333e-03 ...,   4.45783133e-01
    8.30000000e+01  -1.94915254e-01]
 [  4.71300000e-02  -5.14876761e+00   1.63166667e-03 ...,   4.45783133e-01
    8.50000000e+01  -1.94915254e-01]
 ..., 
 [  4.73500000e-02  -1.81092653e+00  -8.54000000e-03 ...,   1.39772727e+00
    2.77000000e+02  -3.43601896e-01]
 [ -6.46000000e-03  -1.13643083e+00   1.06100000e-02 ...,   2.22551929e-01
    2.77000000e+02  -3.43601896e-01]
 [  3.14200000e-02  -5.86377709e+00   1.50850000e-02 ...,   2.22551929e-01
    2.82000000e+02  -2.76699029e-01]]


Edit

Thanks to @y300 apparently the 3d problem is surpassed. My shape now is (119,1,80).

model.summary() returns the following:
--------------------------------------------------------------------------------
Initial input shape: (None, None, 119)
--------------------------------------------------------------------------------
Layer (name)                  Output Shape                  Param #             
--------------------------------------------------------------------------------
SimpleRNN (Unnamed)           (None, 1)                     121                 

Total params: 121


However, I'm still getting a shaping problem in the model.fit line, as you can see below:

File ""/Library/Python/2.7/site-packages/theano/tensor/blas.py"", line 1612, in perform
z[0] = numpy.asarray(numpy.dot(x, y))
ValueError: ('shapes (119,80) and (119,1) not aligned: 80 (dim 1) != 119 (dim 0)', (119, 80), (119, 1))
Apply node that caused the error: Dot22(Alloc.0, <TensorType(float32, matrix)>)
Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]
Inputs shapes: [(119, 80), (119, 1)]
Inputs strides: [(320, 4), (4, 4)]
Inputs values: ['not shown', 'not shown']


Why is it happening and how could I fix it?","You can check what your model looks like by doing

model.summary()


In this case, your should look something like this (actual values may differ):

--------------------------------------------------------------------------------
Initial input shape: (None, None, 100)
--------------------------------------------------------------------------------
Layer (name)                  Output Shape                  Param #             
--------------------------------------------------------------------------------
SimpleRNN (simplernn)         (None, 1)                     102                 
  --------------------------------------------------------------------------------
Total params: 102
--------------------------------------------------------------------------------


As you can see, the input is a 3D tensor, not a 2D one. So you need to reshape your arrays to fit what keras is expecting. In particular, the input X_train should have dimensions (num_samples,1,input_dim). Here's a working example with some randomly generated x/y data:

model.add(keras.layers.SimpleRNN(init='uniform',output_dim=1,input_dim=100))
model.compile(loss=""mse"", optimizer=""sgd"")
X_train = np.random.rand(300,1,100)
y_train = np.random.rand(300)
model.fit(X=X_train, y=y_train, batch_size=32,show_accuracy=True)",11/18/2015 19:45,11/21/2015 17:53,"Add a comment
                 |","Thanks for that @y300, apparently I was able to create a 3d tensor  with shape (119,1,80). However I'm still getting a shape error described above in the edit I made to the question. Do you have any idea where is this shape error happening?
                
                
– aabujamra
                
                
                    Commented
                    Mar 30, 2016 at 22:23
                
            
        
    
    
        
            
            
        
        
            
                
                Have you worked out your issue ? From the model summary Initial input shape: (None, None, 119) it seems like it expects a feature vector of length 119, but yours is length 80.
                
                
– yhenon
                
                
                    Commented
                    Mar 31, 2016 at 14:30
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/36392966,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Train,Static,0,<python><rgb><theano><conv-neural-network><keras>,6,42464.02153,42464.61319,6,2837,3427404,220,5390189,5.820833333,3,https://stackoverflow.com/questions/36556868,"I'm trying to run a CNN similar to the one in the Keras documantation ""VGG-like convnet"" but for a custom set of images and binary classification instead of a 10-class output.

When I try to fit the CNN, i get this longwinded error that I assume is telling me my input image size is not the right size for the CNN input.

ValueError: GpuDnnConv images and kernel must have the same stack size

Apply node that caused the error: GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})
Toposort index: 130
Inputs types: [CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), <theano.gof.type.CDataType object at 0x7f0eefc8d790>, Scalar(float32), Scalar(float32)]
Inputs shapes: [(32, 232, 300, 3), (300, 1, 3, 3), (32, 300, 298, 1), 'No shapes', (), ()]
Inputs strides: [(208800, 900, 3, 1), (9, 0, 3, 1), (89400, 298, 1, 0), 'No strides', (), ()]
Inputs values: ['not shown', 'not shown', 'not shown', <PyCObject object at 0x7f0efaba8e68>, 1.0, 0.0]
Inputs name: ('image', 'kernel', 'output', 'descriptor', 'alpha', 'beta')


The thing is I thought I reshaped all my images to fit. My input is a stack of 4000 232x300 px RBG images and the output is an array of 4000 boolean values.

Input: im_list.shape
Out[49]: (4000, 232, 300, 3)

Output: np.asarray(cls).shape
Out[50]: (4000,)

This is the function to build the CNN

CNN = buildCNN(3, 232, 300, 2)
CNN.fit(im_list, cls, batch_size=32, nb_epoch=1)

    def buildCNN(depth,width,height,outputShape):
    CNN = Sequential()
        # input: 232x300 images with 3 channels -> (3, 100, 100) tensors.
        # this applies 32 convolution filters of size 3x3 each.
        CNN.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(depth,width,height)))
        CNN.add(Activation('relu'))
        CNN.add(Convolution2D(32, 3, 3))
        CNN.add(Activation('relu'))
        CNN.add(MaxPooling2D(pool_size=(2, 2)))
        CNN.add(Dropout(0.25))
        #
        CNN.add(Convolution2D(64, 3, 3, border_mode='valid'))
        CNN.add(Activation('relu'))
        CNN.add(Convolution2D(64, 3, 3))
        CNN.add(Activation('relu'))
        CNN.add(MaxPooling2D(pool_size=(2, 2)))
        CNN.add(Dropout(0.25))
        #
        CNN.add(Flatten())
        # Note: Keras does automatic shape inference.
        CNN.add(Dense(256))
        CNN.add(Activation('relu'))
        CNN.add(Dropout(0.5))
        #
        CNN.add(Dense(outputShape))
        CNN.add(Activation('softmax'))
        #
        sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
        CNN.compile(loss='categorical_crossentropy', optimizer=sgd)
        #
    return CNN


I've been banging my head against the wall with this long enough that I thought maybe someone else has had this problem. Any thoughts? Thanks in advance.","You specified input as (depth,width,height). So the array you must must have dimensions (N,depth,width,height), where N is the number of training examples. 

The input you are actually passing, (4000, 232, 300, 3), doesn't match. It should be reshaped to be (4000, depth, width, height). This means you'll have to resize each image, and reorder the axes.",11/19/2015 17:52,11/19/2015 19:03,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/36556868,keras.models.evaluate,SAM,DT,PT,PT,Unacceptable Input Type,Crash,Keras,Model Evaluation,Static,0,<python><machine-learning><keras><neural-network><deep-learning>,6,42471.78472,42471.79167,10,28691,5974433,231,4451856,3.18125,0,https://stackoverflow.com/questions/36599237,"The following line of code gives me the above error in Keras. model is a Graph model in Keras.

score, acc = model.evaluate({
    'input1': X_test1,
    'input2': X_test2,
    'output':Y_test}, batch_size=450)


but when I change it to the following, it runs fine.

predictions = model.predict({
    'input1': X_test1,
    'input2': X_test2}, batch_size=450)['output']


The Y_test here is a <type 'numpy.ndarray'> of <type 'numpy.ndarray'>. A one-hot encoded vector. 

Sample Y_test:

[[1.,0.,0.],[1.,0.,0.],[0.,0.,1.]]","As you can see here :

https://github.com/fchollet/keras/blob/master/keras/engine/training.py

The evaluate method returns only test loss (or losses). So assigning result of this method to a pair results in error.",11/20/2015 18:41,11/20/2015 19:26,"Add a comment
                 |","1
            
        
        
            
                
                Thank you, for pointing that out. It seemed to work fine for the Sequential model, but now I am guessing that it returns two metrics.
                
                
– a'-
                
                
                    Commented
                    Apr 11, 2016 at 19:07
                
            
        
    
    
        
            
            
        
        
            
                
                Actually, it returns the test loss and any metrics requested. Assigning the result to a pair score, acc would have worked had the accuracy metric been requested from the model fitting
                
                
– Shadi
                
                
                    Commented
                    Sep 19, 2017 at 13:22
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                This is an quite old answer. Keras changed a lot since this time.
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Sep 19, 2017 at 15:44
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/36599237,keras.models.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Train,Static,0,<python><machine-learning><keras>,12,42473.54236,42491.96528,6,549,1536437,123,6198808,3.666666667,0,https://stackoverflow.com/questions/36886711,"I use jupyter notebook with anaconda. I use kerast firstly, and i can't do tutorial. About this issues are two themes in stackoverflow, but solve not found.

My code:

model = Sequential()
model.add(Dense(1, input_dim=1, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

X_train_shape = X_train.reshape(len(X_train), 1)
Y_train_shape = Y_train.reshape(len(Y_train), 1)
model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)


And I have error, it's some random and sometimes one or two epoch competed:


  Epoch 1/5  4352/17500 [======>.......................]
  
  --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
  last)  in ()
        2 # of 32 samples
        3 #sleep(0.1)
  ----> 4 model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)
        5 #sleep(0.1)
  
  C:\Anaconda3\envs\py27\lib\site-packages\keras\models.pyc in fit(self,
  x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, **kwargs)
      395                               shuffle=shuffle,
      396                               class_weight=class_weight,
  --> 397                               sample_weight=sample_weight)
      398 
      399     def evaluate(self, x, y, batch_size=32, verbose=1,
  
  C:\Anaconda3\envs\py27\lib\site-packages\keras\engine\training.pyc in
  fit(self, x, y, batch_size, nb_epoch, verbose, callbacks,
  validation_split, validation_data, shuffle, class_weight,
  sample_weight)    1009                               verbose=verbose,
  callbacks=callbacks,    1010

  val_f=val_f, val_ins=val_ins, shuffle=shuffle,
  -> 1011                               callback_metrics=callback_metrics)    1012     1013     def
  evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):
  
  C:\Anaconda3\envs\py27\lib\site-packages\keras\engine\training.pyc in
  _fit_loop(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)
      753                     batch_logs[l] = o
      754 
  --> 755                 callbacks.on_batch_end(batch_index, batch_logs)
      756 
      757                 epoch_logs = {}
  
  C:\Anaconda3\envs\py27\lib\site-packages\keras\callbacks.pyc in
  on_batch_end(self, batch, logs)
       58         t_before_callbacks = time.time()
       59         for callback in self.callbacks:
  ---> 60             callback.on_batch_end(batch, logs)
       61         self._delta_ts_batch_end.append(time.time() - t_before_callbacks)
       62         delta_t_median = np.median(self._delta_ts_batch_end)
  
  C:\Anaconda3\envs\py27\lib\site-packages\keras\callbacks.pyc in
  on_batch_end(self, batch, logs)
      187         # will be handled by on_epoch_end
      188         if self.verbose and self.seen < self.params['nb_sample']:
  --> 189             self.progbar.update(self.seen, self.log_values)
      190 
      191     def on_epoch_end(self, epoch, logs={}):
  
  C:\Anaconda3\envs\py27\lib\site-packages\keras\utils\generic_utils.pyc
  in update(self, current, values)
      110                 info += ((prev_total_width - self.total_width) * "" "")
      111 
  --> 112             sys.stdout.write(info)
      113             sys.stdout.flush()
      114 
  
  C:\Anaconda3\envs\py27\lib\site-packages\ipykernel\iostream.pyc in
  write(self, string)
      315 
      316             is_child = (not self._is_master_process())
  --> 317             self._buffer.write(string)
      318             if is_child:
      319                 # newlines imply flush in subprocesses
  
  ValueError: I/O operation on closed file","Change your verbose level in 
model.fit()
to 
    verbose=0.

See github.com/fchollet/keras/issues/2110

It's not a straight-on ""fix"", but it should help alleviate a race condition associated with updating of the iPython console.",11/21/2015 16:58,5/29/2016 15:23,"4
            
        
        
            
                
                Does the problem go away if you change your verbose level in model.fit() to verbose=0? See github.com/fchollet/keras/issues/2110
                
                
– Amw 5G
                
                
                    Commented
                    Apr 18, 2016 at 16:53
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                This worked for me. Thanks for posting it.
                
                
– jss367
                
                
                    Commented
                    Apr 22, 2016 at 12:24
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                @Amw5G Could you post that as an answer?
                
                
– 1''
                
                
                    Commented
                    May 1, 2016 at 18:56
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/36895627,keras.models.fit,SAM,DT,BIT,BIT,Unacceptable Input Type,IF,Keras,Train,Static,0,<python><subclassing><keras>,6,42487.68472,42487.95625,10,41498,349130,2798,5606352,0.820833961,0,https://stackoverflow.com/questions/36952763,"I'm using Keras to predict a time series. As standard I'm using 20 epochs. I want to know what did my neural network predict for each one of the 20 epochs.

By using model.predict I'm getting only one prediction among all epochs (not sure how Keras select it). I want all predictions, or at least the 10 best.

According to a previous answer I got, I should compute the predictions after each training epoch by implementing an appropriate callback by subclassing Callback() and calling predict on the model inside the on_epoch_end function.

Well, the theory seems in shape but I'm in trouble to code that. Would anyone be able to give a code example on that?

Not sure how to implement the Callback() subclassing and neither how to mix that with the model.predict inside an on_epoch_end  function.

Your help will be highly appreciated :)



EDIT

Well, I evolved a little bit.
Found out how to create the subclass and how to link it to the model.predict.
However, I'm burning my brain on how to create a list with all the predictions. Below is my current code:

#Creating a Callback subclass that stores each epoch prediction
class prediction_history(Callback):
    def on_epoch_end(self, epoch, logs={}):
        self.predhis=(model.predict(predictor_train))

#Calling the subclass
predictions=prediction_history()

#Executing the model.fit of the neural network
model.fit(X=predictor_train, y=target_train, nb_epoch=2, batch_size=batch,validation_split=0.1,callbacks=[predictions]) 

#Printing the prediction history
print predictions.predhis


However, all I'm getting with that is a list of predictions of the last epoch (same effect as printing model.predict(predictor_train)).

The question now is: How do I adapt my code so it adds to predhis  the predictions of each one of the epochs?","You are overwriting the prediction for each epoch, that is why it doesn't work. I would do it like this:

class prediction_history(Callback):
    def __init__(self):
        self.predhis = []
    def on_epoch_end(self, epoch, logs={}):
        self.predhis.append(model.predict(predictor_train))


This way self.predhis is now a list and each prediction is appended to the list at the end of each epoch.",11/24/2015 9:04,11/24/2015 18:27,"Add a comment
                 |","Outstanding. Thanks a lot for all the help Matias.
                
                
– aabujamra
                
                
                    Commented
                    Apr 27, 2016 at 23:09
                
            
        
    
    
        
            
            
        
        
            
                
                I have tried using the code snippet, but in the ""def on_batch_end()"" part of the Callback. But when doing this, model.predict(predictor_train) evaluates the model on the entire training set instead of just the current batch. Is there a way to changes this?
                
                
– Lemon
                
                
                    Commented
                    Nov 2, 2017 at 15:50
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @Lemon There is no ""current batch"" at the end of the epoch, you just have to change predictor_train to whatever data you want to use for evaluation.
                
                
– Dr. Snoopy
                
                
                    Commented
                    Nov 3, 2017 at 9:34
                
            
        
    
    
        
            
            
        
        
            
                
                For more detail see the official Keras documentation on callbacks, keras.io/callbacks
                
                
– mrk
                
                
                    Commented
                    Nov 26, 2019 at 17:03
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/36952763,keras.models.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,IF,Keras,Train,Static,0,<python><neural-network><nlp><deep-learning><keras>,42,42490.36458,42490.625,24,2335,5310324,2335,5310324,25,25,https://stackoverflow.com/questions/36966392,"Using Anaconda Python 2.7 Windows 10.

I am training a language model using the Keras exmaple:

print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

def sample(a, temperature=1.0):
    # helper function to sample an index from a probability array
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))


# train the model, output generated text after each iteration
for iteration in range(1, 3):
    print()
    print('-' * 50)
    print('Iteration', iteration)
    model.fit(X, y, batch_size=128, nb_epoch=1)
    start_index = random.randint(0, len(text) - maxlen - 1)

    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print()
        print('----- diversity:', diversity)

        generated = ''
        sentence = text[start_index: start_index + maxlen]
        generated += sentence
        print('----- Generating with seed: ""' + sentence + '""')
        sys.stdout.write(generated)

        for i in range(400):
            x = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(sentence):
                x[0, t, char_indices[char]] = 1.

            preds = model.predict(x, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()


According to Keras documentation, the model.fit method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics.

hist = model.fit(X, y, validation_split=0.2)
print(hist.history)


After training my model, if I run print(model.history) I get the error:

 AttributeError: 'Sequential' object has no attribute 'history'


How do I return my model history after training my model with the above code?

UPDATE

The issue was that:

The following had to first be defined:

from keras.callbacks import History 
history = History()


The callbacks option had to be called

model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])


But now if I print

print(history.History)


it returns

{}


even though I ran an iteration.","Just an example started from

history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0)


You can use

print(history.history.keys())


to list all data in history.

Then, you can print the history of validation loss like this: 

print(history.history['val_loss'])",11/25/2015 15:08,11/25/2015 17:28,"Could you specify if you run this code from console or do you run your script from command line (or IDE)? Do you have access to hist variable after training?
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Apr 30, 2016 at 18:32
                
            
        
    
    
        
            
            
        
        
            
                
                I'm running it off Anaconda. I have found a solution that lets me access the hist variable. But it always returns an empty curly bracket.
                
                
– ishido
                
                
                    Commented
                    Apr 30, 2016 at 23:01
                
            
        
    
    
        
            
            
        
        
            
                
                is there a way to retrieve it after the model is fit. I.e. I trained the model but did not create a new variable model.fit(). Can I obtain the loss history somehow or do I have to repeat the whole training process
                
                
– Maciek Wo?niak
                
                
                    Commented
                    Oct 22, 2021 at 23:20
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                When I do this, I only get 'acc' and 'loss', I do not see 'val_loss'
                
                
– taga
                
                
                    Commented
                    Nov 5, 2020 at 0:41
                
            
        
    
    
        
            
            
        
        
            
                
                @taga You would get both a ""train_loss"" and a ""val_loss"" if you had given the model both a training and a validation set to learn from: the training set would be used to fit the model, and the validation set could be used e.g. to evaluate the model on unseen data after each epoch and stop fitting if the validation loss ceases to decrease.
                
                
– CharlesG
                
                
                    Commented
                    Dec 5, 2020 at 13:47
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Note: calling model.predict(...) destroys all history.
                
                
– Contango
                
                
                    Commented
                    Dec 29, 2022 at 14:42
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/37084155,keras.layers.model.compile,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Unknown,Keras,Model Construction,Static,0,<python-2.7><keras>,9,42497.13958,42501.31736,33,20099,1397061,2165,1972924,3.777777778,0,https://stackoverflow.com/questions/37088571,"I am using Keras and I want to use logloss as metric for training. How I can pass that into my model?

My code is as follows:

model = Sequential()
model.add(Dense(output_dim=1000, input_dim=390, init='uniform'))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=500, input_dim=1000, init=""lecun_uniform""))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=10, input_dim=300, init=""lecun_uniform""))
model.add(Activation(""sigmoid""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=200, input_dim=10, init=""lecun_uniform""))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=100, input_dim=200, init =""glorot_normal""))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=50, input_dim=100, init =""he_normal""))
model.add(Activation(""sigmoid""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=2, input_dim=50, init = ""normal""))
model.add(Activation(""softmax""))
model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])

model.fit(train.values, y1,  nb_epoch=10,
          batch_size=50000, verbose=2,validation_split=0.3, class_weight={1:0.96, 0:0.04})


proba = model.predict_proba(train.values)
log_loss(y, proba[:,1])


How can I pass log_loss in place of accuracy?","You already are: loss='binary_crossentropy' specifies that your model should optimize the log loss for binary classification.  metrics=['accuracy'] specifies that accuracy should be printed out, but log loss is also printed out by default.",11/28/2015 17:27,11/28/2015 17:40,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/37293642,keras.models.fit,SAM,DT,RT,RT,Unacceptable Input Type,Unknown,Keras,Train,Static,0,<python><machine-learning><neural-network><conv-neural-network><keras>,73,42508.33472,42508.41389,69,2392,2115332,2392,2115332,12,12,https://stackoverflow.com/questions/37657260,"Currently I use the following code:

callbacks = [
    EarlyStopping(monitor='val_loss', patience=2, verbose=0),
    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),
]
model.fit(X_train.astype('float32'), Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
      shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),
      callbacks=callbacks)


It tells Keras to stop training when loss didn't improve for 2 epochs. But I want to stop training after loss became smaller than some constant ""THR"":

if val_loss < THR:
    break


I've seen in documentation there are possibility to make your own callback:
http://keras.io/callbacks/
But nothing found how to stop training process. I need an advice.","I found the answer. I looked into Keras sources and find out code for EarlyStopping. I made my own callback, based on it:

class EarlyStoppingByLossVal(Callback):
    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):
        super(Callback, self).__init__()
        self.monitor = monitor
        self.value = value
        self.verbose = verbose

    def on_epoch_end(self, epoch, logs={}):
        current = logs.get(self.monitor)
        if current is None:
            warnings.warn(""Early stopping requires %s available!"" % self.monitor, RuntimeWarning)

        if current < self.value:
            if self.verbose > 0:
                print(""Epoch %05d: early stopping THR"" % epoch)
            self.model.stop_training = True


And usage:

callbacks = [
    EarlyStoppingByLossVal(monitor='val_loss', value=0.00001, verbose=1),
    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),
    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),
]
model.fit(X_train.astype('float32'), Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
      shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),
      callbacks=callbacks)",12-08-2015 13:03,12-08-2015 13:36,"Add a comment
                 |","1
            
        
        
            
                
                Just if it will be useful for someone - in my case I used monitor='loss', it worked well.
                
                
– QtRoS
                
                
                    Commented
                    Feb 18, 2017 at 9:24
                
            
        
    
    
        
            
                    15
            
        
        
            
                
                It seems Keras has been updated. The EarlyStopping callback function has min_delta built into it now. No need to hack the source code anymore, yay! stackoverflow.com/a/41459368/3345375
                
                
– jkdev
                
                
                    Commented
                    Jun 21, 2017 at 4:22
                
                        
                            
                        
            
        
    
    
        
            
                    4
            
        
        
            
                
                Upon re-reading the question and answers, I need to correct myself: min_delta means ""Stop early if there is not enough improvement per epoch (or per multiple epochs)."" However, the OP asked how to ""Stop early when the loss gets below a certain level.""
                
                
– jkdev
                
                
                    Commented
                    Jun 22, 2017 at 1:49
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                NameError: name 'Callback' is not defined... How will I fix it?
                
                
– alyssaeliyah
                
                
                    Commented
                    Nov 26, 2018 at 10:13
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                Eliyah try this:  from keras.callbacks import Callback
                
                
– ZFTurbo
                
                
                    Commented
                    Nov 28, 2018 at 20:39
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 2 more comments"
https://stackoverflow.com/questions/37657260,keras.layers.model.compile,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><neural-network><deep-learning><keras><metrics>,19,42527.51181,42527.72083,11,28691,5974433,407,5190014,3.18125,0,https://stackoverflow.com/questions/37666887,"I get this error : 


  sum() got an unexpected keyword argument 'out'


when I run this code:

import pandas as pd, numpy as np
import keras
from keras.layers.core import Dense, Activation
from keras.models import Sequential

def AUC(y_true,y_pred):
    not_y_pred=np.logical_not(y_pred)
    y_int1=y_true*y_pred
    y_int0=np.logical_not(y_true)*not_y_pred
    TP=np.sum(y_pred*y_int1)
    FP=np.sum(y_pred)-TP
    TN=np.sum(not_y_pred*y_int0)
    FN=np.sum(not_y_pred)-TN
    TPR=np.float(TP)/(TP+FN)
    FPR=np.float(FP)/(FP+TN)
    return((1+TPR-FPR)/2)

# Input datasets

train_df = pd.DataFrame(np.random.rand(91,1000))
train_df.iloc[:,-2]=(train_df.iloc[:,-2]>0.8)*1


model = Sequential()
model.add(Dense(output_dim=60, input_dim=91, init=""glorot_uniform""))
model.add(Activation(""sigmoid""))
model.add(Dense(output_dim=1, input_dim=60, init=""glorot_uniform""))
model.add(Activation(""sigmoid""))

model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=[AUC])


train_df.iloc[:,-1]=np.ones(train_df.shape[0]) #bias
X=train_df.iloc[:,:-1].values
Y=train_df.iloc[:,-1].values
print X.shape,Y.shape

model.fit(X, Y, batch_size=50,show_accuracy = False, verbose = 1)


Is it possible to implement a custom metric aside from doing a loop on batches and editing the source code?","Here I'm answering to OP's topic question rather than his exact problem. I'm doing this as the question shows up in the top when I google the topic problem.

You can implement a custom metric in two ways.


As mentioned in Keras docu.


import keras.backend as K

def mean_pred(y_true, y_pred):
    return K.mean(y_pred)

model.compile(optimizer='sgd',
          loss='binary_crossentropy',
          metrics=['accuracy', mean_pred])


But here you have to remember as mentioned in Marcin Mo?ejko's answer that y_true and y_pred are tensors. So in order to correctly calculate the metric you need to use keras.backend functionality. Please look at this SO question for details How to calculate F1 Macro in Keras?
Or you can implement it in a hacky way as mentioned in Keras GH issue. For that you need to use callbacks argument of model.fit.


import keras as keras
import numpy as np
from keras.optimizers import SGD
from sklearn.metrics import roc_auc_score

model = keras.models.Sequential()
# ...
sgd = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])


class Metrics(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self._data = []

    def on_epoch_end(self, batch, logs={}):
        X_val, y_val = self.validation_data[0], self.validation_data[1]
        y_predict = np.asarray(model.predict(X_val))

        y_val = np.argmax(y_val, axis=1)
        y_predict = np.argmax(y_predict, axis=1)

        self._data.append({
            'val_rocauc': roc_auc_score(y_val, y_predict),
        })
        return

    def get_data(self):
        return self._data

metrics = Metrics()
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[metrics])
metrics.get_data()",12-10-2015 01:32,12-10-2015 02:08,"Add a comment
                 |","I suggest  using self.model rather than model so that this class can be stashed away at a different file.
                
                
– Dan Erez
                
                
                    Commented
                    Oct 29, 2018 at 10:57
                
            
        
    
    
        
            
                    9
            
        
        
            
                
                @For people working with large validation dataset, you will face twice the validation time. One validation done by keras and one done by your metrics by calling predict. Another issue is now your metrics uses GPU to do predict and cpu to compute metrics using numpy, thus GPU and CPU are in serial. If metric is compute expensive, you will face worse GPU utilization and will have to do optimization that are already done in keras.
                
                
– saurabheights
                
                
                    Commented
                    Aug 1, 2019 at 1:39
                
            
        
    
    
        
            
            
        
        
            
                
                @saurabheights any workarounds for this then using callbacks
                
                
– Likith Reddy
                
                
                    Commented
                    Dec 19, 2020 at 11:37
                
            
        
    
    
        
            
            
        
        
            
                
                I don't think the validation_data will be passed to the on_epoch_end method when I use this code, it says None type not subscriptable
                
                
– Likith Reddy
                
                
                    Commented
                    Dec 19, 2020 at 14:18
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                This technique works well. Hint: Use y_predict = np.asarray(model.predict(X_val, batch_size=32768)) to drop prediction time, in my  case it went from 14 seconds to 0.40 seconds.
                
                
– Contango
                
                
                    Commented
                    Jan 8, 2021 at 23:06
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 3 more comments"
https://stackoverflow.com/questions/37666887,keras.models.predict_classes,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Prediction,Static,0,<python><python-3.x><tensorflow><reshape><keras>,6,42527.87847,42527.9125,6,22889,5098368,8732,445142,6.31986932,7,https://stackoverflow.com/questions/37984304,"I'm following the Keras CIFAR10 tutorial here.
The only changes I made were:

[a] added to the end of tutorial file

model.save_weights('./weights.h5', overwrite=True)


[b] changed ~./keras/keras.json to

{""floatx"": ""float32"", ""backend"": ""tensorflow"", ""epsilon"": 1e-07}


I can run the model successfully.

Then I want to test a single image against the trained model. My code:

[... similar to tutorial file with model being created and compiled...]
...
model = Sequential()
...
model.compile()

model.load_weights('./ddx_weights.h5')

img = cv2.imread('car.jpeg', -1) # this is is a 32x32 RGB image
img = np.array(img)
y_pred = model.predict_classes(img, 1)
print(y_pred)


I get this error:

ValueError: Cannot feed value of shape (1, 32, 3) for Tensor 'convolution2d_input_1:0', which has shape '(?, 3, 32, 32)'


What is the correct way of reshaping the input data for a single image to be tested?

I have not added ""image_dim_ordering"": ""tf"" in ./keras/keras.json.","You have to reshape the input image to have a shape of [?, 3, 32, 32] where ? is the batch size. In your case, since you have 1 image the batch size is 1, so you can do:

img = np.array(img)
img = img.reshape((1, 3, 32, 32))",12-10-2015 05:12,12-10-2015 10:47,"1
            
        
        
            
                
                care to explain the down vote? that would help for future posting.
                
                
– pepe
                
                
                    Commented
                    Jun 6, 2016 at 21:16
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/38294046,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,IF,Keras,Train,Static,0,<python><neural-network><keras><recurrent-neural-network>,7,42561.68472,42561.85833,17,1506,5486261,73,3406687,3.8,0,https://stackoverflow.com/questions/38445982,"I am trying to code a very simple RNN example with keras but the results are not as expected.

My X_train is a repeated list with length 6000 like: 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...

I formatted this to shape: (6000, 1, 1)

My y_train is a repeated list with length 6000 like: 1, 0.8, 0.6, 0, 0, 0, 1, 0.8, 0.6, 0, ...

I formatted this to shape: (6000, 1)

In my understanding, the recurrent neural network should learn to predict the 0.8 and 0.6 correctly because it can remember the 1 in X_train two timesteps ago.

My model:

model=Sequential()
model.add(SimpleRNN(input_dim=1, output_dim=50))
model.add(Dense(output_dim=1, activation = ""sigmoid""))
model.compile(loss=""mse"", optimizer=""rmsprop"")
model.fit(X_train, y_train, nb_epoch=10, batch_size=32)


The model can be trained successfully with minimal loss ~0.1015 but the results are not as expected.

test case ---------------------------------------------  model result -------------expected result 

model.predict(np.array([[[1]]])) --------------------0.9825--------------------1

model.predict(np.array([[[1],[0]]])) ----------------0.2081--------------------0.8

model.predict(np.array([[[1],[0],[0]]])) ------------0.2778 -------------------0.6

model.predict(np.array([[[1],[0],[0],[0]]]))---------0.3186--------------------0


Any hints what I am misunderstanding here?","The input format should be three-dimensional: the three components represent sample size, number of time steps and output dimension

Once appropriately reformatted the RNN does indeed manage to predict the target sequence well.

np.random.seed(1337)

sample_size = 256
x_seed = [1, 0, 0, 0, 0, 0]
y_seed = [1, 0.8, 0.6, 0, 0, 0]

x_train = np.array([[x_seed] * sample_size]).reshape(sample_size,len(x_seed),1)
y_train = np.array([[y_seed]*sample_size]).reshape(sample_size,len(y_seed),1)

model=Sequential()
model.add(SimpleRNN(input_dim  =  1, output_dim = 50, return_sequences = True))
model.add(TimeDistributed(Dense(output_dim = 1, activation  =  ""sigmoid"")))
model.compile(loss = ""mse"", optimizer = ""rmsprop"")
model.fit(x_train, y_train, nb_epoch = 10, batch_size = 32)

print(model.predict(np.array([[[1],[0],[0],[0],[0],[0]]])))
#[[[ 0.87810659]
#[ 0.80646527]
#[ 0.61600274]
#[ 0.01652312]
#[ 0.00930419]
#[ 0.01328572]]]",12-11-2015 13:37,12/17/2015 18:38,"Add a comment
                 |","1
            
        
        
            
                
                + Functional, my first run of a RNN model :)
                
                
– Masud Rahman
                
                
                    Commented
                    Jul 29, 2018 at 5:38
                
            
        
    
    
        
            
            
        
        
            
                
                Giving +1 for 1337, keeping those traditions alive.
                
                
– Mindaugas Bernatavi?ius
                
                
                    Commented
                    Dec 7, 2020 at 13:57
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/38656566,keras.layers.add.Conv1D,SAM,DT,MT,MT,Unacceptable Input Type,IF,Keras,Model Construction,Static,0,<python><neural-network><theano><conv-neural-network><keras>,22,42580.4375,42581.48194,16,28691,5974433,791,6555213,3.18125,0,https://stackoverflow.com/questions/38714959,"really finding it hard to understand the input dimensions to the convolutional 1d layer in keras:

Input shape

3D tensor with shape: (samples, steps, input_dim).

Output shape

3D tensor with shape: (samples, new_steps, nb_filter). steps value might have changed due to padding.

I want my network to take in a time series of prices (101, in order) and output 4 probabilities. My current non-convolutional network which does this fairly well (with a training set of 28000) looks like this:

standardModel = Sequential()
standardModel.add(Dense(input_dim=101, output_dim=100, W_regularizer=l2(0.5), activation='sigmoid'))
standardModel.add(Dense(4, W_regularizer=l2(0.7), activation='softmax'))


To improve this, I want to make a feature map from the input layer which has a local receptive field of length 10. (and therefore has 10 shared weights and 1 shared bias). I then want to use max pooling and feed this in to a hidden layer of 40 or so neurons and then output this with 4 neurons with softmax in the outer layer. 

picture (it's quite awful sorry!)

So ideally, the convolutional layer would take a 2d tensor of dimensions:

(minibatch_size, 101)

and output a 3d tensor of dimensions

(minibatch_size, 91, no_of_featuremaps)

However, the keras layer seems to require a dimension in the input called step. I've tried understanding this and still don't quite get it. In my case, should step be 1 as each step in the vector is an increase in the time by 1? Also, what is new_step? 

In addition, how do you turn the output of the pooling layers (a 3d tensor) into input suitable for the standard hidden layer (i.e a Dense keras layer) in the form of a 2d tensor?

Update: After the very helpful suggestions given, I tried making a convolutional network like so:

conv = Sequential()
conv.add(Convolution1D(64, 10, input_shape=(1,101)))
conv.add(Activation('relu'))
conv.add(MaxPooling1D(2))
conv.add(Flatten())
conv.add(Dense(10))
conv.add(Activation('tanh'))
conv.add(Dense(4))
conv.add(Activation('softmax'))


The line conv.Add(Flatten()) throws a range exceeds valid bounds error. Interestingly, this error is not thrown for just this code:

conv = Sequential()
conv.add(Convolution1D(64, 10, input_shape=(1,101)))
conv.add(Activation('relu'))
conv.add(MaxPooling1D(2))
conv.add(Flatten())


doing 

print conv.input_shape
print conv.output_shape


results in 

(None, 1, 101
(None, -256)


being returned

Update 2:

Changed 

conv.add(Convolution1D(64, 10, input_shape=(1,101)))


to

conv.add(Convolution1D(10, 10, input_shape=(101,1))


and it started working. However, is there any important different between 
inputting (None, 101, 1) to a 1d conv layer or (None, 1, 101) that I should be aware of? Why does (None, 1, 101) not work?","The reason why it look like this is that Keras designer intended to make 1-dimensional convolutional framework to be interpreted as a framework to deal with sequences. To fully understand the difference - try to imagine that you have a sequence of a multiple feature vectors. Then your output will be at least two dimensional - where first dimension is connected with time and other dimensions are connected with features. 1-dimensional convolutional framework was designed to in some way bold this time dimension and try to find the reoccuring patterns in data - rather than performing a classical multidimensional convolutional transformation.

In your case you must simply reshape your data to have shape (dataset_size, 101, 1) - because you have only one feature. It could be easly done using numpy.reshape function. To understand what does a new step mean - you must understand that you are doing the convolution over time - so you change the temporal structure of your data - which lead to new time-connected structure. In order to get your data to a format which is suitable for dense / static layers use keras.layers.flatten layer - the same as in classic convolutional case.

UPDATE: As I mentioned before - the first dimension of input is connected with time. So the difference between (1, 101) and (101, 1) lies in that in first case you have one time step with 101 features and in second - 101 timesteps with 1 feature. The problem which you mentioned after your first change has its origin in making pooling with size 2 on such input. Having only one timestep - you cannot pool any value on a time window of size 2 - simply because there is not enough timesteps to do that.",12-12-2015 00:21,12-12-2015 06:55,"Add a comment
                 |","ah ok I sort of see. So if my data was not just price against time, but price, rainfall and market volume per time, i would give the first layer something of dimensions (sample_size, 101, 3)?
                
                
– Nick
                
                
                    Commented
                    Jul 30, 2016 at 16:46
                
            
        
    
    
        
            
            
        
        
            
                
                I also tried doing something similar to this just now, and the flatten layer is throwing a strange error (some sort of overflow?)
                
                
– Nick
                
                
                    Commented
                    Jul 30, 2016 at 16:49
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/39779710,keras.models.fit,SAM,DT,RT,RT,Unacceptable Input Type,IF,Keras,Train,Static,0,<python><optimization><keras>,8,42642.86528,42644.54792,8,2406,6901690,2406,6901690,0.322580645,0.322580645,https://stackoverflow.com/questions/39815518,"I'm setting up a Learning Rate Scheduler in Keras, using history loss as an updater to self.model.optimizer.lr, but the value on self.model.optimizer.lr does not get inserted in the SGD optimizer and the optimizer is using the dafault learning rate. The code is:

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.preprocessing import StandardScaler

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.model.optimizer.lr=3
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.model.optimizer.lr=lr-10000*self.losses[-1]

def base_model():
    model=Sequential()
    model.add(Dense(4, input_dim=2, init='uniform'))
    model.add(Dense(1, init='uniform'))
    sgd = SGD(decay=2e-5, momentum=0.9, nesterov=True)


model.compile(loss='mean_squared_error',optimizer=sgd,metrics['mean_absolute_error'])
    return model

history=LossHistory()

estimator = KerasRegressor(build_fn=base_model,nb_epoch=10,batch_size=16,verbose=2,callbacks=[history])

estimator.fit(X_train,y_train,callbacks=[history])

res = estimator.predict(X_test)


Everything works fine using Keras as a regressor for continuous variables, But I want to reach a smaller derivative by updating the optimizer learning rate.","keras.callbacks.LearningRateScheduler(schedule, verbose=0)


In new Keras API you can use more general version of schedule function which takes two arguments epoch and lr.

From docs:


  schedule: a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float).


From sources:



    try:  # new API
        lr = self.schedule(epoch, lr)
    except TypeError:  # old API for backward compatibility
        lr = self.schedule(epoch)
    if not isinstance(lr, (float, np.float32, np.float64)):
        raise ValueError('The output of the ""schedule"" function '
                         'should be float.')


So your function could be:

def lr_scheduler(epoch, lr):
    decay_rate = 0.1
    decay_step = 90
    if epoch % decay_step == 0 and epoch:
        return lr * decay_rate
    return lr

callbacks = [
    keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)
]

model.fit(callbacks=callbacks, ... )",12/30/2015 19:56,12/31/2015 9:47,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/39815518,keras.layers.MaxPooling2D,Hybrid,SL,IC-1,SAM(Level-3),Missing Options,Crash,Keras,Model Construction,Static,0,<python><neural-network><tensorflow><deep-learning><keras>,24,42645.38264,42647.49375,23,60660,147019,733,4516640,7.666666667,1.5,https://stackoverflow.com/questions/39930952,"I am trying to replicate VGG16 model in keras, the following is my code:

model = Sequential()
model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))
model.add(Convolution2D(64, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(64, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2))) ###This line gives error
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1000, activation='softmax'))


The maxpooling2d layer gives an error at the line which is commented

The error says:

ValueError: Negative dimension size caused by subtracting 2 from 1 for 'MaxPool_7' (op: 'MaxPool') with input shapes: [?,1,112,128].


What might be the reason behind this? How to solve this?

Edit:
A more detailed error log:


  
  
  ValueError                                Traceback (most recent call
  last)  in ()
       12 model.add(Convolution2D(128, 3, 3, activation='relu'))
       13 
  ---> 14 model.add(MaxPooling2D((2,2), strides=(2,2)))
       15 
       16 model.add(ZeroPadding2D((1,1)))
  
  /usr/local/lib/python2.7/dist-packages/keras/models.pyc in add(self,
  layer)
      306                  output_shapes=[self.outputs[0]._keras_shape])
      307         else:
  --> 308             output_tensor = layer(self.outputs[0])
      309             if type(output_tensor) is list:
      310                 raise Exception('All layers in a Sequential model '
  
  /usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in
  call(self, x, mask)
      512         if inbound_layers:
      513             # this will call layer.build() if necessary
  --> 514             self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
      515             input_added = True
      516 
  
  /usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in
  add_inbound_node(self, inbound_layers, node_indices, tensor_indices)
      570         # creating the node automatically updates self.inbound_nodes
      571         # as well as outbound_nodes on inbound layers.
  --> 572         Node.create_node(self, inbound_layers, node_indices, tensor_indices)
      573 
      574     def get_output_shape_for(self, input_shape):
  
  /usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in
  create_node(cls, outbound_layer, inbound_layers, node_indices,
  tensor_indices)
      147 
      148         if len(input_tensors) == 1:
  --> 149             output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
      150             output_masks = to_list(outbound_layer.compute_mask(input_tensors[0], input_masks[0]))
      151             # TODO: try to auto-infer shape if exception is raised by get_output_shape_for
  
  /usr/local/lib/python2.7/dist-packages/keras/layers/pooling.pyc in
  call(self, x, mask)
      160                                         strides=self.strides,
      161                                         border_mode=self.border_mode,
  --> 162                                         dim_ordering=self.dim_ordering)
      163         return output
      164 
  
  /usr/local/lib/python2.7/dist-packages/keras/layers/pooling.pyc in
  _pooling_function(self, inputs, pool_size, strides, border_mode, dim_ordering)
      210                           border_mode, dim_ordering):
      211         output = K.pool2d(inputs, pool_size, strides,
  --> 212                           border_mode, dim_ordering, pool_mode='max')
      213         return output
      214 
  
  /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc
  in pool2d(x, pool_size, strides, border_mode, dim_ordering, pool_mode)
  1699     1700     if pool_mode == 'max':
  -> 1701         x = tf.nn.max_pool(x, pool_size, strides, padding=padding)    1702     elif pool_mode == 'avg':    1703

  x = tf.nn.avg_pool(x, pool_size, strides, padding=padding)
  
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc
  in max_pool(value, ksize, strides, padding, data_format, name)    1391
  padding=padding,    1392

  data_format=data_format,
  -> 1393                                 name=name)    1394     1395 
  
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.pyc
  in _max_pool(input, ksize, strides, padding, data_format, name)

  1593   result = _op_def_lib.apply_op(""MaxPool"", input=input,
  ksize=ksize,    1594                                 strides=strides,
  padding=padding,
  -> 1595                                 data_format=data_format, name=name)    1596   return result    1597 
  
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc
  in apply_op(self, op_type_name, name, **keywords)
      747           op = g.create_op(op_type_name, inputs, output_types, name=scope,
      748                            input_types=input_types, attrs=attr_protos,
  --> 749                            op_def=op_def)
      750           outputs = op.outputs
      751           return _Restructure(ops.convert_n_to_tensor(outputs),
  
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc
  in create_op(self, op_type, inputs, dtypes, input_types, name, attrs,
  op_def, compute_shapes, compute_device)    2388

  original_op=self._default_original_op, op_def=op_def)    2389     if
  compute_shapes:
  -> 2390       set_shapes_for_outputs(ret)    2391     self._add_op(ret)    2392

  self._record_op_seen_by_control_dependencies(ret)
  
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc
  in set_shapes_for_outputs(op)    1783       raise RuntimeError(""No
  shape function registered for standard op: %s""    1784

  % op.type)
  -> 1785   shapes = shape_func(op)    1786   if shapes is None:    1787     raise RuntimeError(
  
  /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc
  in call_cpp_shape_fn(op, input_tensors_needed, debug_python_shape_fn)
      594                                                              status)
      595   except errors.InvalidArgumentError as err:
  --> 596     raise ValueError(err.message)
      597 
      598   # Convert TensorShapeProto values in output_shapes.
  
  ValueError: Negative dimension size caused by subtracting 2 from 1 for
  'MaxPool_7' (op: 'MaxPool') with input shapes: [?,1,112,128].","Quoting an answer mentioned in github, you need to specify the dimension ordering:

Keras is a wrapper over Theano or Tensorflow libraries. Keras uses the setting variable image_dim_ordering to decide if the input layer is Theano or Tensorflow format. This setting can be specified in 2 ways - 


specify 'tf' or 'th' in ~/.keras/keras.json like so -  image_dim_ordering: 'th'. Note: this is a json file.
or specify the image_dim_ordering in your model like so: model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=""th""))


Update: Apr 2020 Keras 2.2.5 link seems to have an updated API where dim_ordering is changed to data_format so:

keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format='channels_first') to get NCHW or use channels_last to get NHWC

Appendix: image_dim_ordering in 'th' mode the channels dimension (the depth) is at index 1 (e.g. 3, 256, 256). In 'tf' mode is it at index 3 (e.g. 256, 256, 3). Quoting @naoko from comments.",01-04-2016 15:14,01-04-2016 17:08,"Add a comment
                 |","@PranayMathur did not notice that!
                
                
– Srikar Appalaraju
                
                
                    Commented
                    Oct 5, 2016 at 17:55
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Note: dim_ordering, In 'th' mode, the channels dimension (the depth) is at index 1, in 'tf' mode is it at index 3.  For tensorflow 0.10 would use ""tf"" (default) and 0.11 would use ""th"". You can set this behavior in ~/.keras/keras.json
                
                
– naoko
                
                
                    Commented
                    Nov 29, 2016 at 16:23
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @SrikarAppalaraju In you appendix, wouldn't that be Index 0 and Index 2 instead of 1 and 3?
                
                
– Alexander Rossa
                
                
                    Commented
                    Apr 15, 2018 at 15:48
                
            
        
    
    
        
            
            
        
        
            
                
                TF2: Keyword argument not understood: 'dim_ordering'
                
                
– xdola
                
                
                    Commented
                    Apr 5, 2020 at 19:21
                
            
        
    
    
        
            
            
        
        
            
                
                is this answer valid for keras 2.2.5 ? cuz MaxPooling2D does not seem to have dim_ordering argument! keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)
                
                
– doplano
                
                
                    Commented
                    Apr 27, 2020 at 7:38
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/39950311,keras.models.predict,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Prediction,Static,0,<python><django><neural-network><artificial-intelligence><keras>,12,42653.07431,42682.59306,28,396,5020533,166,6939704,28,0,https://stackoverflow.com/questions/40046619,"I am trying to use a keras neural network to recognize canvas images of drawn digits and output the digit. I have saved the neural network and use django to run the web interface. But whenever I run it, I get an internal server error and an error on the server side code. The error says Exception: Error when checking : expected dense_input_1 to have shape (None, 784) but got array with shape (784, 1). My only main view is 

from django.shortcuts import render
from django.http import HttpResponse
import StringIO
from PIL import Image
import numpy as np
import re
from keras.models import model_from_json
def home(request):
    if request.method==""POST"":
        vari=request.POST.get(""imgBase64"","""")
        imgstr=re.search(r'base64,(.*)', vari).group(1)
        tempimg = StringIO.StringIO(imgstr.decode('base64'))
        im=Image.open(tempimg).convert(""L"")
        im.thumbnail((28,28), Image.ANTIALIAS)
        img_np= np.asarray(im)
        img_np=img_np.flatten()
        img_np.astype(""float32"")
        img_np=img_np/255
        json_file = open('model.json', 'r')
        loaded_model_json = json_file.read()
        json_file.close()
        loaded_model = model_from_json(loaded_model_json)
        # load weights into new model
        loaded_model.load_weights(""model.h5"")
        # evaluate loaded model on test data
        loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
        output=loaded_model.predict(img_np)
        score=output.tolist()
        return HttpResponse(score)
    else:
        return render(request, ""digit/index.html"")


The links I have checked out are:


Here
 Here 
and Here


Edit
Complying with Rohan's suggestion, this is my stack trace

Internal Server Error: /home/
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py"", line 149, in get_response
    response = self.process_exception_by_middleware(e, request)
  File ""/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py"", line 147, in get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File ""/home/vivek/keras/neural/digit/views.py"", line 27, in home
output=loaded_model.predict(img_np)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 671, in predict
return self.model.predict(x, batch_size=batch_size, verbose=verbose)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1161, in predict
check_batch_dim=False)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 108, in standardize_input_data
str(array.shape))
Exception: Error when checking : expected dense_input_1 to have shape (None, 784) but got array with shape (784, 1)


Also, I have my model that I used to train the network initially.

import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
(X_train, y_train), (X_test, y_test) = mnist.load_data()
for item in y_train.shape:
    print item
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
# normalize inputs from 0-255 to 0-1
X_train = X_train / 255
X_test = X_test / 255
print X_train.shape
# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
# define baseline model
def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))
    model.add(Dense(num_classes, init='normal', activation='softmax'))
    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
# build the model
model = baseline_model()
# Fit the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=20, batch_size=200, verbose=1)
# Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print(""Baseline Error: %.2f%%"" % (100-scores[1]*100))
# serialize model to JSON
model_json = model.to_json()
with open(""model.json"", ""w"") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights(""model.h5"")
print(""Saved model to disk"")


Edit
I tried reshaping the img to (1,784) and it also failed, giving the same error as the title of this question

Thanks for the help, and leave comments on how I should add to the question.","You're asking the neural network to evaluate 784 cases with one input each instead of a single case with 784 inputs. I had the same problem and I solved it having an array with a single element which is an array of the inputs. See the example below, the first one works whereas the second one gives the same error you're experiencing.

model.predict(np.array([[0.5, 0.0, 0.1, 0.0, 0.0, 0.4, 0.0, 0.0, 0.1, 0.0, 0.0]]))
model.predict(np.array([0.5, 0.0, 0.1, 0.0, 0.0, 0.4, 0.0, 0.0, 0.1, 0.0, 0.0]))


hope this solves it for you as well :)",01-06-2016 11:18,01-06-2016 16:17,"Check if this works outside of django. Post full stack trace. How did you train the model?
                
                
– Rohan
                
                
                    Commented
                    Oct 10, 2016 at 3:35
                
            
        
    
    
        
            
            
        
        
            
                
                @Rohan I added the traceback stack and the original keras file, so take I look
                
                
– Superman
                
                
                    Commented
                    Oct 10, 2016 at 13:34
                
            
        
    
    
        
            
            
        
        
            
                
                Have you try to do something like: ""img_np.reshape((None, 784))"" before  ""loaded_model.predict(img_np)"" ?
                
                
– Dror Hilman
                
                
                    Commented
                    Oct 14, 2016 at 3:32
                
            
        
    
    
        
            
            
        
        
            
                
                @Dror Hillman I have tried this, but it then give the error ""Value must be an integer"" and fails, so this doesn't work
                
                
– Superman
                
                
                    Commented
                    Oct 14, 2016 at 15:15
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                I think your network needs to receive the pixels one by one, or at least you are passing the X_train as the training set as a dataset of shape (784,1). That is 784 samples of one pixel. But you have declared your input_dim as 784 and the network will have that number of neurons in the input layer. Or either you change the input_dim to 1 or make the train be an array of elements with length 784.
                
                
– marquex
                
                
                    Commented
                    Oct 14, 2016 at 21:25
                
            
        
    

            
	    

        
                    
                 | 
            Show 2 more comments","stackoverflow.com/questions/47295025/… any suggesions
                
                
– Dexter
                
                
                    Commented
                    Nov 15, 2017 at 13:16
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/40220683,keras.layers.Conv2D,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><machine-learning><neural-network><theano><keras>,7,42667.59167,42668.39444,7,28691,5974433,13846,497600,3.18125,0.923076923,https://stackoverflow.com/questions/40278868,"This question exists as a github issue , too.
I would like to build a neural network in Keras which contains both 2D convolutions and an LSTM layer.

The network should classify MNIST.
The training data in MNIST are 60000 grey-scale images of handwritten digits from 0 to 9. Each image is 28x28 pixels.

I've splitted the images into four parts (left/right, up/down) and rearranged them in four orders to get sequences for the LSTM.

|     |      |1 | 2|
|image|  ->  -------   -> 4 sequences: |1|2|3|4|,  |4|3|2|1|, |1|3|2|4|, |4|2|3|1|
|     |      |3 | 4|


One of the small sub-images has the dimension 14 x 14. The four sequences are stacked together along the width (shouldn't matter whether width or height).

This creates a vector with the shape [60000, 4, 1, 56, 14] where:


60000 is the number of samples
4 is the number of elements in a sequence (# of timesteps)
1 is the depth of colors (greyscale)
56 and 14 are width and height


Now this should be given to a Keras model.
The problem is to change the input dimensions between the CNN and the LSTM.
I searched online and found this question: Python keras how to change the size of input after convolution layer into lstm layer

The solution seems to be a Reshape layer which flattens the image but retains the timesteps (as opposed to a Flatten layer which would collapse everything but the batch_size).

Here's my code so far:

nb_filters=32
kernel_size=(3,3)
pool_size=(2,2)
nb_classes=10
batch_size=64

model=Sequential()

model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
    border_mode=""valid"", input_shape=[1,56,14]))
model.add(Activation(""relu""))
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))
model.add(Activation(""relu""))
model.add(MaxPooling2D(pool_size=pool_size))


model.add(Reshape((56*14,)))
model.add(Dropout(0.25))
model.add(LSTM(5))
model.add(Dense(50))
model.add(Dense(nb_classes))
model.add(Activation(""softmax""))


This code creates an error message:


  ValueError: total size of new array must be unchanged


Apparently the input to the Reshape layer is incorrect. As an alternative, I tried to pass the timesteps to the Reshape layer, too:

model.add(Reshape((4,56*14)))


This doesn't feel right and in any case, the error stays the same.

Am I doing this the right way ?
Is a Reshape layer the proper tool to connect CNN and LSTM ?

There are rather complex approaches to this problem. 
Such as this:
https://github.com/fchollet/keras/pull/1456
A TimeDistributed Layer which seems to hide the timestep dimension from following layers.

Or this: https://github.com/anayebi/keras-extra
A set of special layers for combining CNNs and LSTMs.

Why are there so complicated (at least they seem complicated to me) solutions, if a simple Reshape does the trick ?

UPDATE:

Embarrassingly, I forgot that the dimensions will be changed by the pooling and (for lack of padding) the convolutions, too.
kgrm advised me to use model.summary() to check the dimensions.

The output of the layer before the Reshape layer is (None, 32, 26, 5),
I changed the reshape to: model.add(Reshape((32*26*5,))).

Now the ValueError is gone, instead the LSTM complains:


  Exception: Input 0 is incompatible with layer lstm_5: expected ndim=3, found ndim=2


It seems like I need to pass the timestep dimension through the entire network. How can I do that ? If I add it to the input_shape of the Convolution, it complains, too: Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode=""valid"", input_shape=[4, 1, 56,14])


  Exception: Input 0 is incompatible with layer convolution2d_44: expected ndim=4, found ndim=5","According to Convolution2D definition your input must be 4-dimensional with dimensions (samples, channels, rows, cols). This is the direct reason why are you getting an error.

To resolve that you must use TimeDistributed wrapper. This allows you to use static (not recurrent) layers across the time.",01-08-2016 23:24,4/14/2016 3:18,"You can simply adopt a reshape operation: stackoverflow.com/a/63789979/10375049
                
                
– Marco Cerliani
                
                
                    Commented
                    Mar 21, 2021 at 14:53
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/40220683," keras.layers.TimeDistributed,keras.layers.Conv2D",AMO,G,G,G,Missing Required Method Order,Crash,Keras,Model Construction,Static,0,<python><machine-learning><neural-network><theano><keras>,7,42667.59167,42668.39444,7,28691,5974433,13846,497600,3.18125,0.923076923,https://stackoverflow.com/questions/40310035,"This question exists as a github issue , too.
I would like to build a neural network in Keras which contains both 2D convolutions and an LSTM layer.

The network should classify MNIST.
The training data in MNIST are 60000 grey-scale images of handwritten digits from 0 to 9. Each image is 28x28 pixels.

I've splitted the images into four parts (left/right, up/down) and rearranged them in four orders to get sequences for the LSTM.

|     |      |1 | 2|
|image|  ->  -------   -> 4 sequences: |1|2|3|4|,  |4|3|2|1|, |1|3|2|4|, |4|2|3|1|
|     |      |3 | 4|


One of the small sub-images has the dimension 14 x 14. The four sequences are stacked together along the width (shouldn't matter whether width or height).

This creates a vector with the shape [60000, 4, 1, 56, 14] where:


60000 is the number of samples
4 is the number of elements in a sequence (# of timesteps)
1 is the depth of colors (greyscale)
56 and 14 are width and height


Now this should be given to a Keras model.
The problem is to change the input dimensions between the CNN and the LSTM.
I searched online and found this question: Python keras how to change the size of input after convolution layer into lstm layer

The solution seems to be a Reshape layer which flattens the image but retains the timesteps (as opposed to a Flatten layer which would collapse everything but the batch_size).

Here's my code so far:

nb_filters=32
kernel_size=(3,3)
pool_size=(2,2)
nb_classes=10
batch_size=64

model=Sequential()

model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
    border_mode=""valid"", input_shape=[1,56,14]))
model.add(Activation(""relu""))
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))
model.add(Activation(""relu""))
model.add(MaxPooling2D(pool_size=pool_size))


model.add(Reshape((56*14,)))
model.add(Dropout(0.25))
model.add(LSTM(5))
model.add(Dense(50))
model.add(Dense(nb_classes))
model.add(Activation(""softmax""))


This code creates an error message:


  ValueError: total size of new array must be unchanged


Apparently the input to the Reshape layer is incorrect. As an alternative, I tried to pass the timesteps to the Reshape layer, too:

model.add(Reshape((4,56*14)))


This doesn't feel right and in any case, the error stays the same.

Am I doing this the right way ?
Is a Reshape layer the proper tool to connect CNN and LSTM ?

There are rather complex approaches to this problem. 
Such as this:
https://github.com/fchollet/keras/pull/1456
A TimeDistributed Layer which seems to hide the timestep dimension from following layers.

Or this: https://github.com/anayebi/keras-extra
A set of special layers for combining CNNs and LSTMs.

Why are there so complicated (at least they seem complicated to me) solutions, if a simple Reshape does the trick ?

UPDATE:

Embarrassingly, I forgot that the dimensions will be changed by the pooling and (for lack of padding) the convolutions, too.
kgrm advised me to use model.summary() to check the dimensions.

The output of the layer before the Reshape layer is (None, 32, 26, 5),
I changed the reshape to: model.add(Reshape((32*26*5,))).

Now the ValueError is gone, instead the LSTM complains:


  Exception: Input 0 is incompatible with layer lstm_5: expected ndim=3, found ndim=2


It seems like I need to pass the timestep dimension through the entire network. How can I do that ? If I add it to the input_shape of the Convolution, it complains, too: Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode=""valid"", input_shape=[4, 1, 56,14])


  Exception: Input 0 is incompatible with layer convolution2d_44: expected ndim=4, found ndim=5","According to Convolution2D definition your input must be 4-dimensional with dimensions (samples, channels, rows, cols). This is the direct reason why are you getting an error.

To resolve that you must use TimeDistributed wrapper. This allows you to use static (not recurrent) layers across the time.",01-11-2016 17:20,1/13/2016 16:28,"You can simply adopt a reshape operation: stackoverflow.com/a/63789979/10375049
                
                
– Marco Cerliani
                
                
                    Commented
                    Mar 21, 2021 at 14:53
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/40278868," keras.models.fit,keras.models.compile",AMO,G,G,G,Missing Required Method Order,Crash,Keras,Train,Static,0,<python><machine-learning><tensorflow><keras>,7,42670.31806,42670.31944,7,13846,497600,13846,497600,0.923076923,0.923076923,https://stackoverflow.com/questions/40396042,"Quick answer:

This is in fact really easy.
Here's the code (for those who don't want to read all that text):

inputs=Input((784,))
encode=Dense(10, input_shape=[784])(inputs)
decode=Dense(784, input_shape=[10])

model=Model(input=inputs, output=decode(encode))

inputs_2=Input((10,))
decode_model=Model(input=inputs_2, output=decode(inputs_2))


In this setup, the decode_model will use the same decode layer as the model.
If you train the model, the decode_model will be trained, too.

Actual question:

I'm trying to create a simple autoencoder for MNIST in Keras:

This is the code so far:

model=Sequential()
encode=Dense(10, input_shape=[784])
decode=Dense(784, input_shape=[10])

model.add(encode)
model.add(decode)


model.compile(loss=""mse"",
             optimizer=""adadelta"",
             metrics=[""accuracy""])

decode_model=Sequential()
decode_model.add(decode)


I'm training it to learn the identity function

model.fit(X_train,X_train,batch_size=50, nb_epoch=10, verbose=1, 
          validation_data=[X_test, X_test])


The reconstruction is quite interesting:



But I would also like to look at the representations of cluster.
What is the output of passing [1,0...0] to the decoding layer ?
This should be the ""cluster-mean"" of one class in MNIST.

In order to do that I created a second model decode_model, which reuses the decoder layer.
But if I try to use that model, it complains:


  Exception: Error when checking : expected dense_input_5 to have shape (None, 784) but got array with shape (10, 10)


That seemed strange. It's simply a dense layer, the Matrix wouldn't even be able to process 784-dim input.
I decided to look at the model summary:

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_14 (Dense)                 (None, 784)           8624        dense_13[0][0]                   
====================================================================================================
Total params: 8624


It is connected to dense_13.
It's difficult to keep track of the names of the layers, but that looks like the encoder layer. Sure enough, the model summary of the whole model is:

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_13 (Dense)                 (None, 10)            7850        dense_input_6[0][0]              
____________________________________________________________________________________________________
dense_14 (Dense)                 (None, 784)           8624        dense_13[0][0]                   
====================================================================================================
Total params: 16474
____________________


Apparently the layers are permanently connected.
Strangely there is no input layer in my decode_model.

How can I reuse a layer in Keras ?
I've looked at the functional API, but there too, layers are fused together.","Oh, nevermind.

I should have read the entire functional API:
https://keras.io/getting-started/functional-api-guide/#shared-layers

Here's one of the predictions (maybe still lacking some training):


I'm guessing this could be a 3 ?
Well at least it works now.

And for those with similar problems,
here's the updated code:

inputs=Input((784,))
encode=Dense(10, input_shape=[784])(inputs)
decode=Dense(784, input_shape=[10])

model=Model(input=inputs, output=decode(encode))


model.compile(loss=""mse"",
             optimizer=""adadelta"",
             metrics=[""accuracy""])

inputs_2=Input((10,))
decode_model=Model(input=inputs_2, output=decode(inputs_2))


I only compiled one of the models.
For training you need to compile a model, for prediction that is not necessary.",01-12-2016 01:19,01-12-2016 15:35,"Did you check out this tutorial? blog.keras.io/building-autoencoders-in-keras.html
                
                
– pyan
                
                
                    Commented
                    Oct 27, 2016 at 15:37
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                No, but it looks very interesting. Thanks
                
                
– lhk
                
                
                    Commented
                    Oct 28, 2016 at 6:54
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/40496069," keras.models.fit, keras.models.load_weights, keras.models.save_weights,keras.models.compile",AMO,F,F,F,Missing Required State-specific Method Order,IF,Keras,Load,Static,0,<python><keras><keras-layer>,25,42682.86319,42868.86458,30,1790,2835460,253,3420102,40.25,0,https://stackoverflow.com/questions/41061457,"I'd like to reset (randomize) the weights of all layers in my Keras (deep learning) model. The reason is that I want to be able to train the model several times with different data splits without having to do the (slow) model recompilation every time.
Inspired by this discussion, I'm trying the following code:
# Reset weights
for layer in KModel.layers:
    if hasattr(layer,'init'):
        input_dim = layer.input_shape[1]
        new_weights = layer.init((input_dim, layer.output_dim),name='{}_W'.format(layer.name))
        layer.trainable_weights[0].set_value(new_weights.get_value())

However, it only partly works.
Partly, becuase I've inspected some layer.get_weights() values, and they seem to change. But when I restart the training, the cost values are much lower than the initial cost values on the first run. It's almost like I've succeeded resetting some of the weights, but not all of them.","Save the initial weights right after compiling the model but before training it:

model.save_weights('model.h5')


and then after training, ""reset"" the model by reloading the initial weights:

model.load_weights('model.h5')


This gives you an apples to apples model to compare different data sets and should be quicker than recompiling the entire model.",1/19/2016 7:14,1/19/2016 13:05,"Add a comment
                 |","16
            
        
        
            
                
                I ended up doing something similar. Saving to disk and loading takes a lot of time, so I just keep the weights in a variable: weights = model.get_weights() I get the initial weights like this before running the first training. Then, before each subsequentt training, I reload the initial weights and run jkleint's shuffle method, as mentioned in the link that I posted. Seems to work smoothly..
                
                
– Tor
                
                
                    Commented
                    May 15, 2017 at 13:06
                
                        
                            
                        
            
        
    
    
        
            
                    5
            
        
        
            
                
                For the full code snippet of @Tor's suggestion: weights = model.get_weights(), model.compile(args), model.fit(args), model.set_weights(weights)
                
                
– BallpointBen
                
                
                    Commented
                    May 7, 2018 at 21:01
                
            
        
    
    
        
            
            
        
        
            
                
                Based on this, I've started making a lambda function when I initialize my model.  I build the model, then do something like weights = model.get_weights(); reset_model = lambda model: model.set_weights(weights), that way I can just call reset_model(model) later.
                
                
– Andrew
                
                
                    Commented
                    Jul 5, 2019 at 14:15
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                This has an obvious issue, all models will have the same starting weights. What we want (I think) is weights to be randomly initialized again.
                
                
– J Agustin Barrachina
                
                
                    Commented
                    Sep 15, 2021 at 8:53
                
            
        
    
    
        
            
            
        
        
            
                
                Is saving model architecture and weights separately not a valid option? Is there possible problem that comes with such method? I don't see anyone suggesting it
                
                
– haneulkim
                
                
                    Commented
                    Oct 27, 2022 at 5:53
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41255268,keras.models.add,SAM,BET,IC-1,IC-1,Unacceptable Input Value,BP,Keras,Model Construction,Runtime Checking,0,<python><machine-learning><neural-network><deep-learning><keras>,6,42725.19931,42725.25486,6,7611,6347629,75,3521339,0.623376623,0,https://stackoverflow.com/questions/41458859,"I have 70k samples of text which I have embedded using Keras 'one hot' preprocessing. This gives me an array of [40, 20, 142...] which I then pad for a length of 28 (the longest sample length). All I am trying to do is predict these values to some categorical label (0 to 5 lets say). When I train the model I cannot get anything beyond -.13% accuracy (currently my error is this I have tried many ways to pass the input). 

This is my data currently and am just trying to create a simple LSTM. Again my data is X -> [length of 28 integer values, embeddings] and Y -> [1 integer of length 3, (100, 143 etc.)]. Any idea what I am doing wrong?? I have asked many people and no one has been able to help. Here is the code for my model... any ideas? :( 

optimizer = RMSprop(lr=0.01) #saw this online, no idea
model = Sequential()
model.add(Embedding(input_dim=28,output_dim=1,init='uniform')) #28 features, 1 dim output?
model.add(LSTM(150)) #just adding my LSTM nodes
model.add(Dense(1)) #since I want my output to be 1 integer value

model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
print(model.summary())


Edit: 

using model.add(Embedding(input_dim=900,output_dim=8,init='uniform')) seems to work however still the accuracy never improves, I am at a loss of what to do.","I have two suggestions.


Use one hot representation for the target variable(y) also. If you give Y as integer, it will become a regression problem. Only if you give a one hot encoding, it becomes a classification problem.
Try word2vec embedding when you have large amount of text, instead of one hot embedding.



optimizer = RMSprop(lr=0.01) 
embedding_vecor_length = 32
max_review_length = 28
nb_classes= 8
model = Sequential()
model.add(Embedding(input_dim=900, output_dim=embedding_vecor_length,
                    input_length=max_review_length)) 

model.add(LSTM(150))

#output_dim is a categorical variable with 8 classes
model.add(Dense(output_dim=nb_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
print(model.summary())

model.fit(X_train, y_train, nb_epoch=3, batch_size=64)

# Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print(""Accuracy: %.2f%%"" % (scores[1]*100))",1/22/2016 9:36,1/22/2016 16:48,"Add a comment
                 |","If you use sparse_categorical_crossentropy as the loss function you don't have to one-hot encode the target variable.
                
                
– Gaslight Deceive Subvert
                
                
                    Commented
                    May 28, 2020 at 13:18
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41378461," keras.models.pop,keras.applications.vgg16.VGG16,keras.layers,keras.models",Hybrid,SL,"F,F,IC-1",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,IF,Keras,Model Construction,Static,0,<python><deep-learning><keras>,17,42733.46319,42733.83472,47,7101,2217210,1109,6636290,10.67857143,0,https://stackoverflow.com/questions/41651628,"I want to get pretrained VGG16 model in Keras, remove its output layer, and then put a new output layer with the number of classes suited for my problem, and then to fit it on new data. For this reason, I am trying to use the model here: https://keras.io/applications/#vgg16, but since it is not Sequential, I cannot just model.pop(). Popping from layers and adding it also does not work, because in the predictions it still expects the old shape. How would I do that? Is there a way to convert this type of model to Sequential?","You can use pop() on model.layers and then use model.layers[-1].output to create new layers. 

Example:

from keras.models import Model
from keras.layers import Dense,Flatten
from keras.applications import vgg16
from keras import backend as K

model = vgg16.VGG16(weights='imagenet', include_top=True)

model.input

model.summary(line_length=150)

model.layers.pop()
model.layers.pop()

model.summary(line_length=150)

new_layer = Dense(10, activation='softmax', name='my_dense')

inp = model.input
out = new_layer(model.layers[-1].output)

model2 = Model(inp, out)
model2.summary(line_length=150)


Alternatively, you can use include_top=False option of these models. In this case if you need to use flatten the layer then you need to pass the input_shape also.

model3 = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model3.summary(line_length=150)

flatten = Flatten()
new_layer2 = Dense(10, activation='softmax', name='my_dense_2')

inp2 = model3.input
out2 = new_layer2(flatten(model3.output))

model4 = Model(inp2, out2)
model4.summary(line_length=150)",1/25/2016 7:49,1/25/2016 8:30,"Add a comment
                 |","5
            
        
        
            
                
                This is very important! iIt solves my problem which I have been struggling for hours! Thank you !
                
                
– user40780
                
                
                    Commented
                    May 4, 2017 at 1:20
                
            
        
    
    
        
            
                    6
            
        
        
            
                
                Incredibly useful. This should be in official Keras documentation.
                
                
– ?ukasz Sromek
                
                
                    Commented
                    Jan 17, 2018 at 21:03
                
            
        
    
    
        
            
            
        
        
            
                
                If the model was trained, and we pop the last layer, does it affect the weight of the other layers?
                
                
– Wesam Nabki
                
                
                    Commented
                    Apr 6, 2018 at 8:54
                
            
        
    
    
        
            
            
        
        
            
                
                @WesamNa no. the weights of remaining layers stay the same.
                
                
– Helder
                
                
                    Commented
                    Jul 9, 2018 at 13:56
                
            
        
    
    
        
            
            
        
        
            
                
                It does not work anymore on Tensorflow 2.5.0
                
                
– SashimiDélicieux
                
                
                    Commented
                    May 27, 2021 at 12:33
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41651628,"keras.json.set_image_dim_ordering,keras.models..fit",Hybrid,SL,"F,IC-2",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,Crash,Keras,Train,Static,0,<python><tensorflow><keras>,41,42749.64375,42754.54167,78,1508,2911687,3323,2326911,8.555555556,0,https://stackoverflow.com/questions/41668813,"I'm using Keras with Tensorflow as backend , here is my code:

import numpy as np
np.random.seed(1373) 
import tensorflow as tf
tf.python.control_flow_ops = tf

import os
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.utils import np_utils

batch_size = 128
nb_classes = 10
nb_epoch = 12


img_rows, img_cols = 28, 28

nb_filters = 32

nb_pool = 2

nb_conv = 3


(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape[0])

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)


X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255


print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

model = Sequential()

model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
border_mode='valid',
input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes)) 
model.add(Activation('softmax')) 

model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=[""accuracy""])


model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
verbose=1, validation_data=(X_test, Y_test))

score = model.evaluate(X_test, Y_test, verbose=0)

print('Test score:', score[0])
print('Test accuracy:', score[1])


and Trackback error:

Using TensorFlow backend.
60000
('X_train shape:', (60000, 1, 28, 28))
(60000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""mnist.py"", line 154, in <module>
    input_shape=(1, img_rows, img_cols)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 276, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 370, in create_input_layer
    self(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 514, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 149, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 466, in call
    filter_shape=self.W_shape)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1579, in conv2d
    x = tf.nn.conv2d(x, kernel, strides, padding=padding)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 396, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2242, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1568, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Negative dimension size caused by subtracting 3 from 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].


First I saw some answers that problem is with Tensorflow version so I upgrade Tensorflow to 0.12.0, but still exist , is that problem with network or I missing something, what should input_shape looks like?

Update
Here is ./keras/keras.json:

{
    ""image_dim_ordering"": ""tf"", 
    ""epsilon"": 1e-07, 
    ""floatx"": ""float32"", 
    ""backend"": ""tensorflow""
}","Your issue comes from the image_ordering_dim in keras.json.

From Keras Image Processing doc:     


  dim_ordering: One of {""th"", ""tf""}. ""tf"" mode means that the images should have shape (samples, height, width, channels), ""th"" mode means that the images should have shape (samples, channels, height, width). It defaults to the image_dim_ordering value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be ""tf"".


Keras maps the convolution operation to the chosen backend (theano or tensorflow). However, both backends have made different choices for the ordering of the dimensions. If your image batch is of N images of HxW size with C channels, theano uses the NCHW ordering while tensorflow uses the NHWC ordering.

Keras allows you to choose which ordering you prefer and will do the conversion to map to the backends behind. But if you choose image_ordering_dim=""th"" it expects Theano-style ordering (NCHW, the one you have in your code) and if image_ordering_dim=""tf"" it expects tensorflow-style ordering (NHWC).

Since your image_ordering_dim is set to ""tf"", if you reshape your data to the tensorflow style it should work:

X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows, 1)
X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows, 1)


and 

input_shape=(img_cols, img_rows, 1)",1/29/2016 22:06,1/29/2016 22:17,"1
            
        
        
            
                
                It may be problem with your keras input shape order setting. If you change input_shape=(1, img_rows, img_cols) to input_shape=(img_rows, img_cols, 1), does it work?
                
                
– pyan
                
                
                    Commented
                    Jan 15, 2017 at 1:47
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                @pyan , no it doesn't , Exception: Error when checking model input: expected convolution2d_input_1 to have shape (None, 28, 28, 1) but got array with shape (60000, 1, 28, 28)
                
                
– ?????
                
                
                    Commented
                    Jan 15, 2017 at 11:27
                
            
        
    
    
        
            
            
        
        
            
                
                Can you check what is the content in the file of .keras/keras.json, especially the value of ""image_dim_ordering""
                
                
– pyan
                
                
                    Commented
                    Jan 16, 2017 at 2:08
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                @Arman Ajoooo nice question! ^_^
                
                
– Peyman.H
                
                
                    Commented
                    Jan 20, 2017 at 20:40
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                Using this script: elitedatascience.com/…   and making the 2 changes to lines 15,16&29 worked for me. Thank you!
                
                
– DJ Howarth
                
                
                    Commented
                    Jun 26, 2017 at 16:40
                
            
        
    
    
        
            
            
        
        
            
                
                Additional info: You can check which format is used (NHWC vs NCHW) at runtime using if K.image_data_format() == 'channels_first': (with from keras import backend as K).
                
                
– Hendrik
                
                
                    Commented
                    Aug 31, 2017 at 6:32
                
            
        
    
    
        
            
            
        
        
            
                
                I ise if K.image_data_format() == 'channels_first':     input_shape = (3, img_width, img_height) else:     input_shape = (img_width, img_height, 3) and still get the error. Adding padding=""same"" helps in so far that my skript runs, but it seems to only mask rather than solve the issue... It is not clear to me what the actual problem is...
                
                
– Peter
                
                
                    Commented
                    Jan 9, 2019 at 17:01
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41651628," keras.models.evaluate,keras.json.set_image_dim_ordering",Hybrid,SL,"F,IC-2",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,Crash,Keras,Model Evaluation,Static,0,<python><tensorflow><keras>,41,42749.64375,42754.54167,78,1508,2911687,3323,2326911,8.555555556,0,https://stackoverflow.com/questions/41672114,"I'm using Keras with Tensorflow as backend , here is my code:

import numpy as np
np.random.seed(1373) 
import tensorflow as tf
tf.python.control_flow_ops = tf

import os
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.utils import np_utils

batch_size = 128
nb_classes = 10
nb_epoch = 12


img_rows, img_cols = 28, 28

nb_filters = 32

nb_pool = 2

nb_conv = 3


(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape[0])

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)


X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255


print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

model = Sequential()

model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
border_mode='valid',
input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes)) 
model.add(Activation('softmax')) 

model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=[""accuracy""])


model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
verbose=1, validation_data=(X_test, Y_test))

score = model.evaluate(X_test, Y_test, verbose=0)

print('Test score:', score[0])
print('Test accuracy:', score[1])


and Trackback error:

Using TensorFlow backend.
60000
('X_train shape:', (60000, 1, 28, 28))
(60000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""mnist.py"", line 154, in <module>
    input_shape=(1, img_rows, img_cols)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 276, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 370, in create_input_layer
    self(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 514, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 149, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 466, in call
    filter_shape=self.W_shape)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1579, in conv2d
    x = tf.nn.conv2d(x, kernel, strides, padding=padding)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 396, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2242, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1568, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Negative dimension size caused by subtracting 3 from 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].


First I saw some answers that problem is with Tensorflow version so I upgrade Tensorflow to 0.12.0, but still exist , is that problem with network or I missing something, what should input_shape looks like?

Update
Here is ./keras/keras.json:

{
    ""image_dim_ordering"": ""tf"", 
    ""epsilon"": 1e-07, 
    ""floatx"": ""float32"", 
    ""backend"": ""tensorflow""
}","Your issue comes from the image_ordering_dim in keras.json.

From Keras Image Processing doc:     


  dim_ordering: One of {""th"", ""tf""}. ""tf"" mode means that the images should have shape (samples, height, width, channels), ""th"" mode means that the images should have shape (samples, channels, height, width). It defaults to the image_dim_ordering value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be ""tf"".


Keras maps the convolution operation to the chosen backend (theano or tensorflow). However, both backends have made different choices for the ordering of the dimensions. If your image batch is of N images of HxW size with C channels, theano uses the NCHW ordering while tensorflow uses the NHWC ordering.

Keras allows you to choose which ordering you prefer and will do the conversion to map to the backends behind. But if you choose image_ordering_dim=""th"" it expects Theano-style ordering (NCHW, the one you have in your code) and if image_ordering_dim=""tf"" it expects tensorflow-style ordering (NHWC).

Since your image_ordering_dim is set to ""tf"", if you reshape your data to the tensorflow style it should work:

X_train = X_train.reshape(X_train.shape[0], img_cols, img_rows, 1)
X_test = X_test.reshape(X_test.shape[0], img_cols, img_rows, 1)


and 

input_shape=(img_cols, img_rows, 1)",02-02-2016 08:12,02-02-2016 14:36,"1
            
        
        
            
                
                It may be problem with your keras input shape order setting. If you change input_shape=(1, img_rows, img_cols) to input_shape=(img_rows, img_cols, 1), does it work?
                
                
– pyan
                
                
                    Commented
                    Jan 15, 2017 at 1:47
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                @pyan , no it doesn't , Exception: Error when checking model input: expected convolution2d_input_1 to have shape (None, 28, 28, 1) but got array with shape (60000, 1, 28, 28)
                
                
– ?????
                
                
                    Commented
                    Jan 15, 2017 at 11:27
                
            
        
    
    
        
            
            
        
        
            
                
                Can you check what is the content in the file of .keras/keras.json, especially the value of ""image_dim_ordering""
                
                
– pyan
                
                
                    Commented
                    Jan 16, 2017 at 2:08
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                @Arman Ajoooo nice question! ^_^
                
                
– Peyman.H
                
                
                    Commented
                    Jan 20, 2017 at 20:40
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                Using this script: elitedatascience.com/…   and making the 2 changes to lines 15,16&29 worked for me. Thank you!
                
                
– DJ Howarth
                
                
                    Commented
                    Jun 26, 2017 at 16:40
                
            
        
    
    
        
            
            
        
        
            
                
                Additional info: You can check which format is used (NHWC vs NCHW) at runtime using if K.image_data_format() == 'channels_first': (with from keras import backend as K).
                
                
– Hendrik
                
                
                    Commented
                    Aug 31, 2017 at 6:32
                
            
        
    
    
        
            
            
        
        
            
                
                I ise if K.image_data_format() == 'channels_first':     input_shape = (3, img_width, img_height) else:     input_shape = (img_width, img_height, 3) and still get the error. Adding padding=""same"" helps in so far that my skript runs, but it seems to only mask rather than solve the issue... It is not clear to me what the actual problem is...
                
                
– Peter
                
                
                    Commented
                    Jan 9, 2019 at 17:01
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41668813," keras.layers, keras.models,keras.models.load_weights",AMO,F,F,F,Missing Required State-specific Method Order,Crash,Keras,Model Construction,Static,0,<python><theano><keras><keras-layer>,40,42751.12222,42751.28681,52,7101,2217210,8288,996366,10.67857143,2,https://stackoverflow.com/questions/41771965,"I am trying to do a transfer learning; for that purpose I want to remove the last two layers of the neural network and add another two layers. This is an example code which also output the same error.
from keras.models import Sequential
from keras.layers import Input,Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Dropout, Activation
from keras.layers.pooling import GlobalAveragePooling2D
from keras.models import Model

in_img = Input(shape=(3, 32, 32))
x = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)
x = Activation('relu', name='relu_conv1')(x)
x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)
x = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)
x = Activation('relu', name='relu_conv2')(x)
x = GlobalAveragePooling2D()(x)
o = Activation('softmax', name='loss')(x)
model = Model(input=in_img, output=[o])
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
#model.load_weights('model_weights.h5', by_name=True)
model.summary()

model.layers.pop()
model.layers.pop()
model.summary()
model.add(MaxPooling2D())
model.add(Activation('sigmoid', name='loss'))

I removed the layer using pop() but when I tried to add its outputting this error
AttributeError: 'Model' object has no attribute 'add'

I know the most probable reason for the error is improper use of model.add(). what other syntax should I use?
EDIT:
I tried to remove/add layers in keras but its not  allowing it to be added after loading external weights.
from keras.models import Sequential
from keras.layers import Input,Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Dropout, Activation
from keras.layers.pooling import GlobalAveragePooling2D
from keras.models import Model
in_img = Input(shape=(3, 32, 32))

def gen_model():
    in_img = Input(shape=(3, 32, 32))
    x = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)
    x = Activation('relu', name='relu_conv1')(x)
    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)
    x = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)
    x = Activation('relu', name='relu_conv2')(x)
    x = GlobalAveragePooling2D()(x)
    o = Activation('softmax', name='loss')(x)
    model = Model(input=in_img, output=[o])
    return model

#parent model
model=gen_model()
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
model.summary()

#saving model weights
model.save('model_weights.h5')

#loading weights to second model
model2=gen_model()
model2.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
model2.load_weights('model_weights.h5', by_name=True)

model2.layers.pop()
model2.layers.pop()
model2.summary()

#editing layers in the second model and saving as third model
x = MaxPooling2D()(model2.layers[-1].output)
o = Activation('sigmoid', name='loss')(x)
model3 = Model(input=in_img, output=[o])

its showing this error
RuntimeError: Graph disconnected: cannot obtain value for tensor input_4 at layer ""input_4"". The following previous layers were accessed without issue: []","You can take the output of the last model and create a new model. The lower layers remains the same.
model.summary()
model.layers.pop()
model.layers.pop()
model.summary()

x = MaxPooling2D()(model.layers[-1].output)
o = Activation('sigmoid', name='loss')(x)

model2 = Model(inputs=in_img, outputs=[o])
model2.summary()

Check How to use models from keras.applications for transfer learnig?
Update on Edit:
The new error is because you are trying to create the new model on global in_img which is actually not used in the previous model creation.. there you are actually defining a local in_img. So the global in_img is obviously not connected to the upper layers in the symbolic graph. And it has nothing to do with loading weights.
To better resolve this problem you should instead use model.input to reference to the input.
model3 = Model(input=model2.input, output=[o])",02-04-2016 18:29,02-04-2016 19:00,"This seem similar [1]: stackoverflow.com/questions/54284898/… to your question.
                
                
– tryinToLearnGCP
                
                
                    Commented
                    Mar 23, 2019 at 8:48
                
            
        
    

            
	    

        
                    Add a comment
                 |","In Keras 2.4.0, the keywords for keras.model.Model are ""inputs"" and ""outputs"" rather than ""input"" and ""output""
                
                
– user160623
                
                
                    Commented
                    Sep 1, 2022 at 9:54
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41771965," keras.layers.Conv2D, keras.models.fit,keras.json.set_image_dim_ordering",Hybrid,SL,"F,IC-1,MT",Comb. of SAM(Level 3) and AMO(Level 2),Missing Options,Crash,Keras,Train,Static,0,<python><tensorflow><deep-learning><keras>,6,42755.85833,42755.97222,12,7101,2217210,17918,2414957,10.67857143,0.8,https://stackoverflow.com/questions/41823068,"Can someone please guide how to fix this error? I just started on Keras:

 1 from keras.datasets import cifar10
  2 from matplotlib import pyplot
  3 from scipy.misc import toimage
  4 
  5 (x_train, y_train), (x_test, y_test) = cifar10.load_data()
  6 for i in range(0, 9):
  7     pyplot.subplot(330 + 1 + i)
  8     pyplot.imshow(toimage(x_train[i]))
  9 pyplot.show()
 10 
 11 import numpy
 12 from keras.models import Sequential
 13 from keras.layers import Dense
 14 from keras.layers import Dropout
 15 from keras.layers import Flatten
 16 from keras.constraints import maxnorm
 17 from keras.optimizers import SGD
 18 from keras.layers.convolutional import Convolution2D
 19 from keras.layers.convolutional import MaxPooling2D
 20 from keras.utils import np_utils
 21 from keras import backend
 22 backend.set_image_dim_ordering('th')
 23 
 24 seed = 7
 25 numpy.random.seed(seed)
 26 
 27 x_train = x_train.astype('float32')
 28 x_test = x_test.astype('float32')
 29 x_train = x_train / 255.0
 30 x_test = x_test / 255.0
 31 
 32 y_train = np_utils.to_categorical(y_train)
 33 y_test = np_utils.to_categorical(y_test)
 34 num_classes = y_test.shape[1]
 35 
 36 model = Sequential()
 37 model.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), border_mode='same', activation='relu', W_constraint=maxnorm(3)))
 38 model.add(Dropout(0.2))
 39 model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same', W_constraint=maxnorm(3)))
 40 model.add(Flatten())
 41 model.add(Dense(512, activation='relu', W_constraint=maxnorm(3)))
 42 model.add(Dropout(0.5))
 43 model.add(Dense(num_classes, activation='softmax'))
 44 
 45 epochs = 25
 46 learning_rate = 0.01
 47 learning_rate_decay = learning_rate/epochs
 48 sgd = SGD(lr=learning_rate, momentum=0.9, decay=learning_rate_decay, nesterov=False)
 49 model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
 50 print(model.summary())
 51 
 52 model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=epochs, batch_size=32)
 53 scores = model.evaluate(x_test, y_test, verbose=0)
 54 print(""Accuracy: %.2f%%"" % (scores[1]*100))


Output is:

mona@pascal:/data/wd1$ python test_keras.py 
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 32, 32, 32)    896         convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32, 32, 32)    0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 32, 32, 32)    9248        dropout_1[0][0]                  
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 32768)         0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 512)           16777728    flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 512)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  
====================================================================================================
Total params: 16,793,002
Trainable params: 16,793,002
Non-trainable params: 0
____________________________________________________________________________________________________
None
Traceback (most recent call last):
  File ""test_keras.py"", line 52, in <module>
    model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=epochs, batch_size=32)
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 664, in fit
    sample_weight=sample_weight)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1068, in fit
    batch_size=batch_size)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 981, in _standardize_user_data
    exception_prefix='model input')
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 113, in standardize_input_data
    str(array.shape))
ValueError: Error when checking model input: expected convolution2d_input_1 to have shape (None, 3, 32, 32) but got array with shape (50000, 32, 32, 3)","If you print x_train.shape you will see the shape being (50000, 32, 32, 3) whereas you have given input_shape=(3, 32, 32) in the first layer. The error simply says that the expected input shape and data given are different. 

All you need to do is give  input_shape=(32, 32, 3). Also if you use this shape then you must use tf as your image ordering. backend.set_image_dim_ordering('tf'). 

Otherwise you can permute the axis of data. 

x_train = x_train.transpose(0,3,1,2)
x_test = x_test.transpose(0,3,1,2)
print x_train.shape",02-08-2016 19:40,02-08-2016 20:53,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/41823068,keras.models.add,SAM,BET,IC-1,IC-1,Unacceptable Input Value,BP,Keras,Model Construction,Runtime Checking,0,<python><machine-learning><deep-learning><keras><one-hot-encoding>,12,42759.34167,42759.39444,14,3269,5215538,493,7085724,3.542857143,0,https://stackoverflow.com/questions/41855512,"I was following a tutorial to learn train a classifier using Keras
https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
Specifically, from the second script given by the author, I wanted to transform the script into a one that can train multi-class classifier(was a binary for cat and dog). I have 5 classes in my train folder so I did the following change:
In the function of train_top_model():
I changed
model = Sequential()
model.add(Flatten(input_shape=train_data.shape[1:]))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

into
model = Sequential()
model.add(Flatten(input_shape=train_data.shape[1:]))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

train_labels = to_categorical(train_labels, 5)
validation_labels = to_categorical(validation_labels, 5)

After done the training, the model reached training accuracy of near 99% but only for like 70% accuracy of the validation accuracy. Thus I started thinking maybe it's not that simple to convert 2 classes training to 5 classes. Maybe I need to use one-hot encoding when labeling the classes (but I don't know how)
EDIT:
I attached my fine-tuning script as well. Another problem: the accuracy did not effectively increase when fine-tuning starts.
import os
import h5py
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.layers import Activation, Dropout, Flatten, Dense

# path to the model weights files.
weights_path = 'D:/Users/EJLTZ/Desktop/vgg16_weights.h5'
top_model_weights_path = 'bottleneck_weights_2.h5'
# dimensions of our images.
img_width, img_height = 150, 150

train_data_dir = 'D:/Users/EJLTZ/Desktop/BodyPart-full/train_new'
validation_data_dir = 'D:/Users/EJLTZ/Desktop/BodyPart-full/validation_new'
nb_train_samples = 500
nb_validation_samples = 972
nb_epoch = 50

# build the VGG16 network
model = Sequential()
model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))

model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))
model.add(ZeroPadding2D((1, 1)))
model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))
model.add(MaxPooling2D((2, 2), strides=(2, 2)))

# load the weights of the VGG16 networks
# (trained on ImageNet, won the ILSVRC competition in 2014)
# note: when there is a complete match between your model definition
# and your weight savefile, you can simply call model.load_weights(filename)
assert os.path.exists(weights_path), 'Model weights not found (see ""weights_path"" variable in script).'
f = h5py.File(weights_path)
for k in range(f.attrs['nb_layers']):
    if k >= len(model.layers):
        # we don't look at the last (fully-connected) layers in the savefile
        break
    g = f['layer_{}'.format(k)]
    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]
    model.layers[k].set_weights(weights)
f.close()
print('Model loaded.')

# build a classifier model to put on top of the convolutional model
top_model = Sequential()
top_model.add(Flatten(input_shape=model.output_shape[1:]))
top_model.add(Dense(256, activation='relu'))
top_model.add(Dropout(0.5))
top_model.add(Dense(5, activation='softmax'))

# note that it is necessary to start with a fully-trained
# classifier, including the top classifier,
# in order to successfully do fine-tuning
top_model.load_weights(top_model_weights_path)

# add the model on top of the convolutional base
model.add(top_model)

# set the first 25 layers (up to the last conv block)
# to non-trainable (weights will not be updated)
for layer in model.layers[:25]:
    layer.trainable = False

# compile the model with a SGD/momentum optimizer
# and a very slow learning rate.
model.compile(loss='categorical_crossentropy',
          optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),
          metrics=['accuracy'])

# prepare data augmentation configuration
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=32,
    class_mode= 'categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_height, img_width),
    batch_size=32,
    class_mode= 'categorical')

# fine-tune the model
model.fit_generator(
    train_generator,
    samples_per_epoch=nb_train_samples,
    nb_epoch=nb_epoch,
    validation_data=validation_generator,
    nb_val_samples=nb_validation_samples)

model.save_weights(""fine-tune_weights.h5"")
model.save(""fine-tune_model.h5"", True)","Use softmax as activation function of the output layer, it is a generalization of the logistic function for a multi class case. Read more about it here.
If validation error is much greater than the training one, as in your case, it is an indicator of overfitting. You should do some regularization, which is defined as any changes of the learning algorithm, that are intended to reduce the test error but not the training one. You can try things like data augmentation, early stopping, noise injection, more aggressive dropout, etc.
If you have the same set-up as in the linked tutorial, change the class_modeof the train_generatorand validation_generator to categorical and it will one-hot encode your classes.",02-09-2016 17:26,02-09-2016 23:08,"Can you mention how your training and test set is organized? Meaning, are the different class images in different folders in the path that you provide or something else?
                
                
– Mohit Motwani
                
                
                    Commented
                    Aug 13, 2018 at 6:18
                
            
        
    

            
	    

        
                    Add a comment
                 |","Thx! I am trying out your solutions. The training takes time. I will let you know how it goes.
                
                
– PIZZA PIZZA
                
                
                    Commented
                    Jan 24, 2017 at 12:51
                
            
        
    
    
        
            
            
        
        
            
                
                I got the validation accuracy of 80% this time. Impressive! However when I followed the 3rd script (fine-tune: to train the top layer of the network)   gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975  the accuracy started from 20% for the 1st epoch and did not increase. IDK why.(I set the class_mode='categorical')
                
                
– PIZZA PIZZA
                
                
                    Commented
                    Jan 24, 2017 at 13:08
                
            
        
    
    
        
            
            
        
        
            
                
                Have you trained your own fully-connected classifier on top of convolutional blocks before starting fine-tuning? @AustinChen
                
                
– Sergii Gryshkevych
                
                
                    Commented
                    Jan 24, 2017 at 13:38
                
            
        
    
    
        
            
            
        
        
            
                
                I was assuming the classifier is the one that I got from the last step -- the top model (the best performing model before fine tuning), then I need to load this model and fine-tune the top layer right? I tried the 2 classes cat-dog example, the accuracy of the model was like 98% before fine-tuning and increased (slowly) once I started fine-tunning. However, for my case with 5 classes, fine-tunning started from 20%. I checked the model I loaded is the one I just trained after the bottleneck features. @Sergii Gryshkevych
                
                
– PIZZA PIZZA
                
                
                    Commented
                    Jan 24, 2017 at 13:46
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                Just in case: did you freeze all conv layers except the bottleneck ones? It is hard to see what is wrong without seeing the code.
                
                
– Sergii Gryshkevych
                
                
                    Commented
                    Jan 24, 2017 at 13:52
                
            
        
    

            
	    

        
                    
                 | 
            Show 4 more comments"
https://stackoverflow.com/questions/41859997,"keras.models.add,keras.models.load_weights",AMO,G,G,G,Missing Required Method Order,Crash,Keras,Load,Static,0,<python><neural-network><keras>,8,42760.80972,42760.81667,20,2090,7438048,141,7230498,6,0,https://stackoverflow.com/questions/41933958,"I'm using the Keras library to create a neural network in python. I have loaded the training data (txt file), initiated the network and ""fit"" the weights of the neural network. I have then written code to generate the output text. Here is the code:

#!/usr/bin/env python

# load the network weights
filename = ""weights-improvement-19-2.0810.hdf5""
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')


My problem is: on execution the following error is produced:

 model.load_weights(filename)
 NameError: name 'model' is not defined


I have added the following but the error still persists: 

from keras.models import Sequential
from keras.models import load_model


Any help would be appreciated.","you need to first create the network object called model, compile it and only after call the model.load_weights(fname)

working example:

from keras.models import Sequential
from keras.layers import Dense, Activation


def build_model():
    model = Sequential()

    model.add(Dense(output_dim=64, input_dim=100))
    model.add(Activation(""relu""))
    model.add(Dense(output_dim=10))
    model.add(Activation(""softmax""))

    # you can either compile or not the model
    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
    return model


model1 = build_model()
model1.save_weights('my_weights.model')


model2 = build_model()
model2.load_weights('my_weights.model')

# do stuff with model2 (e.g. predict())


Save & Load an Entire Model

in Keras we can save & load the entire model like this (more info here):

from keras.models import load_model

model1 = build_model()
model1.save('my_model.hdf5')

model2 = load_model('my_model.hdf5')
# do stuff with model2 (e.g. predict()",2/20/2016 1:32,2/21/2016 13:06,"Add a comment
                 |","3
            
        
        
            
                
                Thank you vey much, that works perfectly. However I had to change from keras.layers import Dense, Activation to: from keras.layers.core import Dense.
                
                
– Deadulus
                
                
                    Commented
                    Jan 25, 2017 at 20:35
                
                        
                            
                        
            
        
    
    
        
            
                    2
            
        
        
            
                
                As per keras docs keras, no need to compile the model.
                
                
– CKM
                
                
                    Commented
                    Aug 8, 2019 at 9:27
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/41863921,keras.layers.LSTM,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><machine-learning><keras><one-hot-encoding>,7,42761.00208,42761.1,6,40960,1643939,1097,3466910,2.555844156,0,https://stackoverflow.com/questions/41947039,"I have inputs that look like this: 

[
[1, 2, 3]
[4, 5, 6]
[7, 8, 9]
...]


of shape (1, num_samples, num_features), and labels that look like this:

[
[0, 1]
[1, 0]
[1, 0]
...]


of shape (1, num_samples, 2).

However, when I try to run the following Keras code, I get this error:
ValueError: Error when checking model target: expected dense_1 to have 2 dimensions, but got array with shape (1, 8038, 2). From what I've read, this appears to stem from the fact that my labels are 2D, and not simply integers. Is this correct, and if so, how can I use one-hot labels with Keras?

Here's the code:

num_features = 463
trX = np.random(8038, num_features)
trY = # one-hot array of shape (8038, 2) as described above

def keras_builder():  #generator to build the inputs
    while(1):
        x = np.reshape(trX, (1,) + np.shape(trX))
        y = np.reshape(trY, (1,) + np.shape(trY))
        print(np.shape(x)) # (1, 8038, 463)
        print(np.shape(y)) # (1, 8038, 2)
        yield x, y

model = Sequential()
model.add(LSTM(100, input_dim = num_features))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit_generator(keras_builder(), samples_per_epoch = 1, nb_epoch=3, verbose = 2, nb_worker = 1)


Which promptly throws the error above:

Traceback (most recent call last):
  File ""file.py"", line 35, in <module>
    model.fit_generator(keras_builder(), samples_per_epoch = 1, nb_epoch=3, verbose = 2, nb_worker = 1)
  ...
ValueError: Error when checking model target: expected dense_1 to have 2 dimensions, but got array with shape (1, 8038, 2)


Thank you!","There are a lot of things that do not add up.

I assume that you are trying to solve a sequential classification task, i.e. your data is shaped as (<batch size>, <sequence length>, <feature length>). 

In your batch generator you create a batch consisting of one sequence of length 8038 and 463 features per sequence element. You create a matching Y batch to compare against, consisting of one sequence with 8038 elements, each of size 2. 

Your problem is that Y does not match up with the output of the last layer. Your Y is 3-dimensional while the output of your model is only 2-dimensional: Y.shape = (1, 8038, 2) does not match dense_1.shape = (1,1). This explains the error message you get.

The solution to this: you need to enable return_sequences=True in the LSTM layer to return a sequence instead of only the last element (effectively removing the time-dimension). This would give an output shape of (1, 8038, 100) at the LSTM layer. Since the Dense layer is not able to handle sequential data you need to apply it to each sequence element individually which is done by wrapping it in a TimeDistributed wrapper. This then gives your model the output shape (1, 8038, 1).

Your model should look like this:

from keras.layers.wrappers import TimeDistributed

model = Sequential()
model.add(LSTM(100, input_dim=num_features, return_sequences=True))
model.add(TimeDistributed(Dense(1, activation='sigmoid')))


This can be easily spotted when examining the summary of the model:

print(model.summary())",2/26/2016 21:01,2/26/2016 21:24,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/41863921," keras.layers.Dense, keras.layers.TimeDistributed,keras.layers.LSTM",Hybrid,SAI,"F,IC-1",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Crash,Keras,Model Construction,Static,0,<python><machine-learning><keras><one-hot-encoding>,7,42761.00208,42761.1,6,40960,1643939,1097,3466910,2.555844156,0,https://stackoverflow.com/questions/42002717,"I have inputs that look like this: 

[
[1, 2, 3]
[4, 5, 6]
[7, 8, 9]
...]


of shape (1, num_samples, num_features), and labels that look like this:

[
[0, 1]
[1, 0]
[1, 0]
...]


of shape (1, num_samples, 2).

However, when I try to run the following Keras code, I get this error:
ValueError: Error when checking model target: expected dense_1 to have 2 dimensions, but got array with shape (1, 8038, 2). From what I've read, this appears to stem from the fact that my labels are 2D, and not simply integers. Is this correct, and if so, how can I use one-hot labels with Keras?

Here's the code:

num_features = 463
trX = np.random(8038, num_features)
trY = # one-hot array of shape (8038, 2) as described above

def keras_builder():  #generator to build the inputs
    while(1):
        x = np.reshape(trX, (1,) + np.shape(trX))
        y = np.reshape(trY, (1,) + np.shape(trY))
        print(np.shape(x)) # (1, 8038, 463)
        print(np.shape(y)) # (1, 8038, 2)
        yield x, y

model = Sequential()
model.add(LSTM(100, input_dim = num_features))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit_generator(keras_builder(), samples_per_epoch = 1, nb_epoch=3, verbose = 2, nb_worker = 1)


Which promptly throws the error above:

Traceback (most recent call last):
  File ""file.py"", line 35, in <module>
    model.fit_generator(keras_builder(), samples_per_epoch = 1, nb_epoch=3, verbose = 2, nb_worker = 1)
  ...
ValueError: Error when checking model target: expected dense_1 to have 2 dimensions, but got array with shape (1, 8038, 2)


Thank you!","There are a lot of things that do not add up.

I assume that you are trying to solve a sequential classification task, i.e. your data is shaped as (<batch size>, <sequence length>, <feature length>). 

In your batch generator you create a batch consisting of one sequence of length 8038 and 463 features per sequence element. You create a matching Y batch to compare against, consisting of one sequence with 8038 elements, each of size 2. 

Your problem is that Y does not match up with the output of the last layer. Your Y is 3-dimensional while the output of your model is only 2-dimensional: Y.shape = (1, 8038, 2) does not match dense_1.shape = (1,1). This explains the error message you get.

The solution to this: you need to enable return_sequences=True in the LSTM layer to return a sequence instead of only the last element (effectively removing the time-dimension). This would give an output shape of (1, 8038, 100) at the LSTM layer. Since the Dense layer is not able to handle sequential data you need to apply it to each sequence element individually which is done by wrapping it in a TimeDistributed wrapper. This then gives your model the output shape (1, 8038, 1).

Your model should look like this:

from keras.layers.wrappers import TimeDistributed

model = Sequential()
model.add(LSTM(100, input_dim=num_features, return_sequences=True))
model.add(TimeDistributed(Dense(1, activation='sigmoid')))


This can be easily spotted when examining the summary of the model:

print(model.summary())",42427.77917,42427.85139,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/41933958,keras.layers.Embedding,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><neural-network><keras><recurrent-neural-network><sequence-to-sequence>,6,42765.44931,42765.92222,6,28691,5974433,8288,996366,3.18125,2,https://stackoverflow.com/questions/42260265,"I am trying to write a sequence to sequence RNN in keras. I coded this program using what I understood from the web. I first tokenized the text then converted the text into sequence and padded to form feature variable X. The target variable Y was obtained first shifting x to left and then padding it. Lastly I fed my feature and target variable to my LSTM model.

This is  my code I written in keras for that purpose.

from keras.preprocessing.text import Tokenizer,base_filter
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Activation,Dropout,Embedding
from keras.layers import LSTM


def shift(seq, n):
    n = n % len(seq)
    return seq[n:] + seq[:n]

txt=""abcdefghijklmn""*100

tk = Tokenizer(nb_words=2000, filters=base_filter(), lower=True, split="" "")
tk.fit_on_texts(txt)
x = tk.texts_to_sequences(txt)
#shifing to left
y = shift(x,1)

#padding sequence
max_len = 100
max_features=len(tk.word_counts)
X = pad_sequences(x, maxlen=max_len)
Y = pad_sequences(y, maxlen=max_len)

#lstm model
model = Sequential()
model.add(Embedding(max_features, 128, input_length=max_len, dropout=0.2))
model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))
model.add(Dense(max_len))
model.add(Activation('softmax'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop')

model.fit(X, Y, batch_size=200, nb_epoch=10)


The problem is its showing an error

Epoch 1/10
IndexError: index 14 is out of bounds for size 14
Apply node that caused the error: AdvancedSubtensor1(if{inplace}.0, Reshape{1}.0)
Toposort index: 80","The problem lies in: 

model.add(Embedding(max_features, 128, input_length=max_len, dropout=0.2))


In the Embedding documentation you may see that the first argument provided to it should be set to size of vocabulary + 1. It's because there should be always a place for a null word which index is 0. Because of that you need to change this line to:

model.add(Embedding(max_features + 1, 128, input_length=max_len, dropout=0.2))",2/28/2016 20:11,2/28/2016 20:59,"Could you provide us a max_features value?
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Jan 30, 2017 at 15:22
                
            
        
    
    
        
            
            
        
        
            
                
                its same as the length of the abcdefghijklmn that is 14
                
                
– Eka
                
                
                    Commented
                    Jan 30, 2017 at 15:49
                
            
        
    
    
        
            
            
        
        
            
                
                I suspect what you really mean is a many to many LSTM. Sequence to Sequence means something else (used in Machine Translation). See here for an example similar to yours: github.com/sachinruk/deepschool.io/blob/master/…
                
                
– sachinruk
                
                
                    Commented
                    Sep 18, 2017 at 3:59
                
            
        
    

            
	    

        
                    Add a comment
                 |","Hey thanks it worked. Can I ask another question; from this model how should I predict new sequence?
                
                
– Eka
                
                
                    Commented
                    Jan 31, 2017 at 1:13
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                In the newest Keras version some parameters have been renamed: from nb_words to num_words, and nb_epoch to epochs .
                
                
– Pablo Casas
                
                
                    Commented
                    Jan 18, 2018 at 21:29
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42047497,"keras.backend,keras.backend.clear_session",AMO,G,G,G,Missing Required Method Order,MOB,Keras,Model Initialization,Static,0,<python><memory-management><out-of-memory><keras><grid-search>,15,42771.04514,42771.05903,19,7101,2217210,2133,7517192,10.67857143,2.678571429,https://stackoverflow.com/questions/42322698,"I'm running multiple nested loops to do hyper parameter grid search. Each nested loop runs through a list of hyper parameter values and inside the innermost loop, a Keras sequential model is built and evaluated each time using a generator. (I'm not doing any training, I'm just randomly initializing and then evaluating the model multiple times and then retrieving the average loss).

My problem is that during this process, Keras seems to be filling up my GPU memory, so that I eventually get an OOM error.

Does anybody know how to solve this and free up the GPU memory each time after a model is evaluated?

I do not need the model anymore at all after it has been evaluated, I can throw it away entirely every time before building a new one in the next pass of the inner loop.

I'm using the Tensorflow backend.

Here is the code, although much of it isn't relevant to the general problem. The model is built inside the fourth loop,

for fsize in fsizes:


I guess the details of how the model is built don't matter much, but here is all of it anyway:

model_losses = []
model_names = []

for activation in activations:
    for i in range(len(layer_structures)):
        for width in layer_widths[i]:
            for fsize in fsizes:

                model_name = ""test_{}_struc-{}_width-{}_fsize-{}"".format(activation,i,np.array_str(np.array(width)),fsize)
                model_names.append(model_name)
                print(""Testing new model: "", model_name)

                #Structure for this network
                structure = layer_structures[i]

                row, col, ch = 80, 160, 3  # Input image format

                model = Sequential()

                model.add(Lambda(lambda x: x/127.5 - 1.,
                          input_shape=(row, col, ch),
                          output_shape=(row, col, ch)))

                for j in range(len(structure)):
                    if structure[j] == 'conv':
                        model.add(Convolution2D(width[j], fsize, fsize))
                        model.add(BatchNormalization(axis=3, momentum=0.99))
                        if activation == 'relu':
                            model.add(Activation('relu'))
                        if activation == 'elu':
                            model.add(ELU())
                            model.add(MaxPooling2D())
                    elif structure[j] == 'dense':
                        if structure[j-1] == 'dense':
                            model.add(Dense(width[j]))
                            model.add(BatchNormalization(axis=1, momentum=0.99))
                            if activation == 'relu':
                                model.add(Activation('relu'))
                            elif activation == 'elu':
                                model.add(ELU())
                        else:
                            model.add(Flatten())
                            model.add(Dense(width[j]))
                            model.add(BatchNormalization(axis=1, momentum=0.99))
                            if activation == 'relu':
                                model.add(Activation('relu'))
                            elif activation == 'elu':
                                model.add(ELU())

                model.add(Dense(1))

                average_loss = 0
                for k in range(5):
                    model.compile(optimizer=""adam"", loss=""mse"")
                    val_generator = generate_batch(X_val, y_val, resize=(160,80))
                    loss = model.evaluate_generator(val_generator, len(y_val))
                    average_loss += loss

                average_loss /= 5

                model_losses.append(average_loss)

                print(""Average loss after 5 initializations: {:.3f}"".format(average_loss))
                print()","As indicated, the backend being used is Tensorflow. With the Tensorflow backend the current model is not destroyed, so you need to clear the session.

After the usage of the model just put:

if K.backend() == 'tensorflow':
    K.clear_session()


Include the backend:

from keras import backend as K


Also you can use sklearn wrapper to do grid search. Check this example: here. Also for more advanced hyperparameter search you can use hyperas.",03-06-2016 21:47,03-06-2016 22:00,"2
            
        
        
            
                
                Are you using tensorflow or theano backend?
                
                
– indraforyou
                
                
                    Commented
                    Feb 5, 2017 at 1:12
                
            
        
    
    
        
            
            
        
        
            
                
                @indraforyou I'm using the Tensorflow backend, sorry for not mentioning that!
                
                
– Alex
                
                
                    Commented
                    Feb 5, 2017 at 1:17
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                You are amazing! Thanks a lot, this is exactly what I needed to understand. And thanks a lot for pointing me to hyperopt/hyperas, too!
                
                
– Alex
                
                
                    Commented
                    Feb 5, 2017 at 2:51
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                I thought keras.wrappers.scikit_learn.KerasClassifier took care of it. I didn't face problems running the given example. If you are facing problem please submit an issue at Keras github.
                
                
– indraforyou
                
                
                    Commented
                    Jan 10, 2018 at 10:33
                
            
        
    
    
        
            
            
        
        
            
                
                @indraforyou How am I supposed to use K.clear_session() while using GridSearchCV API, that is grid.fit?
                
                
– tail
                
                
                    Commented
                    Sep 18, 2023 at 10:47
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42264649,keras.models.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,IF,Keras,Train,Static,0,<python><keras>,17,42782.16528,42782.57153,18,2041,5171213,2041,5171213,1.928571429,1.928571429,https://stackoverflow.com/questions/42411891,"I'm following this tutorial (section 6: Tying it All Together), with my own dataset. I can get the example in the tutorial working, no problem, with the sample dataset provided.
I'm getting a binary cross-entropy error that is negative, and no improvements as epochs progress. I'm pretty sure binary cross-entropy should always be positive, and I should see some improvement in the loss. I've truncated the sample output (and code call) below to 5 epochs. Others seem to run into similar problems sometimes when training CNNs, but I didn't see a clear solution in my case. Does anyone know why this is happening?
Sample output:
Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN Black, pci bus id: 0000:84:00.0)
10240/10240 [==============================] - 2s - loss: -5.5378 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 2/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 3/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 4/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 5/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000

My code:
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

dataset = np.loadtxt('train_rows.csv', delimiter="","")
testset = np.loadtxt('test_rows.csv', delimiter="","")

# split into input (X) and output (Y) variables
X = dataset[:, :62]
Y = dataset[:, 62]

X_test = testset[:, :62]
Y_test = testset[:, 62]

### create model
model = Sequential()
model.add(Dense(100, input_dim=(62,), activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
## Fit the model
model.fit(X, Y, validation_data=(X_test, Y_test), epochs=5, batch_size=128)","I should have printed out my response variable. The categories were labelled as 1 and 2 instead of 0 and 1, which confused the classifier.",03-12-2016 02:55,03-12-2016 21:22,"Add a comment
                 |","9
            
        
        
            
                
                In my case, I had an autoencoder with this problem. It turned out that 4 of my columns in my matrix had values larger than 1, whereas the remaining 284 columns were in the range [0,1]. Rebasing the columns with values larger than 1 (using their max values), fixed my issue.
                
                
– Shadi
                
                
                    Commented
                    Jul 5, 2017 at 14:28
                
            
        
    
    
        
            
                    17
            
        
        
            
                
                I want to share my case too. I use a CNN U-Net for training image segmentation w/ cross binary-entropy. I happened to convert my masks from True/False to 255/0 and that confused the classifier and caused negative losses.
                
                
– Nicole Finnie
                
                
                    Commented
                    Mar 10, 2018 at 23:21
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @PreetomSahaArko I normalized the values between 0 and 1 github.com/nicolefinnie/kaggle-dsb2018/blob/master/src/modules/…
                
                
– Nicole Finnie
                
                
                    Commented
                    Aug 22, 2018 at 8:29
                
                        
                            
                        
            
        
    
    
        
            
                    2
            
        
        
            
                
                thanks for sharing. Indeed, I've had the same target classes: [2, 1], that were remapped from the source values: [-1, 1]. With source values, loss is still sometimes negative. Log loss: mean(y_true*log(y_pred) + (1-y_true)log(1-y_pred)). Where you get log of a negative (which is undefined), the system ends up optimizing for only half of samples. Would be cool, if somebody could validate my thinking here.
                
                
– D_K
                
                
                    Commented
                    Aug 23, 2019 at 12:37
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                As mentioned in the accepted answer, incorrect labels is one of the primary culprits I've seen across several examples.
                
                
– Shay
                
                
                    Commented
                    Mar 3, 2020 at 7:44
                
            
        
    

            
	    

        
                    
                 | 
            Show 2 more comments"
https://stackoverflow.com/questions/42284873,keras.preprocessing.image.ImageDataGenerator.flow_from_directory,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Load,Static,0,<python><arrays><numpy><keras>,7,42782.88958,42874.65,11,171,8014273,1091,5197034,4,1.555555556,https://stackoverflow.com/questions/42467734,"I'm using the ImageDataGenerator inside Keras to read a directory of images. I'd like to save the result inside a numpy array, so I can do further manipulations and save it to disk in one file.

flow_from_directory() returns an iterator, which is why I tried the following

itr = gen.flow_from_directory('data/train/', batch_size=1, target_size=(32,32))
imgs = np.concatenate([itr.next() for i in range(itr.nb_sample)])


but that produced

ValueError: could not broadcast input array from shape (32,32,3) into shape (1)


I think I'm misusing the concatenate() function, but I can't figure out where I fail.","I had the same problem and solved it the following way:
itr.next returns the next batch of images as two numpy.ndarray objects: batch_x, batch_y. (Source: keras/preprocessing/image.py)
So what you can do is set the batch_size for flow_from_directory to the size of your whole train dataset.
Example, my whole training set consists of 1481 images:
train_datagen = ImageDataGenerator(rescale=1. / 255)
itr = train_datagen.flow_from_directory(
train_data_dir,
target_size=(img_width, img_height),
batch_size=1481,
class_mode='categorical')

X, y = itr.next()",3/15/2016 9:55,3/15/2016 15:52,"i partly solved my problem by adding a [0] behind itr.next(). however this only gives me the x-data and i have to do the same again with [1] for the y-data. i then fail to merge the two given (A,B,C,D) and (A,E) to shape (A,B,C,D,E).
                
                
– pietz
                
                
                    Commented
                    Feb 17, 2017 at 15:52
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                How to deal with large datasets? If I follow your approach I run out of memory for large datasets. Any idea?
                
                
– Grigorios Kalliatakis
                
                
                    Commented
                    Dec 4, 2019 at 11:27
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                I think the only solution is to either increase your machines RAM or play with the batch size and iterating over it.
                
                
– Florian
                
                
                    Commented
                    Dec 4, 2019 at 12:31
                
            
        
    
    
        
            
            
        
        
            
                
                How would you iterate through the whole dataset with smaller batches? thanks
                
                
– Grigorios Kalliatakis
                
                
                    Commented
                    Dec 4, 2019 at 13:03
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42475381," keras.models,keras.applications.vgg16.VGG16,keras.layers.Dropout",AMO,F,F,F,Missing Required State-specific Method Order,Unknown,Keras,Model Construction,Static,0,<python><keras>,28,42792.96389,42793.03056,42,1773,1647057,1773,1647057,8.672727273,8.672727273,https://stackoverflow.com/questions/42596057,"In keras.applications, there is a VGG16 model pre-trained on imagenet.

from keras.applications import VGG16
model = VGG16(weights='imagenet')


This model has the following structure.


Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 3, 224, 224)   0                                            
____________________________________________________________________________________________________
block1_conv1 (Convolution2D)     (None, 64, 224, 224)  1792        input_1[0][0]                    
____________________________________________________________________________________________________
block1_conv2 (Convolution2D)     (None, 64, 224, 224)  36928       block1_conv1[0][0]               
____________________________________________________________________________________________________
block1_pool (MaxPooling2D)       (None, 64, 112, 112)  0           block1_conv2[0][0]               
____________________________________________________________________________________________________
block2_conv1 (Convolution2D)     (None, 128, 112, 112) 73856       block1_pool[0][0]                
____________________________________________________________________________________________________
block2_conv2 (Convolution2D)     (None, 128, 112, 112) 147584      block2_conv1[0][0]               
____________________________________________________________________________________________________
block2_pool (MaxPooling2D)       (None, 128, 56, 56)   0           block2_conv2[0][0]               
____________________________________________________________________________________________________
block3_conv1 (Convolution2D)     (None, 256, 56, 56)   295168      block2_pool[0][0]                
____________________________________________________________________________________________________
block3_conv2 (Convolution2D)     (None, 256, 56, 56)   590080      block3_conv1[0][0]               
____________________________________________________________________________________________________
block3_conv3 (Convolution2D)     (None, 256, 56, 56)   590080      block3_conv2[0][0]               
____________________________________________________________________________________________________
block3_pool (MaxPooling2D)       (None, 256, 28, 28)   0           block3_conv3[0][0]               
____________________________________________________________________________________________________
block4_conv1 (Convolution2D)     (None, 512, 28, 28)   1180160     block3_pool[0][0]                
____________________________________________________________________________________________________
block4_conv2 (Convolution2D)     (None, 512, 28, 28)   2359808     block4_conv1[0][0]               
____________________________________________________________________________________________________
block4_conv3 (Convolution2D)     (None, 512, 28, 28)   2359808     block4_conv2[0][0]               
____________________________________________________________________________________________________
block4_pool (MaxPooling2D)       (None, 512, 14, 14)   0           block4_conv3[0][0]               
____________________________________________________________________________________________________
block5_conv1 (Convolution2D)     (None, 512, 14, 14)   2359808     block4_pool[0][0]                
____________________________________________________________________________________________________
block5_conv2 (Convolution2D)     (None, 512, 14, 14)   2359808     block5_conv1[0][0]               
____________________________________________________________________________________________________
block5_conv3 (Convolution2D)     (None, 512, 14, 14)   2359808     block5_conv2[0][0]               
____________________________________________________________________________________________________
block5_pool (MaxPooling2D)       (None, 512, 7, 7)     0           block5_conv3[0][0]               
____________________________________________________________________________________________________
flatten (Flatten)                (None, 25088)         0           block5_pool[0][0]                
____________________________________________________________________________________________________
fc1 (Dense)                      (None, 4096)          102764544   flatten[0][0]                    
____________________________________________________________________________________________________
fc2 (Dense)                      (None, 4096)          16781312    fc1[0][0]                        
____________________________________________________________________________________________________
predictions (Dense)              (None, 1000)          4097000     fc2[0][0]                        
====================================================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
____________________________________________________________________________________________________



I would like to fine-tune this model with dropout layers between the dense layers (fc1, fc2 and predictions), while keeping all the pre-trained weights of the model intact. I know it's possible to access each layer individually with model.layers, but I haven't found anywhere how to add new layers between the existing layers.

What's the best practice of doing this?","I found an answer myself by using Keras functional API

from keras.applications import VGG16
from keras.layers import Dropout
from keras.models import Model

model = VGG16(weights='imagenet')

# Store the fully connected layers
fc1 = model.layers[-3]
fc2 = model.layers[-2]
predictions = model.layers[-1]

# Create the dropout layers
dropout1 = Dropout(0.85)
dropout2 = Dropout(0.85)

# Reconnect the layers
x = dropout1(fc1.output)
x = fc2(x)
x = dropout2(x)
predictors = predictions(x)

# Create a new model
model2 = Model(input=model.input, output=predictors)


model2 has the dropout layers as I wanted

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 3, 224, 224)   0                                            
____________________________________________________________________________________________________
block1_conv1 (Convolution2D)     (None, 64, 224, 224)  1792        input_1[0][0]                    
____________________________________________________________________________________________________
block1_conv2 (Convolution2D)     (None, 64, 224, 224)  36928       block1_conv1[0][0]               
____________________________________________________________________________________________________
block1_pool (MaxPooling2D)       (None, 64, 112, 112)  0           block1_conv2[0][0]               
____________________________________________________________________________________________________
block2_conv1 (Convolution2D)     (None, 128, 112, 112) 73856       block1_pool[0][0]                
____________________________________________________________________________________________________
block2_conv2 (Convolution2D)     (None, 128, 112, 112) 147584      block2_conv1[0][0]               
____________________________________________________________________________________________________
block2_pool (MaxPooling2D)       (None, 128, 56, 56)   0           block2_conv2[0][0]               
____________________________________________________________________________________________________
block3_conv1 (Convolution2D)     (None, 256, 56, 56)   295168      block2_pool[0][0]                
____________________________________________________________________________________________________
block3_conv2 (Convolution2D)     (None, 256, 56, 56)   590080      block3_conv1[0][0]               
____________________________________________________________________________________________________
block3_conv3 (Convolution2D)     (None, 256, 56, 56)   590080      block3_conv2[0][0]               
____________________________________________________________________________________________________
block3_pool (MaxPooling2D)       (None, 256, 28, 28)   0           block3_conv3[0][0]               
____________________________________________________________________________________________________
block4_conv1 (Convolution2D)     (None, 512, 28, 28)   1180160     block3_pool[0][0]                
____________________________________________________________________________________________________
block4_conv2 (Convolution2D)     (None, 512, 28, 28)   2359808     block4_conv1[0][0]               
____________________________________________________________________________________________________
block4_conv3 (Convolution2D)     (None, 512, 28, 28)   2359808     block4_conv2[0][0]               
____________________________________________________________________________________________________
block4_pool (MaxPooling2D)       (None, 512, 14, 14)   0           block4_conv3[0][0]               
____________________________________________________________________________________________________
block5_conv1 (Convolution2D)     (None, 512, 14, 14)   2359808     block4_pool[0][0]                
____________________________________________________________________________________________________
block5_conv2 (Convolution2D)     (None, 512, 14, 14)   2359808     block5_conv1[0][0]               
____________________________________________________________________________________________________
block5_conv3 (Convolution2D)     (None, 512, 14, 14)   2359808     block5_conv2[0][0]               
____________________________________________________________________________________________________
block5_pool (MaxPooling2D)       (None, 512, 7, 7)     0           block5_conv3[0][0]               
____________________________________________________________________________________________________
flatten (Flatten)                (None, 25088)         0           block5_pool[0][0]                
____________________________________________________________________________________________________
fc1 (Dense)                      (None, 4096)          102764544   flatten[0][0]                    
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 4096)          0           fc1[0][0]                        
____________________________________________________________________________________________________
fc2 (Dense)                      (None, 4096)          16781312    dropout_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 4096)          0           fc2[1][0]                        
____________________________________________________________________________________________________
predictions (Dense)              (None, 1000)          4097000     dropout_2[0][0]                  
====================================================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
____________________________________________________________________________________________________",3/22/2016 15:54,3/22/2016 17:29,"Add a comment
                 |","How to add BatchNormalization with in this functional API?
                
                
– Malathi
                
                
                    Commented
                    Dec 25, 2019 at 16:02
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                @Malathi It was a while ago I worked on Keras last time, but I believe it should be similar. Make sure the initial parameters of the batch normalization has mean 0 and std 1 (I'm not sure if this is the default in Keras). If you have problems you can always ask a new question and hopefully someone more updated than me can answer you.
                
                
– oscfri
                
                
                    Commented
                    Dec 26, 2019 at 17:15
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                How would your solution to this problem look like if you were using Keras Sequential API?
                
                
– Gilfoyle
                
                
                    Commented
                    Mar 1, 2020 at 16:54
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42492824,"keras.models.compile,keras.utils.np_utils.to_categorical,sklearn.model_selection.cross_val_score",Hybrid,SAI,"F,IC-1",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Crash,Keras,Model Evaluation,Static,0,<python><machine-learning><scikit-learn><neural-network><keras>,12,42793.75625,42793.86319,6,8698,7137636,2445,3849791,2.501501501,6.5,https://stackoverflow.com/questions/42606207,"I want the classifier to run faster and stop early if the patience reaches the number I set. In the following code it does 10 iterations of fitting the model.

import numpy
import pandas
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.constraints import maxnorm
from keras.optimizers import SGD
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataframe = pandas.read_csv(""sonar.csv"", header=None)
dataset = dataframe.values
# split into input (X) and output (Y) variables
X = dataset[:,0:60].astype(float)
Y = dataset[:,60]
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)

calls=[EarlyStopping(monitor='acc', patience=10), ModelCheckpoint('C:/Users/Nick/Data Science/model', monitor='acc', save_best_only=True, mode='auto', period=1)]

def create_baseline(): 
    # create model
    model = Sequential()
    model.add(Dropout(0.2, input_shape=(33,)))
    model.add(Dense(33, init='normal', activation='relu', W_constraint=maxnorm(3)))
    model.add(Dense(16, init='normal', activation='relu', W_constraint=maxnorm(3)))
    model.add(Dense(122, init='normal', activation='softmax'))
    # Compile model
    sgd = SGD(lr=0.1, momentum=0.8, decay=0.0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model

numpy.random.seed(seed)
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=300, batch_size=16, verbose=0, callbacks=calls)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)
print(""Baseline: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))


Here is the resulting error-

RuntimeError: Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x000000001D691438>, as the constructor does not seem to set parameter callbacks


I changed the cross_val_score in the following-

numpy.random.seed(seed)
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=300, batch_size=16, verbose=0, callbacks=calls)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold, fit_params={'callbacks':calls})
print(""Baseline: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))


and now I get this error-

ValueError: need more than 1 value to unpack


This code came from here. The code is by far the most accurate I've used so far. The problem is that there is no defined model.fit() anywhere in the code. It also takes forever to fit. The fit() operation occurs at the results = cross_val_score(...) and there's no parameters to throw a callback in there.

How do I go about doing this?
Also, how do I run the model trained on a test set?

I need to be able to save the trained model for later use...","Reading from here, which is the source code of KerasClassifier, you can pass it the arguments of fit and they should be used.
I don't have your dataset so I cannot test it, but you can tell me if this works and if not I will try and adapt the solution. Change this line :

estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=300, batch_size=16, verbose=0, callbacks=[...your_callbacks...])))


A small explaination of what's happening : KerasClassifier is taking all the possibles arguments for fit, predict, score and uses them accordingly when each method is called. They made a function that filters the arguments that should go to each of the above functions that can be called in the pipeline. 
I guess there are several fit and predict calls inside the StratifiedKFold step to train on different splits everytime. 

The reason why it takes forever to fit and it fits 10 times is because one fit is doing 300 epochs, as you asked. So the KFold is repeating this step over the different folds :


calls fit with all the parameters given to KerasClassifier (300 epochs and batch size = 16). It's training on 9/10 of your data and using 1/10 as validation. 


EDIT :

Ok, so I took the time to download the dataset and try your code... First of all you need to correct a ""few"" things in your network : 


your input have a 60 features. You clearly show it in your data prep :

X = dataset[:,:60].astype(float)


so why would you have this :

model.add(Dropout(0.2, input_shape=(33,)))


please change to : 

model.add(Dropout(0.2, input_shape=(60,)))

About your targets/labels. You changed the objective from the original code (binary_crossentropy) to categorical_crossentropy. But you didn't change your Y array. So either do this in your data preparation :

from keras.utils.np_utils import to_categorical
encoded_Y = to_categorical(encoder.transform(Y))


or change your objective back to binary_crossentropy.
Now the network's output size : 122 on the last dense layer? your dataset obviously has 2 categories so why are you trying to output 122 classes? it won't match the target. Please change back your last layer to :

model.add(Dense(2, init='normal', activation='softmax'))


if you choose to use categorical_crossentropy, or 

model.add(Dense(1, init='normal', activation='sigmoid'))


if you go back to binary_crossentropy.


So now that your network compiles, I could start to troubleshout.

here is your solution

So now I could get the real error message. It turns out that when you feed fit_params=whatever in the cross_val_score() function, you are feeding those parameters to a pipeline. In order to know to which part of the pipeline you want to send those parameters you have to specify it like this :

fit_params={'mlp__callbacks':calls}


Your error was saying that the process couldn't unpack 'callbacks'.split('__', 1) into 2 values. It was actually looking for the name of the pipeline's step to apply this to.

It should be working now :)

results = cross_val_score(pipeline, X, encoded_Y, cv=kfold, fit_params={'mlp__callbacks':calls})


BUT, you should be aware of what's happening here... the cross validation actually calls the create_baseline() function to recreate the model from scratch 10 times an trains it 10 times on different parts of the dataset. So it's not doing epochs as you were saying, it's doing 300 epochs 10 times. 
What is also happening as a consequence of using this tool : since the models are always differents, it means the fit() method is applied 10 times on different models, therefore, the callbacks are also applied 10 different times and the files saved by ModelCheckpoint() get overriden and you find yourself only with the best model of the last run.

This is intrinsec to the tools you use, I don't see any way around this. This comes as consequence to using different general tools that weren't especially thought to be used together with all the possible configurations.",3/24/2016 6:28,3/24/2016 15:33,"Could you provide us a callback list definition also?
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Feb 27, 2017 at 23:54
                
            
        
    
    
        
            
            
        
        
            
                
                Can you show where the error occurs? A bit more of the trace
                
                
– Nassim Ben
                
                
                    Commented
                    Feb 28, 2017 at 5:52
                
            
        
    
    
        
            
            
        
        
            
                
                And remove the list of callbacks from KerasClassifier constructor.
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Feb 28, 2017 at 8:31
                
            
        
    
    
        
            
            
        
        
            
                
                solved it, see my edit :)
                
                
– Nassim Ben
                
                
                    Commented
                    Feb 28, 2017 at 11:50
                
            
        
    
    
        
            
            
        
        
            
                
                New question- stackoverflow.com/questions/42511743/…
                
                
– Ravaal
                
                
                    Commented
                    Feb 28, 2017 at 14:59
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                How could I have ever figured that out without your help!? You're truly brilliant sir! Also, I only shared half of my code so all the changes you mentioned didn't apply to my actual code... regardless, it works.
                
                
– Ravaal
                
                
                    Commented
                    Feb 28, 2017 at 14:17
                
            
        
    
    
        
            
            
        
        
            
                
                actually you can find the ' __ ' info in the pipeline doc :   The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.  link : scikit-learn.org/stable/modules/generated/…
                
                
– Nassim Ben
                
                
                    Commented
                    Feb 28, 2017 at 14:19
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                I have a way to save every best model instead of overwriting them... now I'm wondering if theres a way that I can amalgamate different models.
                
                
– Ravaal
                
                
                    Commented
                    Feb 28, 2017 at 14:25
                
            
        
    
    
        
            
            
        
        
            
                
                New question- stackoverflow.com/questions/42511743/…
                
                
– Ravaal
                
                
                    Commented
                    Feb 28, 2017 at 14:59
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42596057,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Train,Static,0,<python><machine-learning><neural-network><deep-learning><keras>,22,42798.52153,42798.54097,29,28691,5974433,756,6354442,3.18125,0,https://stackoverflow.com/questions/42762849,"I got the following error when I tried to train an MLP model in keras(I am using keras version 1.2.2)


  Error when checking model input: the list of Numpy arrays that you
  are passing to your model is not the size the model expected. Expected
  to see 1 arrays but instead got the following list of 12859 arrays:


This is the summary of the model

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
dense_1 (Dense)                  (None, 20)            4020        dense_input_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 2)             42          dense_1[0][0]
====================================================================================================
Total params: 4,062
Trainable params: 4,062
Non-trainable params: 0
____________________________________________________________________________________________________
None


This is the first line of model

 model.add(Dense(20, input_shape=(200,), init='lecun_uniform', activation='tanh'))


For training:

model.fit(X,Y,nb_epoch=100,verbose=1)


where X is a list of elements and each element in turn is a list of 200 values.

Edit :

I also tried

model.add(Dense(20, input_shape=(12859,200), init='lecun_uniform', activation='tanh'))


but I am getting the same error","Your error comes from the fact that your X for some reason wasn't transformed to a numpy.array. In this your X is treated as a list of rows and this is a reason behind your error message (that it expected one input instead of list which has a number of rows elements). Transformation:

X = numpy.array(X)
Y = numpy.array(Y)


I would check a data loading process because something might go wrong there.

UPDATE:

As it was mentioned in a comment - input_shape need to be changed to input_dim.

UPDATE 2:

In order to keep input_shape one should change to it to input_shape=(200,).",3/26/2016 1:21,3/26/2016 8:38,"1
            
        
        
            
                
                Try using the keyword input_dim instead: input_dim=200 which defines the number of input nodes. The number of samples is variable then. With input_shape you have to specify the full shape, i.e. also the number of samples (input_shape=(len(X), 200)).
                
                
– a_guest
                
                
                    Commented
                    Mar 4, 2017 at 12:39
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                How about model.fit(numpy.array(X), Y,nb_epoch=100,verbose=1)  it seems that for some reason your X is not a numpy array.
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Mar 4, 2017 at 12:41
                
                        
                            
                        
            
        
    
    
        
            
                    2
            
        
        
            
                
                Try passing a np.array(X) instead of a list of np.array to model.fit.
                
                
– musically_ut
                
                
                    Commented
                    Mar 4, 2017 at 12:43
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Exactly. It would be better if you provided a full code - with X and Y definition. Try to print out type(X).
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Mar 4, 2017 at 12:44
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Thanks guys .. it seems to work with input_dim=200 , passing np.array(X).
                
                
– MysticForce
                
                
                    Commented
                    Mar 4, 2017 at 12:53
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |","Thank you for summarizing .. I also had to change input_shape to input_dim . Please add that to your answer :)
                
                
– MysticForce
                
                
                    Commented
                    Mar 4, 2017 at 13:02
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Done. If you want to keep input_shape then try input_shape=(200,).
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Mar 4, 2017 at 13:04
                
            
        
    
    
        
            
            
        
        
            
                
                Worked for me too. Oh god! This is such a dumb error. Keras developers should've written some code to just automatically convert the array into a numpy array instead of showing this dumb error!!!
                
                
– Julian
                
                
                    Commented
                    Dec 16, 2019 at 4:28
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42606207,keras.models.compile,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Unknown,Keras,Model Construction,Static,0,<python><machine-learning><tensorflow><classification><keras>,10,42799.35764,42799.42917,20,8698,7137636,2625,1928515,2.501501501,0,https://stackoverflow.com/questions/42785433,"I'm doing a binary classification using Keras (with Tensorflow backend) and I've got about 76% precision and 70% recall. Now I want to try to play with decision threshold. As far as I know Keras uses decision threshold 0.5. Is there a way in Keras to use custom threshold for decision precision and recall?

Thank you for your time!","create custom metrics like this :

Edited thanks to @Marcin : Create functions that returns the desired metrics with threshold_value as argument

def precision_threshold(threshold=0.5):
    def precision(y_true, y_pred):
        """"""Precision metric.
        Computes the precision over the whole batch using threshold_value.
        """"""
        threshold_value = threshold
        # Adaptation of the ""round()"" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.
        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())
        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.
        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))
        # count the predicted positives
        predicted_positives = K.sum(y_pred)
        # Get the precision ratio
        precision_ratio = true_positives / (predicted_positives + K.epsilon())
        return precision_ratio
    return precision

def recall_threshold(threshold = 0.5):
    def recall(y_true, y_pred):
        """"""Recall metric.
        Computes the recall over the whole batch using threshold_value.
        """"""
        threshold_value = threshold
        # Adaptation of the ""round()"" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.
        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())
        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.
        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))
        # Compute the number of positive targets.
        possible_positives = K.sum(K.clip(y_true, 0, 1))
        recall_ratio = true_positives / (possible_positives + K.epsilon())
        return recall_ratio
    return recall


now you can use them in

model.compile(..., metrics = [precision_threshold(0.1), precision_threshold(0.2),precision_threshold(0.8), recall_threshold(0.2,...)])


I hope this helps :)",3/29/2016 14:00,3/29/2016 16:27,"For completion's sake: Keras now natively includes a threshold(s) parameter for metrics based on True/False Positives/Negatives.
                
                
– JS Lavertu
                
                
                    Commented
                    Nov 8, 2021 at 16:13
                
            
        
    

            
	    

        
                    Add a comment
                 |","@NassimBen  nice solution. I would like to do something quite similar but dynmaically caclulate the threshold_value based on the kth largest value in y_pred: I've asked the question here: stackoverflow.com/questions/45720458/…
                
                
– notconfusing
                
                
                    Commented
                    Aug 16, 2017 at 18:18
                
            
        
    
    
        
            
            
        
        
            
                
                if I give it different threshold values and save the model at what precision or recall value the model will the model will be saved ?
                
                
– Mohsin
                
                
                    Commented
                    Nov 8, 2017 at 6:21
                
            
        
    
    
        
            
            
        
        
            
                
                here is [another method] (stackoverflow.com/questions/52041931/…), I don't know why these two code produce different results for the same threshold, and they are all different from the value I count with the predict result (while keras_metrics.precision()  returns the correct answer for 0.5 threshold).
                
                
– yang
                
                
                    Commented
                    Mar 20, 2019 at 4:14
                
            
        
    
    
        
            
            
        
        
            
                
                @Mohsin I believe the model won't save precision or recall value at all. They are for evaluation only. After a training, you may save weights, these weights are aim to reducing loss value.
                
                
– yang
                
                
                    Commented
                    Mar 20, 2019 at 4:18
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/42699956," keras.backend.image_dim_ordering,keras.layers.Conv3D",Hybrid,SAI,"F,IC-1,MT",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Crash,Keras,Model Construction,Static,0,<python><numpy><tensorflow><keras><conv-neural-network>,6,42803.66667,42803.67847,7,1770,6076911,75,7006585,1.533333333,0,https://stackoverflow.com/questions/42821330,"I'm having a problem feeding a 3D CNN using Keras and Python to classify 3D shapes. I have a folder with some models in JSON format. I read those models into a Numpy Array. The models are 25*25*25 and represent the occupancy grid of the voxelized model (each position represents if the voxel in position (i,j,k) has points in it or no), so I only have 1 channel of input, like grayscale images in 2D images. The code that I have is the following:

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution3D, MaxPooling3D
from keras.optimizers import SGD
from keras.utils import np_utils
from keras import backend as K

# Number of Classes and Epochs of Training
nb_classes = 3 # cube, cone or sphere
nb_epoch = 100
batch_size = 2

# Input Image Dimensions
img_rows, img_cols, img_depth = 25, 25, 25

# Number of Convolutional Filters to use
nb_filters = 32

# Convolution Kernel Size
kernel_size = [5,5,5]

X_train, Y_train = [], []

# Read from File
import os
import json

i=0
for filename in os.listdir(os.path.join(os.getcwd(), 'models')):
    with open(os.path.join(os.getcwd(), 'models', filename)) as f:
        file = f.readlines()
        json_file = '\n'.join(file)
        content = json.loads(json_file)
        occupancy = content['model']['occupancy']
        form = []
        for value in occupancy:
            form.append(int(value))
        final_model = [ [ [ 0 for i in range(img_rows) ]
                              for j in range(img_cols) ]
                              for k in range(img_depth) ]
        a = 0
        for i in range(img_rows):
            for j in range(img_cols):
                for k in range(img_depth):
                    final_model[i][j][k] = form[a]
                    a = a + 1
        X_train.append(final_model)
        Y_train.append(content['model']['label'])

X_train = np.array(X_train)
Y_train = np.array(Y_train)

# (1 channel, 25 rows, 25 cols, 25 of depth)
input_shape = (1, img_rows, img_cols, img_depth)

# Init
model = Sequential()

# 3D Convolution layer
model.add(Convolution3D(nb_filters, kernel_size[0], kernel_size[1], kernel_size[2],
                        input_shape=input_shape,
                        activation='relu'))

# Fully Connected layer
model.add(Flatten())
model.add(Dense(128,
          init='normal',
          activation='relu'))
model.add(Dropout(0.5))

# Softmax Layer
model.add(Dense(nb_classes,
                init='normal'))
model.add(Activation('softmax'))

# Compile
model.compile(loss='categorical_crossentropy',
              optimizer=SGD())

# Fit network
model.fit(X_train, Y_train, nb_epoch=nb_epoch,
         verbose=1)


After this, I get the following error


  Using TensorFlow backend. Traceback (most recent call last):   File
  ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"",
  line 670, in _call_cpp_shape_fn_impl
      status)   File ""/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py"",
  line 89, in exit
      next(self.gen)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"",
  line 469, in raise_exception_on_not_ok_status
      pywrap_tensorflow.TF_GetCode(status)) tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative
  dimension size caused by subtracting 5 from 1 for 'Conv3D' (op:
  'Conv3D') with input shapes: [?,1,25,25,25], [5,5,5,25,32].
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):   File ""CNN_3D.py"", line 76, in
  
      activation='relu'))   File ""/usr/local/lib/python3.6/site-packages/keras/models.py"", line 299, in
  add
      layer.create_input_layer(batch_input_shape, input_dtype)   File ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 401, in create_input_layer
      self(x)   File ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 572, in call
      self.add_inbound_node(inbound_layers, node_indices, tensor_indices)   File
  ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 635, in add_inbound_node
      Node.create_node(self, inbound_layers, node_indices, tensor_indices)   File
  ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 166, in create_node
      output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))   File
  ""/usr/local/lib/python3.6/site-packages/keras/layers/convolutional.py"",
  line 1234, in call
      filter_shape=self.W_shape)   File ""/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"",
  line 2831, in conv3d
      x = tf.nn.conv3d(x, kernel, strides, padding)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"",
  line 522, in conv3d
      strides=strides, padding=padding, name=name)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"",
  line 763, in apply_op
      op_def=op_def)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"",
  line 2397, in create_op
      set_shapes_for_outputs(ret)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"",
  line 1757, in set_shapes_for_outputs
      shapes = shape_func(op)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"",
  line 1707, in call_with_requiring
      return call_cpp_shape_fn(op, require_shape_fn=True)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"",
  line 610, in call_cpp_shape_fn
      debug_python_shape_fn, require_shape_fn)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"",
  line 675, in _call_cpp_shape_fn_impl
      raise ValueError(err.message) ValueError: Negative dimension size caused by subtracting 5 from 1 for 'Conv3D' (op: 'Conv3D') with input
  shapes: [?,1,25,25,25], [5,5,5,25,32].


What am I doing wrong to get this error?","I think that the problem is that you are setting the input shape in Theano ordering but you are using Keras with Tensorflow backend and Tensorflow img ordering. In addition the y_train array has to be converted to categorical labels.

Updated code:

from keras.utils import np_utils
from keras import backend as K

if K.image_dim_ordering() == 'th':
    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols, img_depth)
    input_shape = (1, img_rows, img_cols, img_depth)
else:
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, img_depth, 1)
    input_shape = (img_rows, img_cols, img_depth, 1)

Y_train = np_utils.to_categorical(Y_train, nb_classes)


Adding this lines should fix it.",04-08-2016 11:09,04-08-2016 14:15,"I think the shape of your training data is wrong. Tensorflow expects the data to be in the form of (sample, dim1, dim2, ..., channel). Given a list of regular 2D images, you can reshape like this: X_train.reshape((-1, WIDTH, HEIGHT, 1)). Adapting this to your case you could try X_train = X_train.reshape((-1, img_rows, img_cols, img_depth, 1)). And the input_shape should be (img_rows, img_cols, img_depth, 1).
                
                
– Matt
                
                
                    Commented
                    Mar 9, 2017 at 16:16
                
            
        
    
    
        
            
            
        
        
            
                
                I still get the same error. I can pass the creation of the layers when I add more channels (input_shape = (5, img_rows, img_cols, img_depth)) that overcome or equal the size of the convolutional filters. But I only have a channel of input. I think that the problem is in the definition of the Conv3D layer
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:21
                
            
        
    
    
        
            
            
        
        
            
                
                I am wanting to do exactly what you are doing, but I'm having trouble figuring out how to turn a 3D model file into an occupancy grid. How are you doing that? Thank you!
                
                
– Braelyn B
                
                
                    Commented
                    May 15, 2018 at 22:38
                
            
        
    

            
	    

        
                    Add a comment
                 |","Using both codes almost had it. But now I get a new error: Using TensorFlow backend. Traceback (most recent call last): File ""CNN_3D_2.py"", line 86, in <module> verbose=1) ... ValueError: Error when checking model input: expected convolution3d_input_1 to have shape (None, 25, 25, 25, 1) but got array with shape (1, 25, 25, 25, 2)
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:27
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                can you tell me your original (before reshaping) X_train.shape ??
                
                
– David de la Iglesia
                
                
                    Commented
                    Mar 9, 2017 at 16:33
                
            
        
    
    
        
            
            
        
        
            
                
                (2, 25, 25, 25)
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:35
                
            
        
    
    
        
            
            
        
        
            
                
                I updated whe code with the reshape @JoãoPedroFontes. Try now
                
                
– David de la Iglesia
                
                
                    Commented
                    Mar 9, 2017 at 16:37
                
            
        
    
    
        
            
            
        
        
            
                
                David, it is giving the error ValueError: Input 0 is incompatible with layer convolution3d_1: expected ndim=5, found ndim=4. The example that you gave me is not for 2D? In the input shape you only give rows and columns. I tried adding the img_depths and it gave ValueError: Error when checking model target: expected activation_1 to have shape (None, 3) but got array with shape (2, 1)
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:44
                
            
        
    

            
	    

        
                    
                 | 
            Show 6 more comments"
https://stackoverflow.com/questions/42699956,"keras.models.compile,keras.models.fit,keras.utils.np_utils.to_categorical",Hybrid,SAI,"F,IC-1",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Crash,Keras,Model Evaluation,Static,0,<python><numpy><tensorflow><keras><conv-neural-network>,6,42803.66667,42803.67847,7,1770,6076911,75,7006585,1.533333333,0,https://stackoverflow.com/questions/42886049,"I'm having a problem feeding a 3D CNN using Keras and Python to classify 3D shapes. I have a folder with some models in JSON format. I read those models into a Numpy Array. The models are 25*25*25 and represent the occupancy grid of the voxelized model (each position represents if the voxel in position (i,j,k) has points in it or no), so I only have 1 channel of input, like grayscale images in 2D images. The code that I have is the following:

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution3D, MaxPooling3D
from keras.optimizers import SGD
from keras.utils import np_utils
from keras import backend as K

# Number of Classes and Epochs of Training
nb_classes = 3 # cube, cone or sphere
nb_epoch = 100
batch_size = 2

# Input Image Dimensions
img_rows, img_cols, img_depth = 25, 25, 25

# Number of Convolutional Filters to use
nb_filters = 32

# Convolution Kernel Size
kernel_size = [5,5,5]

X_train, Y_train = [], []

# Read from File
import os
import json

i=0
for filename in os.listdir(os.path.join(os.getcwd(), 'models')):
    with open(os.path.join(os.getcwd(), 'models', filename)) as f:
        file = f.readlines()
        json_file = '\n'.join(file)
        content = json.loads(json_file)
        occupancy = content['model']['occupancy']
        form = []
        for value in occupancy:
            form.append(int(value))
        final_model = [ [ [ 0 for i in range(img_rows) ]
                              for j in range(img_cols) ]
                              for k in range(img_depth) ]
        a = 0
        for i in range(img_rows):
            for j in range(img_cols):
                for k in range(img_depth):
                    final_model[i][j][k] = form[a]
                    a = a + 1
        X_train.append(final_model)
        Y_train.append(content['model']['label'])

X_train = np.array(X_train)
Y_train = np.array(Y_train)

# (1 channel, 25 rows, 25 cols, 25 of depth)
input_shape = (1, img_rows, img_cols, img_depth)

# Init
model = Sequential()

# 3D Convolution layer
model.add(Convolution3D(nb_filters, kernel_size[0], kernel_size[1], kernel_size[2],
                        input_shape=input_shape,
                        activation='relu'))

# Fully Connected layer
model.add(Flatten())
model.add(Dense(128,
          init='normal',
          activation='relu'))
model.add(Dropout(0.5))

# Softmax Layer
model.add(Dense(nb_classes,
                init='normal'))
model.add(Activation('softmax'))

# Compile
model.compile(loss='categorical_crossentropy',
              optimizer=SGD())

# Fit network
model.fit(X_train, Y_train, nb_epoch=nb_epoch,
         verbose=1)


After this, I get the following error


  Using TensorFlow backend. Traceback (most recent call last):   File
  ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"",
  line 670, in _call_cpp_shape_fn_impl
      status)   File ""/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py"",
  line 89, in exit
      next(self.gen)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"",
  line 469, in raise_exception_on_not_ok_status
      pywrap_tensorflow.TF_GetCode(status)) tensorflow.python.framework.errors_impl.InvalidArgumentError: Negative
  dimension size caused by subtracting 5 from 1 for 'Conv3D' (op:
  'Conv3D') with input shapes: [?,1,25,25,25], [5,5,5,25,32].
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):   File ""CNN_3D.py"", line 76, in
  
      activation='relu'))   File ""/usr/local/lib/python3.6/site-packages/keras/models.py"", line 299, in
  add
      layer.create_input_layer(batch_input_shape, input_dtype)   File ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 401, in create_input_layer
      self(x)   File ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 572, in call
      self.add_inbound_node(inbound_layers, node_indices, tensor_indices)   File
  ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 635, in add_inbound_node
      Node.create_node(self, inbound_layers, node_indices, tensor_indices)   File
  ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"",
  line 166, in create_node
      output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))   File
  ""/usr/local/lib/python3.6/site-packages/keras/layers/convolutional.py"",
  line 1234, in call
      filter_shape=self.W_shape)   File ""/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"",
  line 2831, in conv3d
      x = tf.nn.conv3d(x, kernel, strides, padding)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"",
  line 522, in conv3d
      strides=strides, padding=padding, name=name)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"",
  line 763, in apply_op
      op_def=op_def)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"",
  line 2397, in create_op
      set_shapes_for_outputs(ret)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"",
  line 1757, in set_shapes_for_outputs
      shapes = shape_func(op)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"",
  line 1707, in call_with_requiring
      return call_cpp_shape_fn(op, require_shape_fn=True)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"",
  line 610, in call_cpp_shape_fn
      debug_python_shape_fn, require_shape_fn)   File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py"",
  line 675, in _call_cpp_shape_fn_impl
      raise ValueError(err.message) ValueError: Negative dimension size caused by subtracting 5 from 1 for 'Conv3D' (op: 'Conv3D') with input
  shapes: [?,1,25,25,25], [5,5,5,25,32].


What am I doing wrong to get this error?","I think that the problem is that you are setting the input shape in Theano ordering but you are using Keras with Tensorflow backend and Tensorflow img ordering. In addition the y_train array has to be converted to categorical labels.

Updated code:

from keras.utils import np_utils
from keras import backend as K

if K.image_dim_ordering() == 'th':
    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols, img_depth)
    input_shape = (1, img_rows, img_cols, img_depth)
else:
    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, img_depth, 1)
    input_shape = (img_rows, img_cols, img_depth, 1)

Y_train = np_utils.to_categorical(Y_train, nb_classes)


Adding this lines should fix it.",04-10-2016 14:22,04-11-2016 09:32,"I think the shape of your training data is wrong. Tensorflow expects the data to be in the form of (sample, dim1, dim2, ..., channel). Given a list of regular 2D images, you can reshape like this: X_train.reshape((-1, WIDTH, HEIGHT, 1)). Adapting this to your case you could try X_train = X_train.reshape((-1, img_rows, img_cols, img_depth, 1)). And the input_shape should be (img_rows, img_cols, img_depth, 1).
                
                
– Matt
                
                
                    Commented
                    Mar 9, 2017 at 16:16
                
            
        
    
    
        
            
            
        
        
            
                
                I still get the same error. I can pass the creation of the layers when I add more channels (input_shape = (5, img_rows, img_cols, img_depth)) that overcome or equal the size of the convolutional filters. But I only have a channel of input. I think that the problem is in the definition of the Conv3D layer
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:21
                
            
        
    
    
        
            
            
        
        
            
                
                I am wanting to do exactly what you are doing, but I'm having trouble figuring out how to turn a 3D model file into an occupancy grid. How are you doing that? Thank you!
                
                
– Braelyn B
                
                
                    Commented
                    May 15, 2018 at 22:38
                
            
        
    

            
	    

        
                    Add a comment
                 |","Using both codes almost had it. But now I get a new error: Using TensorFlow backend. Traceback (most recent call last): File ""CNN_3D_2.py"", line 86, in <module> verbose=1) ... ValueError: Error when checking model input: expected convolution3d_input_1 to have shape (None, 25, 25, 25, 1) but got array with shape (1, 25, 25, 25, 2)
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:27
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                can you tell me your original (before reshaping) X_train.shape ??
                
                
– David de la Iglesia
                
                
                    Commented
                    Mar 9, 2017 at 16:33
                
            
        
    
    
        
            
            
        
        
            
                
                (2, 25, 25, 25)
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:35
                
            
        
    
    
        
            
            
        
        
            
                
                I updated whe code with the reshape @JoãoPedroFontes. Try now
                
                
– David de la Iglesia
                
                
                    Commented
                    Mar 9, 2017 at 16:37
                
            
        
    
    
        
            
            
        
        
            
                
                David, it is giving the error ValueError: Input 0 is incompatible with layer convolution3d_1: expected ndim=5, found ndim=4. The example that you gave me is not for 2D? In the input shape you only give rows and columns. I tried adding the img_depths and it gave ValueError: Error when checking model target: expected activation_1 to have shape (None, 3) but got array with shape (2, 1)
                
                
– João Pedro Fontes
                
                
                    Commented
                    Mar 9, 2017 at 16:44
                
            
        
    

            
	    

        
                    
                 | 
            Show 6 more comments"
https://stackoverflow.com/questions/42886049," keras.backend.clear_session,keras.models.fit",AMO,G,G,G,Missing Required Method Order,MOB,Keras,Train,Static,0,<python-3.x><tensorflow><keras>,9,42813.47847,42815.69792,11,110885,3574081,137,1610120,6.406724279,0,https://stackoverflow.com/questions/43266799,"I am trying to train 1000x of Sequential models in a loop. In every loop my program leaks memory until I run out and get an OOM exception.

I already asked a similar question before
(Training multiple Sequential models in a row slows down)

and have seen others in similar problems (Keras: Out of memory when doing hyper parameter grid search)

and the solution is always to add K.clear_session() to your code after you have finished using the model. So I did that in my previous question and I am still leaking memory

Here is code to reproduce the issue.

import random
import time
from keras.models import Sequential
from keras.layers import Dense
from keras import backend as K
import tracemalloc


def run():
    tracemalloc.start()
    num_input_nodes = 12
    num_hidden_nodes = 8
    num_output_nodes = 1

    random_numbers = random.sample(range(1000), 50)
    train_x, train_y = create_training_dataset(random_numbers, num_input_nodes)

    for i in range(100):
        snapshot = tracemalloc.take_snapshot()
        for j in range(10):
            start_time = time.time()
            nn = Sequential()
            nn.add(Dense(num_hidden_nodes, input_dim=num_input_nodes, activation='relu'))
            nn.add(Dense(num_output_nodes))
            nn.compile(loss='mean_squared_error', optimizer='adam')
            nn.fit(train_x, train_y, nb_epoch=300, batch_size=2, verbose=0)
            K.clear_session()
            print(""Iteration {iter}. Current time {t}. Took {elapsed} seconds"".
                  format(iter=i*10 + j + 1, t=time.strftime('%H:%M:%S'), elapsed=int(time.time() - start_time)))

        top_stats = tracemalloc.take_snapshot().compare_to(snapshot, 'lineno')

        print(""[ Top 5 differences ]"")
        for stat in top_stats[:5]:
            print(stat)


def create_training_dataset(dataset, input_nodes):
    """"""
    Outputs a training dataset (train_x, train_y) as numpy arrays.
    Each item in train_x has 'input_nodes' number of items while train_y items are of size 1
    :param dataset: list of ints
    :param input_nodes:
    :return: (numpy array, numpy array), train_x, train_y
    """"""
    data_x, data_y = [], []
    for i in range(len(dataset) - input_nodes - 1):
        a = dataset[i:(i + input_nodes)]
        data_x.append(a)
        data_y.append(dataset[i + input_nodes])
    return numpy.array(data_x), numpy.array(data_y)

run()


Here is the output I get from the first memory debug print

/tensorflow/python/framework/ops.py:121: size=3485 KiB (+3485 KiB), count=42343 (+42343)
/tensorflow/python/framework/ops.py:1400: size=998 KiB (+998 KiB), count=8413 (+8413)
/tensorflow/python/framework/ops.py:116: size=888 KiB (+888 KiB), count=32468 (+32468)
/tensorflow/python/framework/ops.py:1185: size=795 KiB (+795 KiB), count=3179 (+3179)
/tensorflow/python/framework/ops.py:2354: size=599 KiB (+599 KiB), count=5886 (+5886)

System info:


python 3.5
keras (1.2.2)
tensorflow(1.0.0)","The memory leak stems from Keras and TensorFlow using a single ""default graph"" to store the network structure, which increases in size with each iteration of the inner for loop. 

Calling K.clear_session() frees some of the (backend) state associated with the default graph between iterations, but an additional call to tf.reset_default_graph() is needed to clear the Python state.

Note that there might be a more efficient solution: since nn does not depend on either of the loop variables, you can define it outside the loop, and reuse the same instance inside the loop. If you do that, there is no need to clear the session or reset the default graph, and performance should increase because you benefit from caching between iterations.",4/22/2016 16:11,05-09-2016 00:53,"2
            
        
        
            
                
                Can you try adding a tf.reset_default_graph() (and import tensorflow as tf at the top) after K.clear_session()?
                
                
– mrry
                
                
                    Commented
                    Mar 20, 2017 at 16:06
                
            
        
    
    
        
            
            
        
        
            
                
                Works like a charm. Thanks!
                
                
– G_E
                
                
                    Commented
                    Mar 20, 2017 at 22:07
                
            
        
    
    
        
            
            
        
        
            
                
                Trying to run keras models in a loop. Getting a ValueError: Error when checking target: expected dense_1 to have 2 dimensions, but got array with shape (2000, 10, 10) which is due to the loop. I have tried K.clear_session()  followed by tf.reset_default_graph() at the end of the loop but does not help. Any idea?
                
                
– Anakin
                
                
                    Commented
                    Sep 20, 2018 at 10:07
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |","2
            
        
        
            
                
                Not exactly. K.clear_session() already calls tf.reset_default_graph() inside.  Then you will just duplicate it, don't you?
                
                
– Dilshat
                
                
                    Commented
                    May 17, 2019 at 12:23
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                Dilshat is right, line 95 in the tensorflow_backend.py shows this. So no need for that second call.
                
                
– Markus
                
                
                    Commented
                    Jun 21, 2019 at 22:12
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/43178668,keras.models.fit,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Unknown,Keras,Train,Static,0,<python><machine-learning><neural-network><deep-learning><keras>,16,42828.30278,42828.57569,26,28691,5974433,333,6293886,3.18125,0,https://stackoverflow.com/questions/43386463,"I want to compare the computation time between different models.
During the fit the computation time per epoch is printed to the console.

Epoch 5/5
160000/160000 [==============================] - **10s** ......


I'm looking for a way to store these times in a similar way to the model metrics that are saved in each epoch and avaliable through the history object.","Try the following callback:

class TimeHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, batch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, batch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)


Then:

time_callback = TimeHistory()
model.fit(..., callbacks=[..., time_callback],...)
times = time_callback.times


In this case times should store the epoch computation times.",4/27/2016 8:08,4/27/2016 15:10,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/43196636," keras.layers.Concatenate,keras.models.Model",AMO,F,F,F,Missing Required State-specific Method Order,Unknown,Keras,Model Construction,Static,0,<python><machine-learning><keras><neural-network>,83,42829.03889,42829.07153,107,6285,4356204,2987,944000,4.210342418,0,https://stackoverflow.com/questions/43396572,"I have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this:

x1  x2  x3
 \  /   /
  y1   /
   \  /
    y2


So, I'd created a model with two layers and tried to merge them but it returns an error: The first layer in a Sequential model must get an ""input_shape"" or ""batch_input_shape"" argument. on the line result.add(merged).

Model:

first = Sequential()
first.add(Dense(1, input_shape=(2,), activation='sigmoid'))

second = Sequential()
second.add(Dense(1, input_shape=(1,), activation='sigmoid'))

result = Sequential()
merged = Concatenate([first, second])
ada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)
result.add(merged)
result.compile(optimizer=ada_grad, loss=_loss_tensor, metrics=['accuracy'])","You're getting the error because result defined as Sequential() is just a container for the model and you have not defined an input for it.

Given what you're trying to build set result to take the third input x3.
first = Sequential()
first.add(Dense(1, input_shape=(2,), activation='sigmoid'))

second = Sequential()
second.add(Dense(1, input_shape=(1,), activation='sigmoid'))

third = Sequential()
# of course you must provide the input to result which will be your x3
third.add(Dense(1, input_shape=(1,), activation='sigmoid'))

# lets say you add a few more layers to first and second.
# concatenate them
merged = Concatenate([first, second])

# then concatenate the two outputs

result = Concatenate([merged,  third])

ada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)

result.compile(optimizer=ada_grad, loss='binary_crossentropy',
               metrics=['accuracy'])

However, my preferred way of building a model that has this type of input structure would be to use the functional api.
Here is an implementation of your requirements to get you started:
from keras.models import Model
from keras.layers import Concatenate, Dense, LSTM, Input, concatenate
from keras.optimizers import Adagrad

first_input = Input(shape=(2, ))
first_dense = Dense(1, )(first_input)

second_input = Input(shape=(2, ))
second_dense = Dense(1, )(second_input)

merge_one = concatenate([first_dense, second_dense])

third_input = Input(shape=(1, ))
merge_two = concatenate([merge_one, third_input])

model = Model(inputs=[first_input, second_input, third_input], outputs=merge_two)
ada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)
model.compile(optimizer=ada_grad, loss='binary_crossentropy',
               metrics=['accuracy'])

To answer the question in the comments:

How are result and merged connected? Assuming you mean how are they concatenated.

Concatenation works like this:
  a        b         c
a b c   g h i    a b c g h i
d e f   j k l    d e f j k l

i.e rows are just joined.

Now, x1 is input to first, x2 is input into second and x3 input into third.",4/27/2016 9:19,4/27/2016 10:58,"I think this problem is known as hierarchical fusion in AI, mostly used for multimodal data.
                
                
– sugab
                
                
                    Commented
                    Jul 20, 2021 at 9:20
                
            
        
    

            
	    

        
                    Add a comment
                 |","How are result and merged (or merged2) layers connected with each other on the first part of your answer?
                
                
– rdo
                
                
                    Commented
                    Apr 4, 2017 at 15:04
                
            
        
    
    
        
            
            
        
        
            
                
                and the second question. As I understand x1 and x2 is an input for first_input, x3 for third_input. What's about second_input?
                
                
– rdo
                
                
                    Commented
                    Apr 4, 2017 at 15:05
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                second_input is passed through an  Dense layer and is concatenated  with first_input which also was passed through a Dense layer.  third_input is passed through a dense layer and the concatenated with the result of the previous concatenation (merged)
                
                
– parsethis
                
                
                    Commented
                    Apr 4, 2017 at 15:13
                
                        
                            
                        
            
        
    
    
        
            
                    3
            
        
        
            
                
                @putonspectacles The second way using the functional API works, however, the first way using a Sequential-model is not working for me in Keras 2.0.2. I've roughly checked the implementation and calling ""Concatenate([...])"" does not do much and furthermore, you cannot add it to a sequential model. I actually think one still needs to use the depricated method ""Merge([...], 'concat')"" until they update Keras. What do you think?
                
                
– LFish
                
                
                    Commented
                    Jun 12, 2017 at 20:00
                
            
        
    
    
        
            
                    4
            
        
        
            
                
                What is the difference between Concatenate() and concatenate() layers in Keras?
                
                
– Leevo
                
                
                    Commented
                    Jul 4, 2019 at 13:05
                
            
        
    

            
	    

        
                    
                 | 
            Show 6 more comments"
https://stackoverflow.com/questions/43396572,keras.layers.Conv1D,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><keras><text-classification><keras-layer>,54,42838.65556,42838.76875,103,6285,4356204,651,7027646,4.210342418,0,https://stackoverflow.com/questions/43743593,"I have tried to build a CNN with one layer, but I have some problem with it.
Indeed, the compilator says me that

ValueError: Error when checking model input: expected conv1d_1_input
to have 3 dimensions, but got array with shape (569, 30)

This is the code
import numpy
from keras.models import Sequential
from keras.layers.convolutional import Conv1D

numpy.random.seed(7)

datasetTraining = numpy.loadtxt(""CancerAdapter.csv"",delimiter="","")
X = datasetTraining[:,1:31]
Y = datasetTraining[:,0]
datasetTesting = numpy.loadtxt(""CancereEvaluation.csv"",delimiter="","")
X_test = datasetTraining[:,1:31]
Y_test = datasetTraining[:,0]

model = Sequential()
model.add(Conv1D(2,2,activation='relu',input_shape=X.shape))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=150, batch_size=5)
scores = model.evaluate(X_test, Y_test)

print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))","td; lr you need to reshape you data to have a spatial dimension for Conv1d to make sense:
X = np.expand_dims(X, axis=2) # reshape (569, 30) to (569, 30, 1) 
# now input can be set as 
model.add(Conv1D(2,2,activation='relu',input_shape=(30, 1))

Essentially reshaping a dataset that looks like this:
features    
.8, .1, .3  
.2, .4, .6  
.7, .2, .1  

To:
[[.8
.1
.3],

[.2,
 .4,
 .6
 ],

[.7,
 .2,
 .1]]
 

Explanation and examples
Normally convolution works over spatial dimensions. The kernel is ""convolved"" over the dimension producing a tensor. In the case of Conv1D, the kernel is passed over the 'steps' dimension of every example.
You will see Conv1D used in NLP where steps is a number of words in the sentence (padded to some fixed maximum length). The words would be encoded as vectors of length 4.
Here is an example sentence:
jack   .1   .3   -.52   |
is     .05  .8,  -.7    |<--- kernel is `convolving` along this dimension.
a      .5   .31  -.2    |
boy    .5   .8   -.4   \|/

And the way we would set the input to the conv in this case:
maxlen = 4
input_dim = 3
model.add(Conv1D(2,2,activation='relu',input_shape=(maxlen, input_dim))

In your case, you will treat the features as the spatial dimensions with each feature having length 1. (see below)
Here would be an example from your dataset
att1   .04    |
att2   .05    |  < -- kernel convolving along this dimension
att3   .1     |       notice the features have length 1. each
att4   .5    \|/      example have these 4 featues.

And we would set the Conv1D example as:
maxlen = num_features = 4 # this would be 30 in your case
input_dim = 1 # since this is the length of _each_ feature (as shown above)

model.add(Conv1D(2,2,activation='relu',input_shape=(maxlen, input_dim))

As you see your dataset has to be reshaped in to (569, 30, 1)
use:
X = np.expand_dims(X, axis=2) # reshape (569, 30, 1) 
# now input can be set as 
model.add(Conv1D(2,2,activation='relu',input_shape=(30, 1))

Here is a full-fledged example that you can run (I'll use the Functional API)
from keras.models import Model
from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input
import numpy as np

inp =  Input(shape=(5, 1))
conv = Conv1D(filters=2, kernel_size=2)(inp)
pool = MaxPool1D(pool_size=2)(conv)
flat = Flatten()(pool)
dense = Dense(1)(flat)
model = Model(inp, dense)
model.compile(loss='mse', optimizer='adam')

print(model.summary())

# get some data
X = np.expand_dims(np.random.randn(10, 5), axis=2)
y = np.random.randn(10, 1)

# fit model
model.fit(X, y)",05-06-2016 11:56,05-06-2016 14:07,"Add a comment
                 |","if i have data with dimension 1x690, and I implement a Conv1D layer with 40 filters of kernel size 3, when I look up the weights of that layer, it says I have 40*690*3 weights. I'm not sure I understand why this is, I thought I would only have 40*3 weights? How does it output a 1x40 shape?
                
                
– jerpint
                
                
                    Commented
                    Jul 27, 2017 at 2:06
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @parsethis, actually, your functional example worked even without reshaping X. Only using a sequential approach I managed to reproduce the error.
                
                
– revy
                
                
                    Commented
                    Sep 9, 2020 at 14:47
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/43452441,keras.applications.vgg16.VGG16,Hybrid,SAI,"F,IC-2",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Crash,Keras,Model Construction,Static,0,<python><keras><vgg-net>,8,42842.56042,42842.57708,7,6285,4356204,197,4320184,4.210342418,0,https://stackoverflow.com/questions/43833081,"I combine two VGG net in keras together to make classification task. When I run the program, it shows an error:


  RuntimeError: The name ""predictions"" is used 2 times in the model. All layer names should be unique.


I was confused because I only use prediction layer once in my code:

from keras.layers import Dense
import keras
from keras.models import  Model
model1 = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet',
                                input_tensor=None, input_shape=None,
                                pooling=None,
                                classes=1000)
model1.layers.pop()

model2 =  keras.applications.vgg16.VGG16(include_top=True, weights='imagenet',
                                input_tensor=None, input_shape=None,
                                pooling=None,
                                classes=1000)
model2.layers.pop()
for layer in model2.layers:
    layer.name = layer.name + str(""two"")
model1.summary()
model2.summary()
featureLayer1 = model1.output
featureLayer2 = model2.output
combineFeatureLayer = keras.layers.concatenate([featureLayer1, featureLayer2])
prediction = Dense(1, activation='sigmoid', name='main_output')(combineFeatureLayer)

model = Model(inputs=[model1.input, model2.input], outputs= prediction)
model.summary()


Thanks for @putonspectacles help, I follow his instruction and find some interesting part. If you use model2.layers.pop() and combine the last layer of two models using ""model.layers.keras.layers.concatenate([model1.output, model2.output])"", you will find that the last layer information is still showed using the model.summary(). But actually they do not exist in the structure. So instead, you can use model.layers.keras.layers.concatenate([model1.layers[-1].output, model2.layers[-1].output]). It looks tricky but it works.. I think it is a problem about synchronization of the log and structure.","First, based on the code you posted you have no layers with a name attribute 'predictions', so this error has nothing to do with your layer 
 Dense layer prediction: i.e:

prediction = Dense(1, activation='sigmoid', 
             name='main_output')(combineFeatureLayer)


The VGG16 model has a Dense layer with name predictions. In particular this line:

x = Dense(classes, activation='softmax', name='predictions')(x)


And since you're using two of these models you have layers with duplicate names.

What you could do is rename the layer in the second model to something other than predictions, maybe predictions_1, like so:

model2 =  keras.applications.vgg16.VGG16(include_top=True, weights='imagenet',
                                input_tensor=None, input_shape=None,
                                pooling=None,
                                classes=1000)

# now change the name of the layer inplace.
model2.get_layer(name='predictions').name='predictions_1'",05-07-2016 06:47,05-07-2016 07:33,"Add a comment
                 |","Thank you for the explanation. I print all the layers for both model1 and model2 but there is no layer with name ""predictions"". I think you mean the last layer of VGG but I have already pop the origin last layer. Could you add explanation for this?
                
                
– dashenswen
                
                
                    Commented
                    Apr 17, 2017 at 13:59
                
            
        
    
    
        
            
            
        
        
            
                
                if you're using the latest keras this  link references the layer named 'predictions' github.com/fchollet/keras/blob/…
                
                
– parsethis
                
                
                    Commented
                    Apr 17, 2017 at 14:06
                
            
        
    
    
        
            
            
        
        
            
                
                and based on the error you definitely have layers  named predictions. Did you try my suggestion?
                
                
– parsethis
                
                
                    Commented
                    Apr 17, 2017 at 14:07
                
            
        
    
    
        
            
            
        
        
            
                
                Hi, your suggestion is right but I pop the last layer before I combine two VGG. But you are right, I just update the solution and edit my problem. You will see that..
                
                
– dashenswen
                
                
                    Commented
                    Apr 17, 2017 at 14:14
                
            
        
    
    
        
            
            
        
        
            
                
                AttributeError: Can't set the attribute ""name"", likely because it conflicts with an existing read-only @property of the object. Please choose a different name.
                
                
– Geoffrey Anderson
                
                
                    Commented
                    Feb 13, 2020 at 20:42
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/43715047,keras.layers.Dense,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><tensorflow><deep-learning><keras><keras-layer>,23,42856.25694,42863.86319,20,396,7649152,3418,3747801,7.4,1.714285714,https://stackoverflow.com/questions/43871575,"I am using Windows 10, Python 3.5, and tensorflow 1.1.0. I have the following script:

import tensorflow as tf
import tensorflow.contrib.keras.api.keras.backend as K
from tensorflow.contrib.keras.api.keras.layers import Dense

tf.reset_default_graph()
init = tf.global_variables_initializer()
sess =  tf.Session()
K.set_session(sess) # Keras will use this sesssion to initialize all variables

input_x = tf.placeholder(tf.float32, [None, 10], name='input_x')    
dense1 = Dense(10, activation='relu')(input_x)

sess.run(init)

dense1.get_weights()


I get the error: AttributeError: 'Tensor' object has no attribute 'weights'

What am I doing wrong, and how do I get the weights of dense1? I have look at this and this SO post, but I still can't make it work.","If you want to get weights and biases of all layers, you can simply use:

for layer in model.layers: print(layer.get_config(), layer.get_weights())


This will print all information that's relevant.

If you want the weights directly returned as numpy arrays, you can use:

first_layer_weights = model.layers[0].get_weights()[0]
first_layer_biases  = model.layers[0].get_weights()[1]
second_layer_weights = model.layers[1].get_weights()[0]
second_layer_biases  = model.layers[1].get_weights()[1]


etc.",05-07-2016 17:57,05-07-2016 20:03,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/43855162,keras.models.compile,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><keras><custom-function><loss-function>,25,42863.78403,42864.31528,39,41498,349130,427,6589039,0.820833961,0,https://stackoverflow.com/questions/44036971,"I try to participate in my first Kaggle competition where RMSLE is given as the required loss function. For I have found nothing how to implement this loss function I tried to settle for RMSE. I know this was part of Keras in the past, is there any way to use it in the latest version, maybe with a customized function via backend?

This is the NN I designed:

from keras.models import Sequential
from keras.layers.core import Dense , Dropout
from keras import regularizers

model = Sequential()
model.add(Dense(units = 128, kernel_initializer = ""uniform"", activation = ""relu"", input_dim = 28,activity_regularizer = regularizers.l2(0.01)))
model.add(Dropout(rate = 0.2))
model.add(Dense(units = 128, kernel_initializer = ""uniform"", activation = ""relu""))
model.add(Dropout(rate = 0.2))
model.add(Dense(units = 1, kernel_initializer = ""uniform"", activation = ""relu""))
model.compile(optimizer = ""rmsprop"", loss = ""root_mean_squared_error"")#, metrics =[""accuracy""])

model.fit(train_set, label_log, batch_size = 32, epochs = 50, validation_split = 0.15)


I tried a customized root_mean_squared_error function I found on GitHub but for all I know the syntax is not what is required. I think the y_true and the y_pred would have to be defined before passed to the return but I have no idea how exactly, I just started with programming in python and I am really not that good in math...

from keras import backend as K

def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) 


I receive the following error with this function: 

ValueError: ('Unknown loss function', ':root_mean_squared_error')


Thanks for your ideas, I appreciate every help!","When you use a custom loss, you need to put it without quotes, as you pass the function object, not a string:

def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true))) 

model.compile(optimizer = ""rmsprop"", loss = root_mean_squared_error, 
              metrics =[""accuracy""])",05-11-2016 14:52,5/14/2016 1:06,"1
            
        
        
            
                
                The root_mean_squared_error you defined, seems equivalent to 'mse'(mean squared error) in keras. Just fyi.
                
                
– Kaique Santos
                
                
                    Commented
                    Jul 21, 2018 at 23:22
                
            
        
    

            
	    

        
                    Add a comment
                 |","2
            
        
        
            
                
                Works perfectly fine, thank you very much for pointing out that mistake. I really did not think about it that way as I am kind of new to programming. You would not know by any chance how to edit this custom function so that it computes the root mean square LOGARITHMIC error, would you?
                
                
– dennis
                
                
                    Commented
                    May 9, 2017 at 7:52
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                It gives me Unknown loss function:root_mean_squared_error
                
                
– Jitesh
                
                
                    Commented
                    Sep 13, 2017 at 12:41
                
            
        
    
    
        
            
            
        
        
            
                
                @Jitesh Please do not make such comments, make your own question with source code.
                
                
– Dr. Snoopy
                
                
                    Commented
                    Sep 13, 2017 at 12:42
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                This code gives this same value as MAE, not RMSE (see answer belowe).
                
                
– Jo.Hen
                
                
                    Commented
                    May 5, 2020 at 20:31
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                You should always add the import import tensorflow.keras.backend as K (I added it to the answer)
                
                
– Bersan
                
                
                    Commented
                    Mar 24, 2021 at 14:37
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 4 more comments"
https://stackoverflow.com/questions/44583254," keras.layers.model.compile,keras.layers.add",AMO,F,F,F,Missing Required State-specific Method Order,Crash,Keras,Model Construction,Static,0,<python><keras><lstm><recurrent-neural-network>,15,42902.30764,42902.33264,12,591,5697891,591,5697891,14,14,https://stackoverflow.com/questions/44746507,"I am trying for multi-class classification and here are the details of my training input and output:


  train_input.shape= (1, 95000, 360) (95000 length input array with each
  element being an array of 360 length)
  
  train_output.shape = (1, 95000, 22) (22 Classes are there)


model = Sequential()

model.add(LSTM(22, input_shape=(1, 95000,360)))
model.add(Dense(22, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(train_input, train_output, epochs=2, batch_size=500)


The error is:


  ValueError: Input 0 is incompatible with layer lstm_13: expected ndim=3, found ndim=4
  in line:
  model.add(LSTM(22, input_shape=(1, 95000,360)))


Please help me out, I am not able to solve it through other answers.","I solved the problem by making 


  input size: (95000,360,1) and
  output size: (95000,22)


and changed the input shape to (360,1) in the code where model is defined:

model = Sequential()
model.add(LSTM(22, input_shape=(360,1)))
model.add(Dense(22, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(ml2_train_input, ml2_train_output_enc, epochs=2, batch_size=500)",06-07-2016 21:24,06-07-2016 21:52,"here the fastest and correct way to create data for LSTM/RNN: stackoverflow.com/a/62570576/10375049
                
                
– Marco Cerliani
                
                
                    Commented
                    Mar 5, 2021 at 13:57
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/44627977,keras.layers.RepeatVector,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Evaluation,Static,0,<python><deep-learning><keras><keras-2>,7,42905.43056,42905.59792,9,1092,5536887,297,5807031,2.659090909,0,https://stackoverflow.com/questions/44835223,"I'm trying to build a model as ilustrated in below diagram. The idea is to take more than one categorical features (one-hot vectors) and embed them separately, then combine those embedded vectors with a 3D tensor for a LSTM.

With following code in Keras2.0.2, when creating the Model() object with multiple inputs, it raises an AttributeError: 'NoneType' object has no attribute 'inbound_nodes' similar to this question. Can anyone help me to figure out what's the problem?

Model:



Code:

from keras.layers import Dense, LSTM, Input
from keras.layers.merge import concatenate
from keras import backend as K
from keras.models import Model

cat_feats_dims = [315, 14] # Dimensions of the cat_feats
emd_inputs = [Input(shape=(in_size,)) for in_size in cat_feats_dims]
emd_out = concatenate([Dense(20, use_bias=False)(inp) for inp in emd_inputs])
emd_out_3d = K.repeat(emd_out, 10)

lstm_input = Input(shape=(10,5))

merged = concatenate([emd_out_3d,lstm_input])

lstm_output = LSTM(32)(merged)
dense_output = Dense(1, activation='linear')(lstm_output)

model = Model(inputs=emd_inputs+[lstm_input], outputs=[dense_output])

#ERROR MESSAGE
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda2\envs\mle-env\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-a9da7f276aa7>"", line 14, in <module>
    model = Model(inputs=emd_inputs+[lstm_input], outputs=[dense_output])
  File ""C:\Program Files\Anaconda2\envs\mle-env\lib\site-packages\keras\legacy\interfaces.py"", line 88, in wrapper
    return func(*args, **kwargs)
  File ""C:\Program Files\Anaconda2\envs\mle-env\lib\site-packages\keras\engine\topology.py"", line 1648, in __init__
    build_map_of_graph(x, seen_nodes, depth=0)
  File ""C:\Program Files\Anaconda2\envs\mle-env\lib\site-packages\keras\engine\topology.py"", line 1644, in build_map_of_graph
    layer, node_index, tensor_index)
  File ""C:\Program Files\Anaconda2\envs\mle-env\lib\site-packages\keras\engine\topology.py"", line 1644, in build_map_of_graph
    layer, node_index, tensor_index)
  File ""C:\Program Files\Anaconda2\envs\mle-env\lib\site-packages\keras\engine\topology.py"", line 1639, in build_map_of_graph
    next_node = layer.inbound_nodes[node_index]
AttributeError: 'NoneType' object has no attribute 'inbound_nodes'","keras.backend.repeat is a function, not a layer. Try using keras.layers.core.RepeatVector instead. It has the same functionality as the function.

emd_out_3d = RepeatVector(10)(emd_out)",06-08-2016 08:55,06-08-2016 15:35,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/44704435,0,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Data Preprocessing,0,0,<python><keras><lstm><recurrent-neural-network><valueerror>,25,42908.66944,42908.68194,25,1060,5140684,1060,5140684,4.8,4.8,https://stackoverflow.com/questions/44931347,"My input is simply a csv file with 339732 rows and two columns :


the first being 29 feature values, i.e. X
the second being a binary label value, i.e. Y


I am trying to train my data on a stacked LSTM model:

data_dim = 29
timesteps = 8
num_classes = 2

model = Sequential()
model.add(LSTM(30, return_sequences=True,
               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 30
model.add(LSTM(30, return_sequences=True))  # returns a sequence of vectors of dimension 30
model.add(LSTM(30))  # return a single vector of dimension 30
model.add(Dense(1, activation='softmax'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.summary()
model.fit(X_train, y_train, batch_size = 400, epochs = 20, verbose = 1)


This throws the error:


  Traceback (most recent call last):
    File ""first_approach.py"", line 80, in 
      model.fit(X_train, y_train, batch_size = 400, epochs = 20, verbose = 1)
  
  ValueError: Error when checking model input: expected lstm_1_input to
  have 3 dimensions, but got array with shape (339732, 29)


I tried reshaping my input using X_train.reshape((1,339732, 29)) but it did not work showing error:


  ValueError: Error when checking model input: expected lstm_1_input to
  have shape (None, 8, 29) but got array with shape (1, 339732, 29)


How can I feed in my input to the LSTM ?","Setting timesteps = 1 (since, I want one timestep for each instance) and reshaping the X_train and X_test as:

import numpy as np
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))


This worked!",06-12-2016 05:04,06-12-2016 12:11,"Why are you using a LSTM model with no timesteps?
                
                
– from keras import michael
                
                
                    Commented
                    Aug 27, 2018 at 22:09
                
            
        
    

            
	    

        
                    Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/44720822,keras.layers.concatenate,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Data Preprocessing,Static,0,<python><neural-network><concatenation><keras><valueerror>,8,42909.49444,42909.52569,28,28691,5974433,243,8156923,3.18125,0,https://stackoverflow.com/questions/44931689,"After some search here, I still can't find a solution for this. I'm new to Keras, apologies if there is a solution and I actually didn't understand how it was related to my problem.

I am making a small RNN with Keras 2/Functional API, and I have trouble to make the Concatenate Layer work.

Here is my structure :

inputSentence = Input(shape=(30, 91))
sentenceMatrix = LSTM(91, return_sequences=True, input_shape=(30, 91))(inputSentence)

inputDeletion = Input(shape=(30, 1))
deletionMatrix = (LSTM(30, return_sequences=True, input_shape=(30, 1)))(inputDeletion)

fusion = Concatenate([sentenceMatrix, deletionMatrix])
fusion = Dense(122, activation='relu')(fusion)
fusion = Dense(102, activation='relu')(fusion)
fusion = Dense(91, activation='sigmoid')(fusion)

F = Model(inputs=[inputSentence, inputDeletion], outputs=fusion)


And here is the error:

ValueError: Unexpectedly found an instance of type `<class 'keras.layers.merge.Concatenate'>`. Expected a symbolic tensor instance.


Full History if it helps a bit more :

Using TensorFlow backend.
    str(inputs) + '. All inputs to the layer '
ValueError: Layer dense_1 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.merge.Concatenate'>. Full input: [<keras.layers.merge.Concatenate object at 0x00000000340DC4E0>]. All inputs to the layer should be tensors.
self.assert_input_compatibility(inputs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 425, in assert_input_compatibility
fusion = Dense(122, activation='relu')(fusion)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 552, in __call__
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 419, in assert_input_compatibility
K.is_keras_tensor(x)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 392, in is_keras_tensor
raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '
ValueError: Unexpectedly found an instance of type `<class 'keras.layers.merge.Concatenate'>`. Expected a symbolic tensor instance.


I'm using Python 3.6, with Spyder 3.1.4, on Windows 7. I upgraded TensorFlow and Keras with pip this morning.

Thank you for any help provided !","Try:

fusion = concatenate([sentenceMatrix, deletionMatrix])


Concatenate is used in a Sequential model, whereas concatenate is used in a Functional API.",6/13/2016 18:57,6/28/2016 19:18,"Add a comment
                 |","1
            
        
        
            
                
                Concatenate vs concatenate   Remind me again WHY case sensitivity was a smart idea??
                
                
– Peter Cibulskis
                
                
                    Commented
                    Apr 13, 2022 at 18:25
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/45587378," keras.models.predict_classes,keras.models.fit",AMO,F,F,F,Missing Required State-specific Method Order,Unknown,Keras,Prediction,Static,0,<python><neural-network><keras>,9,42956.41458,42956.44306,12,488,6312218,2796,3128796,12,0,https://stackoverflow.com/questions/45930844,"In Keras test sample evaluation is done like this

score = model.evaluate(testx, testy, verbose=1)


This does not return predicted values. There is a method predict which return predicted values  

model.predict(testx, verbose=1)


returns

[ 
[.57 .21 .21]
[.19 .15 .64]
[.23 .16 .60] 
.....
]


testy is one hot encode and its values are like this

[
[1 0 0]
[0 0 1]
[0 0 1]
]


How can the predicted values like testy or how to convert the predicted values to one hot encoded?

note: my model looks like this

# setup the model, add layers
model = Sequential()
model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(classes, activation='softmax'))

# compile model
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])

# fit the model
model.fit(trainx, trainy, batch_size=batch_size, epochs=iterations, verbose=1, validation_data=(testx, testy))","The values being returned are probabilities of each class. Those values can be useful because they indicates the model's level of confidence.

If you are only interested in the class with the highest probability:

For example[.19 .15 .64] = 2 (because index 2 in the list is largest)

Let the model to it

Tensorflow models have a built in method that returns the index of the highest class probability.

model.predict_classes(testx, verbose=1)


Do it manually

argmax is a generic function to return the index of the highest value in a sequence.

import tensorflow as tf

# Create a session
sess = tf.InteractiveSession()

# Output Values
output = [[.57, .21, .21], [.19, .15, .64], [.23, .16, .60]]

# Index of top values
indexes = tf.argmax(output, axis=1)
print(indexes.eval()) # prints [0 2 2]",42574.78611,42593.47569,"Add a comment
                 |","The values being returned are probabilities, not log likelihoods.
                
                
– Dr. Snoopy
                
                
                    Commented
                    Aug 9, 2017 at 11:11
                
            
        
    
    
        
            
            
        
        
            
                
                what does verbose=1 do?
                
                
– DataMan
                
                
                    Commented
                    Feb 26, 2018 at 13:36
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Question: How can I then get the label name based on the index? Thanks
                
                
– The-IT
                
                
                    Commented
                    Jul 18, 2018 at 7:51
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/45645276,keras.layers.Conv2D,SAM,BET,IC-2,IC-2,Missing Input Value/Type Dependency,Crash,Keras,Model Construction,Static,0,<python><tensorflow><neural-network><keras><keras-layer>,16,42959.00139,42959.31042,32,2103,7847518,18285,1922589,0.819327731,0,https://stackoverflow.com/questions/46036522,"I got this error message when declaring the input layer in Keras.


  ValueError: Negative dimension size caused by subtracting 3 from 1 for
  'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28],
  [3,3,28,32].


My code is like this

model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))


Sample application: https://github.com/IntellijSys/tensorflow/blob/master/Keras.ipynb","By default, Convolution2D (https://keras.io/layers/convolutional/) expects the input to be in the format (samples, rows, cols, channels), which is ""channels-last"". Your data seems to be in the format (samples, channels, rows, cols). You should be able to fix this using the optional keyword data_format = 'channels_first' when declaring the Convolution2D layer.

model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))",7/25/2016 13:08,7/25/2016 13:19,"2
            
        
        
            
                
                I think you want to use a 3x3 kernel. In this case you should write (3, 3) instead of 3, 3.
                
                
– ml4294
                
                
                    Commented
                    Aug 12, 2017 at 7:22
                
            
        
    

            
	    

        
                    Add a comment
                 |","4
            
        
        
            
                
                Note that it can be set globally in ~/.keras/keras.json:     ""image_data_format"": ""channels_first""
                
                
– Shaohua Li
                
                
                    Commented
                    Feb 18, 2018 at 15:17
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/45755022,"keras.models.add,keras.models.load_model",AMO,G,G,G,Missing Required Method Order,Crash,Keras,Load,Static,0,<python><deep-learning><keras><keras-layer>,6,42965.45625,42965.48542,9,40874,1782792,105,6157456,1.391736697,0,https://stackoverflow.com/questions/46155868,"I have a saved a model using model.save(). I'm trying to reload the model and add a few layers and tune some hyper-parameters, however, it throws the AttributeError.

Model is loaded using load_model().

I guess I'm missing understanding how to add layers to saved layers. If someone can guide me here, it will be great. I'm a novice to deep learning and using keras, so probably my request would be silly.

Snippet:

prev_model = load_model('final_model.h5') # loading the previously saved model.

prev_model.add(Dense(256,activation='relu'))
prev_model.add(Dropout(0.5))
prev_model.add(Dense(1,activation='sigmoid'))

model = Model(inputs=prev_model.input, outputs=prev_model(prev_model.output))


And the error it throws:

Traceback (most recent call last):
  File ""image_classifier_3.py"", line 39, in <module>
    prev_model.add(Dense(256,activation='relu'))
AttributeError: 'Model' object has no attribute 'add'


I know adding layers works on new Sequential() model, but how do we add to existing saved models?","The add method is present only in sequential models (Sequential class), which is a simpler interface to the more powerful but complicated functional model (Model class). load_model will always return a Model instance, which is the most generic class.

You can look at the example to see how you can compose different models, but the idea is that, in the end, a Model behaves pretty much like any other layer. So you should be able to do:

prev_model = load_model('final_model.h5') # loading the previously saved model.

new_model = Sequential()
new_model.add(prev_model)
new_model.add(Dense(256,activation='relu'))
new_model.add(Dropout(0.5))
new_model.add(Dense(1,activation='sigmoid'))

new_model.compile(...)",7/26/2016 11:56,7/26/2016 16:35,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/45930844," keras.models.Model,keras.layers.merge.concatenate",Hybrid,SL,F,AMO(Level-2),Missing Options,Unknown,Keras,Model Construction,Static,0,<python><keras><keras-layer><ensemble-learning>,7,42976.19931,42976.32778,6,2700,518804,8288,996366,6,2,https://stackoverflow.com/questions/46242187,"I am trying to create my first ensemble models in keras. I have 3 input values and a single output value in my dataset.

from keras.optimizers import SGD,Adam
from keras.layers import Dense,Merge
from keras.models import Sequential

model1 = Sequential()
model1.add(Dense(3, input_dim=3, activation='relu'))
model1.add(Dense(2, activation='relu'))
model1.add(Dense(2, activation='tanh'))
model1.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])

model2 = Sequential()
model2.add(Dense(3, input_dim=3, activation='linear'))
model2.add(Dense(4, activation='tanh'))
model2.add(Dense(3, activation='tanh'))
model2.compile(loss='mse', optimizer='SGD', metrics=['accuracy'])

model3 = Sequential()
model3.add(Merge([model1, model2], mode = 'concat'))
model3.add(Dense(1, activation='sigmoid'))
model3.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])

model3.input_shape


The ensemble model(model3) compiles without any error but while fitting the model I have to pass the same input two times model3.fit([X,X],y). Which I think is an unnecessary step and instead of passing input twice I want to have a common input nodes for my ensemble model. How can I do it?","Keras functional API seems to be a better fit for your use case, as it allows more flexibility in the computation graph. e.g.:

from keras.layers import concatenate
from keras.models import Model
from keras.layers import Input, Merge
from keras.layers.core import Dense
from keras.layers.merge import concatenate

# a single input layer
inputs = Input(shape=(3,))

# model 1
x1 = Dense(3, activation='relu')(inputs)
x1 = Dense(2, activation='relu')(x1)
x1 = Dense(2, activation='tanh')(x1)

# model 2 
x2 = Dense(3, activation='linear')(inputs)
x2 = Dense(4, activation='tanh')(x2)
x2 = Dense(3, activation='tanh')(x2)

# merging models
x3 = concatenate([x1, x2])

# output layer
predictions = Dense(1, activation='sigmoid')(x3)

# generate a model from the layers above
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Always a good idea to verify it looks as you expect it to 
# model.summary()

data = [[1,2,3], [1,1,3], [7,8,9], [5,8,10]]
labels = [0,0,1,1]

# The resulting model can be fit with a single input:
model.fit(data, labels, epochs=50)


Notes:


There might be slight differences in the API between Keras versions (pre- and post- version 2)
The example above specifies different optimizer and loss function for each of the models. However, since fit() is being called only once (on model3), the same settings - those of model3 - will apply to the entire model. In order to have different settings when training the sub-models, they will have to be fit() separately - 
see comment by @Daniel.


EDIT: updated notes based on comments",7/29/2016 8:53,7/29/2016 9:31,"Add a comment
                 |","The compilation (optimizer and loss) of a model is only taken into account when you use fit for that specific model. If you are using fit in model3, only the compilation for model3 will take effect. --- There is no need at all to compile model1 and model2, unless you're going to train them separately (with model1.fit and model2.fit). Weights and predictions do not require compile.
                
                
– Daniel Möller
                
                
                    Commented
                    Sep 1, 2017 at 16:27
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/45979848,"keras.models.Model,keras.models.add",Hybrid,SL,F,AMO(Level-2),Missing Options,Crash,Keras,Model Construction,Static,0,<python><machine-learning><neural-network><keras><conv-neural-network>,15,42978.45764,42978.52639,19,56010,2097240,731,7309225,1.192391706,0,https://stackoverflow.com/questions/46385797,"I a trying to merge 2 sequential models in keras. Here is the code:

model1 = Sequential(layers=[
    # input layers and convolutional layers
    Conv1D(128, kernel_size=12, strides=4, padding='valid', activation='relu', input_shape=input_shape),
    MaxPooling1D(pool_size=6),
    Conv1D(256, kernel_size=12, strides=4, padding='valid', activation='relu'),
    MaxPooling1D(pool_size=6),
    Dropout(.5),

])

model2 = Sequential(layers=[
    # input layers and convolutional layers
    Conv1D(128, kernel_size=20, strides=5, padding='valid', activation='relu', input_shape=input_shape),
    MaxPooling1D(pool_size=5),
    Conv1D(256, kernel_size=20, strides=5, padding='valid', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(.5),

])

model = merge([model1, model2], mode = 'sum')
Flatten(),
Dense(256, activation='relu'),
Dropout(.5),
Dense(128, activation='relu'),
Dropout(.35),
# output layer
Dense(5, activation='softmax')
return model


Here is the error log:


  File
  ""/nics/d/home/dsawant/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"",
  line 392, in is_keras_tensor
      raise ValueError('Unexpectedly found an instance of type ' + str(type(x)) + '. ' ValueError: Unexpectedly found an instance of
  type <class 'keras.models.Sequential'>. Expected a symbolic tensor
  instance.


Some more log:


  ValueError: Layer merge_1 was called with an input that isn't a
  symbolic tensor. Received type: class 'keras.models.Sequential'.
  Full input: [keras.models.Sequential object at 0x2b32d518a780,
  keras.models.Sequential object at 0x2b32d521ee80]. All inputs to the
  layer should be tensors.


How can I merge these 2 Sequential models that use different window sizes and apply functions like 'max', 'sum' etc to them?","Using the functional API brings you all possibilities. 

When using the functional API, you need to keep track of inputs and outputs, instead of just defining layers. 

You define a layer, then you call the layer with an input tensor to get the output tensor. Models and layers can be called exactly the same way.

For the merge layer, I prefer using other merge layers that are more intuitive, such as Add(), Multiply() and Concatenate() for instance. 

from keras.layers import *

mergedOut = Add()([model1.output,model2.output])
    #Add() -> creates a merge layer that sums the inputs
    #The second parentheses ""calls"" the layer with the output tensors of the two models
    #it will demand that both model1 and model2 have the same output shape


This same idea apply to all the following layers. We keep updating the output tensor giving it to each layer and getting a new output (if we were interested in creating branches, we would use a different var for each output of interest to keep track of them):

mergedOut = Flatten()(mergedOut)    
mergedOut = Dense(256, activation='relu')(mergedOut)
mergedOut = Dropout(.5)(mergedOut)
mergedOut = Dense(128, activation='relu')(mergedOut)
mergedOut = Dropout(.35)(mergedOut)

# output layer
mergedOut = Dense(5, activation='softmax')(mergedOut)


Now that we created the ""path"", it's time to create the Model. Creating the model is just like telling at which input tensors it starts and where it ends:

from keras.models import Model

newModel = Model([model1.input,model2.input], mergedOut)
    #use lists if you want more than one input or output    


Notice that since this model has two inputs, you have to train it with two different X_training vars in a list:

newModel.fit([X_train_1, X_train_2], Y_train, ....)    




Now, suppose you wanted only one input, and both model1 and model2 would take the same input. 

The functional API allows that quite easily by creating an input tensor and feeding it to the models (we call the models as if they were layers):   

commonInput = Input(input_shape)

out1 = model1(commonInput)    
out2 = model2(commonInput)    

mergedOut = Add()([out1,out2])


In this case, the Model would consider this input:

oneInputModel = Model(commonInput,mergedOut)",08-01-2016 08:24,08-01-2016 15:39,"You need to merge the output layers of the two models, I don't think you can merge compiled models in keras. You should look at keras' functional API
                
                
– gionni
                
                
                    Commented
                    Aug 31, 2017 at 11:10
                
            
        
    
    
        
            
            
        
        
            
                
                check: stackoverflow.com/questions/44872982/…
                
                
– Wilmar van Ommeren
                
                
                    Commented
                    Aug 31, 2017 at 11:17
                
            
        
    
    
        
            
            
        
        
            
                
                Got it. I thought we could. It was worth a shot. Thank you for the link
                
                
– Digvijay Sawant
                
                
                    Commented
                    Aug 31, 2017 at 12:12
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Compilation doesn't affect anything about creating/changing models. But you will have to compile again for training when you change. (Compiling in keras is just: ""set the optimizer and the loss function for training"", nothing else). If you're not going to train, you don't even need to ""compile"" a model, it can do everything, including predictions, when it's not compiled, except training.
                
                
– Daniel Möller
                
                
                    Commented
                    Aug 31, 2017 at 12:48
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                So when we want to merge 2 models we cannot declare those 2 models as Sequential()? We have to use functional APIs. I keep getting the error that I stated above in my question when I call Concatenate(). Is that correct that in this case we cannot use Sequential()?
                
                
– Digvijay Sawant
                
                
                    Commented
                    Aug 31, 2017 at 12:48
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                You can keep the sequential models, no problem, but the final model cannot be sequential, it's not feasible. The error message is about: ""you are not passing tensors when calling the layer"". You are very probably passing the models instead. Please notice the model1.output and model2.output tensors in my answer. ---- model1 is a model, while model1.output is a tensor.
                
                
– Daniel Möller
                
                
                    Commented
                    Aug 31, 2017 at 12:50
                
                        
                            
                        
            
        
    
    
        
            
                    2
            
        
        
            
                
                The final model is a functional Model that contains two Sequential models and some additional layers in its path.
                
                
– Daniel Möller
                
                
                    Commented
                    Aug 31, 2017 at 13:00
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                I am finding it difficult to understand the last part. I am training both my models on the same training data. Here is my fit() function - model.fit([train_data, train_data], train_labels,               batch_size=256, epochs=5, validation_data=(test_data, test_labels),              verbose=1, callbacks=callbacks) I still get an error saying : ValueError: The model expects 2input arrays, but only received one array. Found: array with shape (1807, 6000, 1)
                
                
– Digvijay Sawant
                
                
                    Commented
                    Aug 31, 2017 at 13:05
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                It's probably because of the validation_data, it also needs two inputs as you did with train_data.
                
                
– Daniel Möller
                
                
                    Commented
                    Aug 31, 2017 at 15:45
                
            
        
    

            
	    

        
                    
                 | 
            Show 1 more comment"
https://stackoverflow.com/questions/46204569,"keras.layers.Dense,keras.models.compile",Hybrid,SAI,"IC-2,F",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Unknown,Keras,Model Construction,Static,0,<python><machine-learning><neural-network><deep-learning><keras>,17,42991.77153,42991.84028,25,28691,5974433,292,8036988,3.18125,0,https://stackoverflow.com/questions/46495215,"I am trying to perform the usual classification on the MNIST database but with randomly cropped digits. 
Images are cropped the following way : removed randomly first/last and/or row/column.

I would like to use a Convolutional Neural Network using Keras (and Tensorflow backend) to perform convolution and then the usual classification.

Inputs are of variable size and i can't manage to get it to work.

Here is how I cropped digits

import numpy as np
from keras.utils import to_categorical
from sklearn.datasets import load_digits

digits = load_digits()

X = digits.images
X = np.expand_dims(X, axis=3)

X_crop = list()
for index in range(len(X)):
    X_crop.append(X[index, np.random.randint(0,2):np.random.randint(7,9), np.random.randint(0,2):np.random.randint(7,9), :])
X_crop = np.array(X_crop)

y = to_categorical(digits.target)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_crop, y, train_size=0.8, test_size=0.2)


And here is the architecture of the model I want to use

from keras.layers import Dense, Dropout
from keras.layers.convolutional import Conv2D
from keras.models import Sequential

model = Sequential()

model.add(Conv2D(filters=10, 
                 kernel_size=(3,3), 
                 input_shape=(None, None, 1), 
                 data_format='channels_last'))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))


model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

model.summary()

model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))



Does someone have an idea on how to handle variable sized input in my neural network? 
And how to perform classification?","TL/DR - go to point 4

So - before we get to the point - let's fix some problems with your network:


Your network will not work because of activation: with categorical_crossentropy you need to have a softmax activation:

model.add(Dense(10, activation='softmax'))

Vectorize spatial tensors: as Daniel mentioned - you need to, at some stage, switch your vectors from spatial (images) to vectorized (vectors). Currently - applying Dense to output from a Conv2D is equivalent to (1, 1) convolution. So basically - output from your network is spatial - not vectorized what causes dimensionality mismatch (you can check that by running your network or checking the model.summary(). In order to change that you need to use either GlobalMaxPooling2D or GlobalAveragePooling2D. E.g.:

model.add(Conv2D(filters=10, 
             kernel_size=(3, 3), 
             input_shape=(None, None, 1),
             padding=""same"",
             data_format='channels_last'))
model.add(GlobalMaxPooling2D())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))

Concatenated numpy arrays need to have the same shape: if you check the shape of X_crop you'll see that it's not a spatial matrix. It's because you concatenated matrices with different shapes. Sadly - it's impossible to overcome this issue as numpy.array need to have a fixed shape.
How to make your network train on examples of different shape: The most important thing in doing this is to understand two things. First - is that in a single batch every image should have the same size. Second - is that calling fit multiple times is a bad idea - as you reset inner model states. So here is what needs to be done:

a. Write a function which crops a single batch - e.g. a get_cropped_batches_generator which given a matrix cuts a batch out of it and crops it randomly.

b. Use train_on_batch method. Here is an example code:

from six import next

batches_generator = get_cropped_batches_generator(X, batch_size=16)
losses = list()
for epoch_nb in range(nb_of_epochs):
    epoch_losses = list()
    for batch_nb in range(nb_of_batches):
        # cropped_x has a different shape for different batches (in general)
        cropped_x, cropped_y = next(batches_generator) 
        current_loss = model.train_on_batch(cropped_x, cropped_y)
        epoch_losses.append(current_loss)
    losses.append(epoch_losses.sum() / (1.0 * len(epoch_losses))
final_loss = losses.sum() / (1.0 * len(losses))



So - a few comments to code above: First, train_on_batch doesn't use nice keras progress bar. It returns a single loss value (for a given batch) - that's why I added logic to compute loss. You could use Progbar callback for that also. Second - you need to implement get_cropped_batches_generator - I haven't written a code to keep my answer a little bit more clear. You could ask another question on how to implement it. Last thing - I use six to keep compatibility between Python 2 and Python 3.",08-03-2016 00:28,08-03-2016 16:48,"Add a comment
                 |","1
            
        
        
            
                
                @Marcin Mo?ejko can you please give an explaination about the implementation of get_cropped_batches_generator ?
                
                
– anas.khayata
                
                
                    Commented
                    Dec 13, 2018 at 8:05
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Could you please elaborate on the difference between model.fit and train_on_batch? Which inner states of the model are being resetted?
                
                
– hirschme
                
                
                    Commented
                    Jan 15, 2019 at 16:10
                
            
        
    
    
        
            
            
        
        
            
                
                Is it possible to get the number of neurons of the layer before? So that, when the first layer has a dynamic input shape, the next layer has half of them?
                
                
– Stefan
                
                
                    Commented
                    Apr 15, 2019 at 19:50
                
            
        
    
    
        
            
            
        
        
            
                
                ""Second - is that calling fit multiple times is a bad idea - as you reset inner model states"", @Marcin is the statement true even when using checkpointer to avoid training in one go!
                
                
– Prasanjit Rath
                
                
                    Commented
                    Nov 29, 2020 at 9:22
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/46397258,"keras.layers.concatenate,keras.models.Model",Hybrid,SL,F,AMO(Level-2),Missing Options,Crash,Keras,Model Construction,Static,0,<python><keras>,8,43003.15208,43003.17569,14,248,3800591,527,8612523,14,0,https://stackoverflow.com/questions/46705600,"I am trying to merge two Sequential models In Keras 2.0, using the following line:

merged_model.add(Merge([model1, model2], mode='concat'))


This still works fine, but gives a warning: 

""The `Merge` layer is deprecated and will be removed after 08/2017. Use
instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc."" 


However, studying the Keras documentation and trying add, Add(), has not resulted in something that works. I have read several posts from people with the same problem, but found no solution that works in my case below. Any suggestions?

model = Sequential()
model1 = Sequential()
model1.add(Dense(300, input_dim=40, activation='relu', name='layer_1'))
model2 = Sequential()
model2.add(Dense(300, input_dim=40, activation='relu', name='layer_2'))
merged_model = Sequential()

merged_model.add(Merge([model1, model2], mode='concat'))

merged_model.add(Dense(1, activation='softmax', name='output_layer'))
merged_model.compile(loss='binary_crossentropy', optimizer='adam', 
metrics=['accuracy'])

checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc',
save_best_only=True, verbose=2)
early_stopping = EarlyStopping(monitor=""val_loss"", patience=5)

merged_model.fit([x1, x2], y=y, batch_size=384, epochs=200,
             verbose=1, validation_split=0.1, shuffle=True, 
callbacks=[early_stopping, checkpoint])


EDIT: When I tried (as suggested below by Kent Sommer):

from keras.layers.merge import concatenate
merged_model.add(concatenate([model1, model2]))


This was the error message:

Traceback (most recent call last):
  File ""/anaconda/lib/python3.6/site- packages/keras/engine/topology.py"", line 425, 
in assert_input_compatibility
    K.is_keras_tensor(x)
  File ""/anaconda/lib/python3.6/site-
packages/keras/backend/tensorflow_backend.py"", line 403, in     is_keras_tensor
    raise ValueError('Unexpectedly found an instance of type `' +
 str(type(x)) + '`. '
ValueError: Unexpectedly found an instance of type 
`<class'keras.models.Sequential'>`. Expected a symbolic tensor instance.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""quoradeeptest_simple1.py"", line 78, in <module>
    merged_model.add(concatenate([model1, model2]))
  File ""/anaconda/lib/python3.6/site-packages/keras/layers/merge.py"",
 line 600, in concatenate return Concatenate(axis=axis, **kwargs)(inputs)
  File ""/anaconda/lib/python3.6/site-   packages/keras/engine/topology.py"", 
line 558, in __call__self.assert_input_compatibility(inputs)
  File ""/anaconda/lib/python3.6/site-packages/keras/engine/topology.py"", line 431, 
 in assert_input_compatibility str(inputs) + '.All inputs to the layer '
ValueError: Layer concatenate_1 was called with an input that isn't a
symbolic tensor. Received type: <class 'keras.models.Sequential'>. 
Full input: [<keras.models.Sequential object at 0x140fa7ba8>,
<keras.models.Sequential object at 0x140fabdd8>]. All inputs to the
layer should be tensors.","What that warning is saying is that instead of using the Merge layer with a specific mode, the different modes have now been split into their own individual layers.

So Merge(mode='concat') is now concatenate(axis=-1).

However, since you want to merge models not layers, this will not work in your case. What you will need to do is use the functional model since this behavior is no longer supported with the basic Sequential model type. 

In your case that means the code should be changed to the following:

from keras.layers.merge import concatenate
from keras.models import Model, Sequential
from keras.layers import Dense, Input

model1_in = Input(shape=(27, 27, 1))
model1_out = Dense(300, input_dim=40, activation='relu', name='layer_1')(model1_in)
model1 = Model(model1_in, model1_out)

model2_in = Input(shape=(27, 27, 1))
model2_out = Dense(300, input_dim=40, activation='relu', name='layer_2')(model2_in)
model2 = Model(model2_in, model2_out)


concatenated = concatenate([model1_out, model2_out])
out = Dense(1, activation='softmax', name='output_layer')(concatenated)

merged_model = Model([model1_in, model2_in], out)
merged_model.compile(loss='binary_crossentropy', optimizer='adam', 
metrics=['accuracy'])

checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc',
save_best_only=True, verbose=2)
early_stopping = EarlyStopping(monitor=""val_loss"", patience=5)

merged_model.fit([x1, x2], y=y, batch_size=384, epochs=200,
             verbose=1, validation_split=0.1, shuffle=True, 
callbacks=[early_stopping, checkpoint])",08-07-2016 23:41,08-11-2016 07:09,"1
            
        
        
            
                
                Possible duplicate of How to merge keras sequential models with same input?
                
                
– Wilmar van Ommeren
                
                
                    Commented
                    Sep 25, 2017 at 9:13
                
            
        
    

            
	    

        
                    Add a comment
                 |","Thanks! I tried that, but this gave another error - which I have added above after ""EDIT"". Apparently, the two are not entirely equivalent.
                
                
– twhale
                
                
                    Commented
                    Sep 25, 2017 at 5:20
                
            
        
    
    
        
            
            
        
        
            
                
                I have updated my answer to fix your new error. My apologies for not properly reading your question at first!
                
                
– Kent Sommer
                
                
                    Commented
                    Sep 25, 2017 at 5:54
                
            
        
    
    
        
            
            
        
        
            
                
                No problem. Thanks a lot. Will work with this and let you know.
                
                
– twhale
                
                
                    Commented
                    Sep 25, 2017 at 8:13
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                This results in: ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (100, 40)
                
                
– twhale
                
                
                    Commented
                    Sep 25, 2017 at 17:20
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                I changed the input shape to model1_in = Input(shape=(40,)) and now it works thanks!
                
                
– twhale
                
                
                    Commented
                    Sep 26, 2017 at 1:48
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 1 more comment"
https://stackoverflow.com/questions/46464549,keras.models.compile,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Unknown,Keras,Model Construction,Static,0,<python><tensorflow><keras><loss>,14,43006.35694,43006.39931,22,11289,1531463,2819,216295,4.628205128,0,https://stackoverflow.com/questions/46771516,"In Keras (with Tensorflow backend), is the current input pattern available to my custom loss function?

The current input pattern is defined as the input vector used to produce the prediction. For example, consider the following: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=False). Then the current input pattern is the current X_train vector associated with the y_train (which is termed y_true in the loss function).

When designing a custom loss function, I intend to optimize/minimize a value that requires access to the current input pattern, not just the current prediction.

I've taken a look through https://github.com/fchollet/keras/blob/master/keras/losses.py

I've also looked through ""Cost function that isn't just y_pred, y_true?""

I am also familiar with previous examples to produce a customized loss function:

import keras.backend as K

def customLoss(y_true,y_pred):
    return K.sum(K.log(y_true) - K.log(y_pred))


Presumably (y_true,y_pred) are defined elsewhere. I've taken a look through the source code without success and I'm wondering whether I need to define the current input pattern myself or whether this is already accessible to my loss function.","You can wrap the loss function as a inner function and pass your input tensor to it (as commonly done when passing additional arguments to the loss function).

def custom_loss_wrapper(input_tensor):
    def custom_loss(y_true, y_pred):
        return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)
    return custom_loss

input_tensor = Input(shape=(10,))
hidden = Dense(100, activation='relu')(input_tensor)
out = Dense(1, activation='sigmoid')(hidden)
model = Model(input_tensor, out)
model.compile(loss=custom_loss_wrapper(input_tensor), optimizer='adam')


You can verify that input_tensor and the loss value (mostly, the K.mean(input_tensor) part) will change as different X is passed to the model.

X = np.random.rand(1000, 10)
y = np.random.randint(2, size=1000)
model.test_on_batch(X, y)  # => 1.1974642

X *= 1000
model.test_on_batch(X, y)  # => 511.15466",08-08-2016 06:19,08-08-2016 21:35,"Add a comment
                 |","13
            
        
        
            
                
                This doesn't work in tf.keras with eager execution. Any idea how to do it?
                
                
– Luke
                
                
                    Commented
                    Sep 1, 2019 at 20:59
                
            
        
    
    
        
            
                    6
            
        
        
            
                
                It does not work for tf.keras eager execusion :( it says: tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_2:0' shape=(None, 10) dtype=float32>]
                
                
– ch271828n
                
                
                    Commented
                    Mar 29, 2020 at 7:12
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/46619869,keras.models.compile,SAM,DT,RT,RT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><machine-learning><tensorflow><keras>,9,43015.49931,43015.56458,19,2478,4727666,8247,2179021,2.026315789,0,https://stackoverflow.com/questions/47025036,"I am using keras+tensorflow for the first time. I would like to specify the correlation coefficient as the loss function. It makes sense to square it so that it is a number between 0 and 1 where 0 is bad and 1 is good.

My basic code currently looks like:

def baseline_model():
        model = Sequential()
        model.add(Dense(4000, input_dim=n**2, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal'))
        # Compile model
        model.compile(loss='mean_squared_error', optimizer='adam')
        return model

estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, verbose=2)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits=10, random_state=0)
results = cross_val_score(pipeline, X, Y, cv=kfold)
print(""Standardized: %.2f (%.2f) MSE"" % (results.mean(), results.std()))


How can I change this so that it optimizes to minimize the squared correlation coefficient  instead?



I tried the following:

def correlation_coefficient(y_true, y_pred):
    pearson_r, _ = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)
    return 1-pearson_r**2

def baseline_model():
# create model
        model = Sequential()
        model.add(Dense(4000, input_dim=n**2, kernel_initializer='normal', activation='relu'))
#        model.add(Dense(2000, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal'))
        # Compile model
        model.compile(loss=correlation_coefficient, optimizer='adam')
        return model


but this crashes with:

Traceback (most recent call last):
  File ""deeplearning-det.py"", line 67, in <module>
    results = cross_val_score(pipeline, X, Y, cv=kfold)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 321, in cross_val_score
    pre_dispatch=pre_dispatch)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 195, in cross_validate
    for train, test in cv.split(X, y, groups))
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 332, in __init__
    self.results = batch()
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 437, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/pipeline.py"", line 259, in fit
    self._final_estimator.fit(Xt, y, **fit_params)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py"", line 147, in fit
    history = self.model.fit(x, y, **fit_args)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/models.py"", line 867, in fit
    initial_epoch=initial_epoch)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 1575, in fit
    self._make_train_function()
  File ""/home/user/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 960, in _make_train_function
    loss=self.total_loss)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/optimizers.py"", line 432, in get_updates
    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 856, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 364, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.




Update 1

Following the answer below the code now runs. Unfortunately, the correlation_coefficient and correlation_coefficient_loss functions give different values from each other and I am not sure either of them is the same as you would get from 1- scipy.stats.pearsonr()[0]**2.  


  Why are loss functions giving the wrong outputs and how can they be
  corrected to give the same values as 1 -
  scipy.stats.pearsonr()[0]**2 would give?


Here is the completely self contained code that should just run:

import numpy as np
import sys
import math
from scipy.stats import ortho_group
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import tensorflow as tf
from keras import backend as K


def permanent(M):
    n = M.shape[0]
    d = np.ones(n)
    j = 0
    s = 1
    f = np.arange(n)
    v = M.sum(axis=0)
    p = np.prod(v)
    while (j < n-1):
        v -= 2*d[j]*M[j]
        d[j] = -d[j]
        s = -s
        prod = np.prod(v)
        p += s*prod
        f[0] = 0
        f[j] = f[j+1]
        f[j+1] = j+1
        j = f[0]
    return p/2**(n-1)


def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(xm * ym)
    r_den = K.sum(K.sum(K.square(xm)) * K.sum(K.square(ym)))
    r = r_num / r_den
    return 1 - r**2


def correlation_coefficient(y_true, y_pred):
    pearson_r, update_op = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)
    # find all variables created for this metric
    metric_vars = [i for i in tf.local_variables() if 'correlation_coefficient' in i.name.split('/')[1]]

    # Add metric variables to GLOBAL_VARIABLES collection.
    # They will be initialized for new session.
    for v in metric_vars:
        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)

    # force to update metric values
    with tf.control_dependencies([update_op]):
        pearson_r = tf.identity(pearson_r)
        return 1-pearson_r**2


def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(4000, input_dim=no_rows**2, kernel_initializer='normal', activation='relu'))
#    model.add(Dense(2000, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    # Compile model
    model.compile(loss=correlation_coefficient_loss, optimizer='adam', metrics=[correlation_coefficient])
    return model


no_rows = 8

print(""Making the input data using seed 7"", file=sys.stderr)
np.random.seed(7)
U = ortho_group.rvs(no_rows**2)
U = U[:, :no_rows]
# U is a random orthogonal matrix
X = []
Y = []
print(U)
for i in range(40000):
        I = np.random.choice(no_rows**2, size = no_rows)
        A = U[I][np.lexsort(np.rot90(U[I]))]
        X.append(A.ravel())
        Y.append(-math.log(permanent(A)**2, 2))

X = np.array(X)
Y = np.array(Y)

estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, verbose=2)))
pipeline = Pipeline(estimators)
X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    train_size=0.75, test_size=0.25)
pipeline.fit(X_train, y_train)


Update 2

I have given up on the correlation_coefficient function and am now just using the correlation_coefficient_loss one as given by JulioDanielReyes below.  However, either this is still wrong or keras is dramatically overfitting.  Even when I have:

def baseline_model():
        model = Sequential()
        model.add(Dense(40, input_dim=no_rows**2, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal'))
        model.compile(loss=correlation_coefficient_loss, optimizer='adam', metrics=[correlation_coefficient_loss])
        return model


I get a loss of, for example, 0.6653 after 100 epochs but 0.857 when I test the trained model.


  How can it be overfitting which such a tiny number of nodes in the
  hidden layer?","According to keras documentation, you should pass the squared correlation coefficient as a function instead of the string 'mean_squared_error'.

The function needs to receive 2 tensors (y_true, y_pred). You can look at keras source code for inspiration.

There is also a function tf.contrib.metrics.streaming_pearson_correlation implemented on tensorflow. Just be careful on the order of the parameters, it should be something like this:

Update 1: initialize local variables according to this issue

import tensorflow as tf
def correlation_coefficient(y_true, y_pred):
    pearson_r, update_op = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true, name='pearson_r'
    # find all variables created for this metric
    metric_vars = [i for i in tf.local_variables() if 'pearson_r'  in i.name.split('/')]

    # Add metric variables to GLOBAL_VARIABLES collection.
    # They will be initialized for new session.
    for v in metric_vars:
        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)

    # force to update metric values
    with tf.control_dependencies([update_op]):
        pearson_r = tf.identity(pearson_r)
        return 1-pearson_r**2

...

model.compile(loss=correlation_coefficient, optimizer='adam')


Update 2: even though you cannot use the scipy function directly, you can look at the implementation and port it to your code using keras backend. 

Update 3: The tensorflow function as it is may not be differentiable, your loss function needs to be something like this: (Please check the math)

from keras import backend as K
def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)
    return 1 - K.square(r)


Update 4: The results are different on both functions, but correlation_coefficient_loss gives the same results as scipy.stats.pearsonr:
Here is the code to test it:

import tensorflow as tf
from keras import backend as K
import numpy as np
import scipy.stats

inputa = np.array([[3,1,2,3,4,5],
                    [1,2,3,4,5,6],
                    [1,2,3,4,5,6]])
inputb = np.array([[3,1,2,3,4,5],
                    [3,1,2,3,4,5],
                    [6,5,4,3,2,1]])

with tf.Session() as sess:
    a = tf.placeholder(tf.float32, shape=[None])
    b = tf.placeholder(tf.float32, shape=[None])
    f1 = correlation_coefficient(a, b)
    f2 = correlation_coefficient_loss(a, b)

    sess.run(tf.global_variables_initializer())

    for i in range(inputa.shape[0]):

        f1_result, f2_result = sess.run([f1, f2], feed_dict={a: inputa[i], b: inputb[i]})
        scipy_result =1- scipy.stats.pearsonr(inputa[i], inputb[i])[0]**2
        print(""a: ""+ str(inputa[i]) + "" b: "" + str(inputb[i]))
        print(""correlation_coefficient: "" + str(f1_result))
        print(""correlation_coefficient_loss: "" + str(f2_result))
        print(""scipy.stats.pearsonr:"" + str(scipy_result))


Results:

a: [3 1 2 3 4 5] b: [3 1 2 3 4 5]
correlation_coefficient: -2.38419e-07
correlation_coefficient_loss: 0.0
scipy.stats.pearsonr:0.0
a: [1 2 3 4 5 6] b: [3 1 2 3 4 5]
correlation_coefficient: 0.292036
correlation_coefficient_loss: 0.428571
scipy.stats.pearsonr:0.428571428571
a: [1 2 3 4 5 6] b: [6 5 4 3 2 1]
correlation_coefficient: 0.994918
correlation_coefficient_loss: 0.0
scipy.stats.pearsonr:0.0",08-11-2016 17:14,08-11-2016 17:44,"Have you tried 1 - K.square(pearson_r)?
                
                
– Daniel Möller
                
                
                    Commented
                    Oct 10, 2017 at 16:25
                
            
        
    
    
        
            
            
        
        
            
                
                @DanielMöller No I haven't. Would you mind spelling out a little more what you have in mind?
                
                
– Simd
                
                
                    Commented
                    Oct 10, 2017 at 17:40
                
            
        
    
    
        
            
            
        
        
            
                
                Exactly that instead of 1 - pearson_r**2.
                
                
– Daniel Möller
                
                
                    Commented
                    Oct 10, 2017 at 18:24
                
            
        
    
    
        
            
            
        
        
            
                
                But I'm not sure this error is about the loss function... it seems something else.
                
                
– Daniel Möller
                
                
                    Commented
                    Oct 10, 2017 at 18:25
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                If someone can explain why correlation_coefficient is not working, that would be awesome
                
                
– Julio Daniel Reyes
                
                
                    Commented
                    Oct 11, 2017 at 12:25
                
            
        
    

            
	    

        
                    
                 | 
            Show 8 more comments","Another option is to implement the function using primitives and keras.backend
                
                
– Julio Daniel Reyes
                
                
                    Commented
                    Oct 7, 2017 at 13:35
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Thats progress...  there is an extra namespace in the name of the local variables (metrics/correlation_coefficient/pearson/covariance/count/read), in i.name.split('/')[1] I didn't account for that, updated the first function.
                
                
– Julio Daniel Reyes
                
                
                    Commented
                    Oct 8, 2017 at 11:31
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                That's because the function needs tensors as parameters, not numpy arrays. bpaste.net/show/34459748fd6c, but you are right, the results are different, not sure why, sorry
                
                
– Julio Daniel Reyes
                
                
                    Commented
                    Oct 9, 2017 at 12:21
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Here is an explanation of the metrics functions, tensorflow.org/api_guides/python/contrib.metrics.
                
                
– Julio Daniel Reyes
                
                
                    Commented
                    Oct 9, 2017 at 12:28
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Regarding update 3, I added K.epsilon() to r = r_num / (r_den + K.epsilon()) to avoid nan values.
                
                
– ayalaall
                
                
                    Commented
                    Oct 17, 2023 at 6:43
                
            
        
    

            
	    

        
                    
                 | 
            Show 17 more comments"
https://stackoverflow.com/questions/47167630,keras.backend.set_session,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Unknown,Keras,Model Initialization,Static,0,<python><tensorflow><keras>,14,43046.88958,43168.48125,15,470,7175454,1448,3971621,1.5,0.357142857,https://stackoverflow.com/questions/47485216,"Lets say I have a simple neural network with an input layer and a single convolution layer programmed in tensorflow:

  # Input Layer
  input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])

  # Convolutional Layer #1
  conv1 = tf.layers.conv2d(
      inputs=input_layer,
      filters=32,
      kernel_size=[5, 5],
      padding=""same"",
      activation=tf.nn.relu)


I leave out any further parts of the network definitions for the features.

If I wanted to add an LSTM Layer after this convolution layer, I would have to make the convolution layer TimeDistributed (in the language of keras) and then put the output of the TimeDistributed layer into the LSTM. 

Tensorflow offers access to the keras layers in tf.keras.layers. Can I use the keras layers directly in the tensorflow code? If so, how? Could I also use the tf.keras.layers.lstm for the implementation of the LSTM Layer?

So in general: Is a mixture of pure tensorflow code and keras code possible and can I use the tf.keras.layers?","Yes, this is possible.

Import both TensorFlow and Keras and link your Keras session to the TF one:

import tensorflow as tf
import keras
from keras import backend as K

tf_sess = tf.Session()
K.set_session(tf_sess)


Now, in your model definition, you can mix TF and Keras layers like so:

# Input Layer
input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])

# Convolutional Layer #1
conv1 = tf.layers.conv2d(
    inputs=input_layer,
    filters=32,
    kernel_size=[5, 5],
    padding=""same"",
    activation=tf.nn.relu)

# Flatten conv output
flat = tf.contrib.layers.flatten(conv1)

# Fully-connected Keras layer
layer2_dense = keras.layers.Dense(128, activation='relu')(flat)

# Fully-connected TF layer (output)
output_preds = tf.layers.dense(layer2_dense, units=10)


This answer is adopted from a Keras blog post by Francois Chollet.",8/24/2016 0:22,10-07-2016 12:29,"I think this question was answered here: stackoverflow.com/questions/42441431/…
                
                
– ldavid
                
                
                    Commented
                    Nov 8, 2017 at 17:01
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Possible duplicate of How to set the input of a Keras layer with a Tensorflow tensor?
                
                
– ldavid
                
                
                    Commented
                    Nov 8, 2017 at 17:02
                
            
        
    

            
	    

        
                    Add a comment
                 |","So then preds would be a keras layer, right? Is there a way to put this back into a tensorflow layer/operation?
                
                
– Merlin1896
                
                
                    Commented
                    Mar 9, 2018 at 13:40
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @Merlin1896 You can mix and match any layers you want. I have updated the answer so that the final layer is a standard tensorflow layer
                
                
– collector
                
                
                    Commented
                    Mar 9, 2018 at 14:37
                
            
        
    
    
        
            
            
        
        
            
                
                @Merlin1896 if this answered your question you can mark it as answered so that others may find it as well
                
                
– collector
                
                
                    Commented
                    Mar 13, 2018 at 11:22
                
            
        
    
    
        
            
            
        
        
            
                
                Could you provide a complete minimal example for future reference? The current code won't run :)
                
                
– Merlin1896
                
                
                    Commented
                    Mar 13, 2018 at 19:11
                
            
        
    
    
        
            
            
        
        
            
                
                @Merlin1896 I changed the example to fit in with your convolutional example. It should run now! In the example I used a Keras dense layer since I'm not sure how exactly you are going to implement the LSTM
                
                
– collector
                
                
                    Commented
                    Mar 14, 2018 at 11:22
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/47183159,"keras.models,keras.models.layers,keras.models.load_model",Hybrid,SL,F,AMO(Level-2),Missing Options,IF,Keras,Load,Static,0,<python><tensorflow><deep-learning><keras>,17,43047.63194,43047.65556,29,56010,2097240,193,5129113,1.192391706,0,https://stackoverflow.com/questions/47523841,"I am having trouble with the Keras backend functions for setting values.  I am trying to convert a model from PyTorch to Keras and am trying to set the weights of the Keras model, but the weights do not appear to be getting set.  Note: I am not actually setting with np.ones just using that for an example.

I have tried...

Loading an existing model

import keras
from keras.models import load_model, Model
model = load_model(model_dir+file_name)
keras_layer = [layer for layer in model.layers if layer.name=='conv2d_1'][0]


Creating a simple model

img_input = keras.layers.Input(shape=(3,3,3))
x = keras.layers.Conv2D(1, kernel_size=1, strides=1, padding=""valid"", 
use_bias=False, name='conv1')(img_input)
model = Model(img_input, x)
keras_layer = [layer for layer in model.layers if layer.name=='conv1'][0]


Then using set_weights or set_value

keras_layer.set_weights([np.ones((1, 1, 3, 1))])


or...

K.batch_set_value([(weight,np.ones((1, 1, 3, 1))) for weight in keras_layer.weights])


afterwards I call either one of the following:

K.batch_get_value([weight for weight in keras_layer.weights])
keras_layer.get_weights()


And None of the weights appear to have been set.  The same values as before are returned.

[array([[[[  1.61547325e-06],
      [  2.97779252e-06],
      [  1.50160542e-06]]]], dtype=float32)]


How do I set the weights of a layer in Keras with a numpy array of values?","What is keras_layer in your code?

You can set weights these ways:

model.layers[i].set_weights(listOfNumpyArrays)    
model.get_layer(layerName).set_weights(...)
model.set_weights(listOfNumpyArrays)


Where model is an instance of an existing model. 
You can see the expected length of the list and its array shapes using the method get_weights() from the same instances above.",8/24/2016 5:07,8/24/2016 5:23,"1
            
        
        
            
                
                Could you provide your model definition? Also the format of your layers.
                
                
– Marcin Mo?ejko
                
                
                    Commented
                    Nov 8, 2017 at 19:39
                
            
        
    
    
        
            
            
        
        
            
                
                Must have made some other mistake before.  The accepted answer is working for me.
                
                
– DeltaLee
                
                
                    Commented
                    Nov 9, 2017 at 3:12
                
            
        
    

            
	    

        
                    Add a comment
                 |","5
            
        
        
            
                
                If you call get_weights again after you set the weights, it returns the original weights and not the values in the list of numpy arrays passed in or are you seeing different behavior?
                
                
– DeltaLee
                
                
                    Commented
                    Nov 8, 2017 at 22:20
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                When I set weights like this, the method get_weights() returns the new weights.
                
                
– Daniel Möller
                
                
                    Commented
                    Nov 9, 2017 at 11:20
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                I am trying to change the datatype of the weights but when I check the datatype of new weight it is just the same as original and not updated.
                
                
– Aditya
                
                
                    Commented
                    Apr 13, 2018 at 5:25
                
            
        
    
    
        
            
            
        
        
            
                
                @adityathakkar, Why do you need that? I don't think it will work.
                
                
– Daniel Möller
                
                
                    Commented
                    Apr 13, 2018 at 12:06
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                I want to reduce model size so
                
                
– Aditya
                
                
                    Commented
                    Apr 13, 2018 at 12:07
                
            
        
    

            
	    

        
                    
                 | 
            Show 6 more comments"
https://stackoverflow.com/questions/47266383,"keras.models.load_model,keras.models.save_weights",AMO,G,G,G,Missing Required Method Order,Crash,Keras,Load,Static,0,<python><keras>,41,43052.59375,43052.76875,70,1835,8328970,2447,8176285,3.71,0,https://stackoverflow.com/questions/47555829,"Im trying to save and load weights from the model i have trained.

the code im using to save the model is.

TensorBoard(log_dir='/output')
model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=1, epochs=1)
model.save_weights('model.hdf5')
model.save_weights('myModel.h5')


Let me know if this an incorrect way to do it,or if there is a better way to do it.

but when i try to load them,using this,

from keras.models import load_model
model = load_model('myModel.h5')


but i get this error:



ValueError                                Traceback (most recent call 
last)
<ipython-input-7-27d58dc8bb48> in <module>()
      1 from keras.models import load_model
----> 2 model = load_model('myModel.h5')

/home/decentmakeover2/anaconda3/lib/python3.5/site-
packages/keras/models.py in load_model(filepath, custom_objects, compile)
    235         model_config = f.attrs.get('model_config')
    236         if model_config is None:
--> 237             raise ValueError('No model found in config file.')
    238         model_config = json.loads(model_config.decode('utf-8'))
    239         model = model_from_config(model_config, 
custom_objects=custom_objects)

ValueError: No model found in config file.


Any suggestions on what i may be doing wrong?
Thank you in advance.","Here is a YouTube video that explains exactly what you're wanting to do: Save and load a Keras model

There are three different saving methods that Keras makes available. These are described in the video link above (with examples), as well as below. 

First, the reason you're receiving the error is because you're calling load_model incorrectly.

To save and load the weights of the model, you would first use 

model.save_weights('my_model_weights.h5')


to save the weights, as you've displayed. To load the weights, you would first need to build your model, and then call load_weights on the model, as in

model.load_weights('my_model_weights.h5')


Another saving technique is model.save(filepath). This save function saves:


The architecture of the model, allowing to re-create the model.
The weights of the model.
The training configuration (loss, optimizer).
The state of the optimizer, allowing to resume training exactly where you left off.


To load this saved model, you would use the following:

from keras.models import load_model
new_model = load_model(filepath)'


Lastly, model.to_json(), saves only the architecture of the model. To load the architecture, you would use 

from keras.models import model_from_json
model = model_from_json(json_string)",8/29/2016 13:32,8/29/2016 15:00,"Add a comment
                 |","3
            
        
        
            
                
                If I save the weights on python 3.6 is it possible to load them on python 2.7?
                
                
– Rtucan
                
                
                    Commented
                    Dec 5, 2018 at 19:44
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                @Rtucan I think Yes. You can try it.
                
                
– Michael Heidelberg
                
                
                    Commented
                    Feb 20, 2019 at 15:46
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                Is it possible to load weights from the saved model from model.save() , not model.save_weights? If so how to do it?
                
                
– TeenyTinySparkles
                
                
                    Commented
                    Jul 31, 2019 at 18:22
                
            
        
    
    
        
            
            
        
        
            
                
                I have the same curiosity (and really a necessity from my side) as @TeenyTinySparkles . I'm having issues loading the full model and I need to at least try to recover the weights (since I can easily recreate the model).
                
                
– Leonardo TM
                
                
                    Commented
                    Oct 23, 2022 at 21:07
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                For those who may need it, I found it! It can be done just call a model.load_weights(""model_saved_path"")  If you need or want to get to the source of this answer you can check the documentation for the load_weights on Keras Docs - Load_Weights
                
                
– Leonardo TM
                
                
                    Commented
                    Oct 23, 2022 at 21:18
                
            
        
    

            
	    

        
                    
                 | 
            Show 1 more comment"
https://stackoverflow.com/questions/47523841,"keras.backend.set_value,keras.models.load_model",AMO,F,F,F,Missing Required State-specific Method Order,Unknown,Keras,Load,Static,0,<python><keras>,7,43067.17222,43067.24722,16,1348,2641038,1348,2641038,8.5,8.5,https://stackoverflow.com/questions/47671732,"I am trying to use keras to store a model and then load it to retrain. My question is how do I set the learning rate to a new value when loading a model?

Here are my code:  

# Save a model
model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('tanh'))
model.add(Activation('softmax'))
# learning rate is 0.001
sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)
model.fit_generator(...)
model.save()


Then load the model,  

model = load_model(model)
# Change the model's parameters here. Set the learning rate to 0.01.
model.fit_generator(...)


Thank you.","I think I find the answer:

from keras import backend as K
# To get learning rate
print(K.get_value(model.optimizer.lr))
# To set learning rate
K.set_value(model.optimizer.lr, 0.001)
keras.__version__ # 2.0.2",9/15/2016 13:27,9/15/2016 14:25,"Add a comment
                 |","Add a comment
                 |"
https://stackoverflow.com/questions/47665391,keras.layers.Conv2D,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><tensorflow><deep-learning><keras><conv-neural-network>,17,43075.06111,43075.08681,27,56010,2097240,397,7732719,1.192391706,0,https://stackoverflow.com/questions/48034625,"I have checked all the solutions, but still, I am facing the same error. My training images shape is (26721, 32, 32, 1), which I believe it is 4 dimension, but I don't know why error shows it is 5 dimension. 

 model = Sequential()

 model.add(Convolution2D(16, 5, 5, border_mode='same', input_shape= input_shape ))


So this is how I am defining model.fit_generator 

model.fit_generator(train_dataset, train_labels, nb_epoch=epochs, verbose=1,validation_data=(valid_dataset, valid_labels), nb_val_samples=valid_dataset.shape[0],callbacks=model_callbacks)","The problem is input_shape. 

It should actually contain 3 dimensions only. And internally keras will add the batch dimension making it 4. 

Since you probably used input_shape with 4 dimensions (batch included), keras is adding the 5th. 

You should use input_shape=(32,32,1).",9/28/2016 21:38,6/30/2017 8:30,"Add a comment
                 |","2
            
        
        
            
                
                No, that number is free. Keras will show that dimension as None in the model.summry(), for instance.
                
                
– Daniel Möller
                
                
                    Commented
                    Dec 6, 2017 at 11:18
                
            
        
    
    
        
            
                    3
            
        
        
            
                
                My dimension for training data is array: (26721, 32, 32) and for valid. dimension is (6680,32,32). Now I explicitly define image size (32,32,1) , then it gives me error ValueError: Error when checking input: expected conv2d_9_input to have 4 dimensions, but got array with shape (6680, 32, 32)  . I have also edit model_fit.generator in post, could you please check there?
                
                
– Lucky 
                
                
                    Commented
                    Dec 6, 2017 at 11:25
                
            
        
    
    
        
            
                    6
            
        
        
            
                
                Now the problem is in your data. Your data lacks the channel dimension: x_validation = x_validation.reshape(6680,32,32,1)
                
                
– Daniel Möller
                
                
                    Commented
                    Dec 6, 2017 at 11:32
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                Thanks very much for your help
                
                
– Lucky 
                
                
                    Commented
                    Dec 6, 2017 at 11:55
                
            
        
    
    
        
            
            
        
        
            
                
                Can you help us here @DanielMöller. stackoverflow.com/questions/64612084/…
                
                
– Brown
                
                
                    Commented
                    Oct 30, 2020 at 16:29
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 1 more comment"
https://stackoverflow.com/questions/47671732,keras.layers.LSTM,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><keras><lstm><recurrent-neural-network>,9,43075.425,43075.47917,8,56010,2097240,328,3832088,1.192391706,2,https://stackoverflow.com/questions/48140989,"I have read a sequence of images into a numpy array with shape (7338, 225, 1024, 3) where 7338 is the sample size, 225 are the time steps and 1024 (32x32) are flattened image pixels, in 3 channels (RGB).

I have a sequential model with an LSTM layer:

model = Sequential()
model.add(LSTM(128, input_shape=(225, 1024, 3))


But this results in the error:

Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4


The documentation mentions that the input tensor for LSTM layer should be a 3D tensor with shape (batch_size, timesteps, input_dim), but in my case my input_dim is 2D.

What is the suggested way to input a 3 channel image into an LSTM layer  in Keras?","If you want the number of images to be a sequence (like a movie with frames), you need to put pixels AND channels as features:

input_shape = (225,3072)  #a 3D input where the batch size 7338 wasn't informed


If you want more processing before throwing 3072 features into an LSTM, you can combine or interleave 2D convolutions and LSTMs for a more refined model (not necessarily better, though, each application has its particular behavior). 

You can also try to use the new ConvLSTM2D, which will take the five dimensional input:

input_shape=(225,32,32,3) #a 5D input where the batch size 7338 wasn't informed




I'd probably create a convolutional net with several TimeDistributed(Conv2D(...)) and TimeDistributed(MaxPooling2D(...)) before adding a TimeDistributed(Flatten()) and finally the LSTM(). This will very probably improve both your image understanding and the performance of the LSTM.",42644.7875,42664.92361,"have you tried giving input_shape=X_train.shape[1:] . Assuming that X_train is your input array
                
                
– Hari Krishnan
                
                
                    Commented
                    Dec 6, 2017 at 10:44
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                Yes, I have.  X_train.shape[1:] gives me (225, 1024, 3) which is what was hard-coded as the input_shape param
                
                
– shubhamsingh
                
                
                    Commented
                    Dec 6, 2017 at 10:53
                
            
        
    

            
	    

        
                    Add a comment
                 |","I thought of reshaping my data from (1024, 3) to 3072, but I already had the data in batch size of 7338, and reshaping was taking a lot of time. And the LSTM is part of an auto encoder, so wasn't sure if this reshaping would help my cause. Will try reshaping first, then with ConvLSTM2D and TimeDistributed layers. Thanks for your answer.
                
                
– shubhamsingh
                
                
                    Commented
                    Dec 6, 2017 at 12:44
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                Reshaping taking time??? That doesn't sound ok.... the LSTM would be very very slow, though....
                
                
– Daniel Möller
                
                
                    Commented
                    Dec 6, 2017 at 12:54
                
            
        
    
    
        
            
            
        
        
            
                
                Yes, I think that's cause I'll be reshaping 1651050 (7738*225) instances. So, instead of doing it all together, I resorted to Keras model method of fit_generator(), where I create a generator method to reshape the data set, while training.
                
                
– shubhamsingh
                
                
                    Commented
                    Dec 8, 2017 at 15:23
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/47843265,"keras.callbacks.CSVLogger,keras.models.fit",Hybrid,SAI,"F,IC-1",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,Unknown,Keras,Load,Static,0,<python><keras>,7,43085.28056,43333.38056,8,102,8162025,799,3116573,8,0.25,https://stackoverflow.com/questions/48265926,"I have saved a keras model as a h5py file and now want to load it from disk.
When training the model I use:
from keras.models import Sequential

model = Sequential()
H = model.fit(....)

When the model is trained, I want to load it from disk with
model = load_model()

How can I get H from the model variable? It unfortunately does not have a history parameter that I can just call. Is it because the save_model function doesn't save history?","Unfortunately it seems that Keras hasn't implemented the possibility of loading the history directly from a loaded model. Instead you have to set it up in advance. This is how I solved it using CSVLogger (it's actually very convenient storing the entire training history in a separate file. This way you can always come back later and plot whatever history you want instead of being dependent on a variable you can easily lose stored in the RAM):
First we have to set up the logger before initiating the training.
from keras.callbacks import CSVLogger

csv_logger = CSVLogger('training.log', separator=',', append=False)
model.fit(X_train, Y_train, callbacks=[csv_logger])

The entire log history will now be stored in the file 'training.log' (the same information you would get, by in your case, calling H.history). When the training is finished, the next step would simply be to load the data stored in this file. You can do that with pandas read_csv:
import pandas as pd
log_data = pd.read_csv('training.log', sep=',', engine='python')

From here on you can treat the data stored in log_data just as you would by loading it from K.history.
More information in Keras callbacks docs.",10-02-2016 09:11,10-04-2016 11:51,"1
            
        
        
            
                
                Saving a model just keeps model weights and architecture. history object is a dictionary. You can save dictionaries as json files or you can use CSVLogger callback to log your training history into a text file.
                
                
– mkocabas
                
                
                    Commented
                    Dec 16, 2017 at 9:19
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |","2
            
        
        
            
                
                In addition to the possibility to read it back using pandas, is there also a way to load back a history file (saved with CSVLogger() as you mentioned) as an original History() object? Just 'as if' it were freshly computed from model.fit()?
                
                
– swiss_knight
                
                
                    Commented
                    Apr 15, 2019 at 12:16
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                @s.k By using CSVLogger the history file is saved in .csv format by default. If I understand your purpose with loading it that way is to continue training you model. Then, you set argument append=True as in keras.callbacks.callbacks.CSVLogger(filename, separator=',', append=True) while at the same time specifying the initial_epoch argument to the epoch you want to continue training on when fitting the model onto the data again. See the documentation for callbacks as well as for fitting sequential models.
                
                
– Jakob
                
                
                    Commented
                    Nov 15, 2019 at 8:57
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @Jakob Is there a way to do this without pandas e.g. with csv built-in or numpy?
                
                
– jtlz2
                
                
                    Commented
                    Jul 23, 2021 at 12:47
                
            
        
    
    
        
            
            
        
        
            
                
                @Jakob How does one map the dataframe structure to the history object's structure?
                
                
– jtlz2
                
                
                    Commented
                    Jul 23, 2021 at 12:55
                
            
        
    
    
        
            
            
        
        
            
                
                @jtlz2 Referring to loading the training.log file with csv? You should be able to find something in the python documentation. There's a useful example at the top. As for the other question - I believe it warrants opening a new SO question.
                
                
– Jakob
                
                
                    Commented
                    Oct 14, 2021 at 9:11
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/48140989,keras.layers.LSTM,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><tensorflow><deep-learning><keras><lstm>,8,43107.83056,43107.89097,20,40978,712995,824,6659095,2.726763298,0.5,https://stackoverflow.com/questions/48491737,"I was trying to train a LSTM model using keras but I think I got something wrong here.

I got an error of 


  ValueError: Error when checking input: expected lstm_17_input to have
  3 dimensions, but got array with shape (10000, 0, 20)


while my code looks like 

model = Sequential()
model.add(LSTM(256, activation=""relu"", dropout=0.25, recurrent_dropout=0.25, input_shape=(None, 20, 64)))
model.add(Dense(1, activation=""sigmoid""))
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.fit(X_train, y_train,
      batch_size=batch_size,
      epochs=10)


where X_train has a shape of (10000, 20) and the first few data points are like 

array([[ 0,  0,  0, ..., 40, 40,  9],
   [ 0,  0,  0, ..., 33, 20, 51],
   [ 0,  0,  0, ..., 54, 54, 50],
...


and y_train has a shape of (10000, ), which is a binary (0/1) label array.

Could someone point out where I was wrong here?","For the sake of completeness, here's what's happened.

First up, LSTM, like all layers in Keras, accepts two arguments: input_shape and batch_input_shape. The difference is in convention that input_shape does not contain the batch size, while batch_input_shape is the full input shape including the batch size.

Hence, the specification input_shape=(None, 20, 64) tells keras to expect a 4-dimensional input, which is not what you want. The correct would have been just (20,).

But that's not all. LSTM layer is a recurrent layer, hence it expects a 3-dimensional input (batch_size, timesteps, input_dim). That's why the correct specification is input_shape=(20, 1) or batch_input_shape=(10000, 20, 1). Plus, your training array should also be reshaped to denote that it has 20 time steps and 1 input feature per each step.

Hence, the solution:

X_train = np.expand_dims(X_train, 2)  # makes it (10000,20,1)
...
model = Sequential()
model.add(LSTM(..., input_shape=(20, 1)))",10/14/2016 15:17,10/16/2016 4:38,"you say input_shape=(None, 20, 64) but your input has only two dimensions (10000, 20) ?
                
                
– Andre Holzner
                
                
                    Commented
                    Jan 7, 2018 at 20:20
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @AndreHolzner I once tried to set input_shape = (10000, 20), but no luck. The error is ValueError: Error when checking input: expected lstm_28_input to have 3 dimensions, but got array with shape (10000, 20)
                
                
– Mr.cysl
                
                
                    Commented
                    Jan 7, 2018 at 20:23
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                try setting input_shape=(None, 20, 1) and resize the input to size (10000,20,1). E.g. if your input is in a numpy array use numpy.resize.
                
                
– Andre Holzner
                
                
                    Commented
                    Jan 7, 2018 at 20:28
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                @AndreHolzner Just tried. Got a training np.array with a size (10000, 20, 1). However, there is still an error saying the dimension is not matched. I then changed the input size to (20, 1) and it works now!
                
                
– Mr.cysl
                
                
                    Commented
                    Jan 7, 2018 at 20:33
                
            
        
    

            
	    

        
                    Add a comment
                 |","16
            
        
        
            
                
                dear sir, I send you a lot of love.
                
                
– Alex Parakhnevich
                
                
                    Commented
                    Feb 13, 2018 at 18:52
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/48373845,keras.models.load_model,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Load,Static,0,<python><keras>,15,43122.09375,43122.10833,19,2026,3656094,153,9249320,1.035714286,0,https://stackoverflow.com/questions/48699954,"In Keras, if you need to have a custom loss with additional parameters, we can use it like mentioned on https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras

def penalized_loss(noise):
    def loss(y_true, y_pred):
        return K.mean(K.square(y_pred - y_true) - K.square(y_true - noise), axis=-1)
    return loss


The above method works when I am training the model. However, once the model is trained I am having difficulty in loading the model. When I try to use the custom_objects parameter in load_model like below

model = load_model(modelFile, custom_objects={'penalized_loss': penalized_loss} )


it complains ValueError: Unknown loss function:loss

Is there any way to pass in the loss function as one of the custom losses in custom_objects ? From what I can gather, the inner function is not in the namespace during load_model call. Is there any easier way to load the model or use a custom loss with additional parameters","Yes, there is! custom_objects expects the exact function that you used as loss function (the inner one in your case):

model = load_model(modelFile, custom_objects={ 'loss': penalized_loss(noise) })


Unfortunately keras won't store in the model the value of noise, so you need to feed it to the load_model function manually.",10/26/2016 22:26,3/16/2017 8:16,"Add a comment
                 |","1
            
        
        
            
                
                Do you mean that I will have to create a random variable noise when I am loading the trained model ? noise in my training keeps on changing hence affecting the loss value
                
                
– Jason
                
                
                    Commented
                    Jan 22, 2018 at 2:44
                
            
        
    
    
        
            
            
        
        
            
                
                I don't know what the variable noise is in your case, but you need to pass something to your penalized_loss function in order to return the loss function. If it's a random variable, you probably need to create a new one (or use an existing one) and passing that to your function, as you said.
                
                
– rickyalbert
                
                
                    Commented
                    Jan 22, 2018 at 2:47
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                If you are going to use the loaded model for testing, whichever noise variable you are going to pass it won't matter. You just want to be able to load your model :)
                
                
– rickyalbert
                
                
                    Commented
                    Jan 22, 2018 at 2:48
                
            
        
    
    
        
            
            
        
        
            
                
                No problem! Please mark the answer as useful if you feel like it was :)
                
                
– rickyalbert
                
                
                    Commented
                    Jan 22, 2018 at 2:52
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Because I ran this issue: it should be emphasized that the name of your inner function has to match the key in custom_objects. To avoid mistakes I do loss=penalized_loss(noise); custom_objects={loss.__name__: loss}
                
                
– F Lekschas
                
                
                    Commented
                    Mar 6, 2019 at 16:58
                
            
        
    

            
	    

        
                    
                 | 
            Show 3 more comments"
https://stackoverflow.com/questions/48493755,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Train,Static,0,<python><tensorflow><machine-learning><keras><jupyter-notebook>,16,43129.12153,43129.19097,32,1640,6779007,882,7201349,32,0.5,https://stackoverflow.com/questions/48719540,"I'm running a Keras neural network model in Jupyter Notebook (Python 3.6)

I get the following error


  AttributeError: 'list' object has no attribute 'ndim'


after calling the .fit() method from Keras.model

model  = Sequential()
model.add(Dense(5, input_dim=len(X_data[0]), activation='sigmoid' ))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
model.fit(X_data, y_data, epochs=20, batch_size=10)


I checked the requirements.txt file for Keras (in Anaconda3) and the numpy, scipy, and six module versions are all up to date.

What can explain this AttributeError?

The full error message is the following (seems to be somewhat related to Numpy):


  --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
        3 model.add(Dense(1, activation = 'sigmoid'))
        4 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
  ----> 5 model.fit(X_data, y_data, epochs=20, batch_size=10)
  
  ~\Anaconda3\lib\site-packages\keras\models.py in fit(self, x, y,
  batch_size, epochs, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, initial_epoch,
  steps_per_epoch, validation_steps, **kwargs)
      963                               initial_epoch=initial_epoch,
      964                               steps_per_epoch=steps_per_epoch,
  --> 965                               validation_steps=validation_steps)
      966 
      967     def evaluate(self, x=None, y=None,
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x,
  y, batch_size, epochs, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, initial_epoch,
  steps_per_epoch, validation_steps, **kwargs)    1591

  class_weight=class_weight,    1592             check_batch_axis=False,
  -> 1593             batch_size=batch_size)    1594         # Prepare validation data.    1595         do_validation = False
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py in
  _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)    1424

  self._feed_input_shapes,    1425

  check_batch_axis=False,
  -> 1426                                     exception_prefix='input')    1427         y = _standardize_input_data(y, self._feed_output_names,

  1428                                     output_shapes,
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py in
  _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
       68     elif isinstance(data, list):
       69         data = [x.values if x.class.name == 'DataFrame' else x for x in data]
  ---> 70         data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71     else:
       72         data = data.values if data.class.name == 'DataFrame' else data
  
  ~\Anaconda3\lib\site-packages\keras\engine\training.py in
  (.0)
       68     elif isinstance(data, list):
       69         data = [x.values if x.class.name == 'DataFrame' else x for x in data]
  ---> 70         data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71     else:
       72         data = data.values if data.class.name == 'DataFrame' else data
  
  AttributeError: 'list' object has no attribute 'ndim'","model.fit expects x and y to be numpy array. Seems like you pass a list, it tried to get shape of input by reading ndim attribute of numpy array and failed. 

You can simply transform it using np.array:

import numpy as np
...
model.fit(np.array(train_X),np.array(train_Y), epochs=20, batch_size=10)",10/29/2016 15:19,10/29/2016 17:46,"Add a comment
                 |","that was the problem. Fixed it! Funny though, since on another computer Keras could run even if the data was a double matrix instead of a numpy array. Thank you for your help!
                
                
– Larry
                
                
                    Commented
                    Jan 30, 2018 at 3:08
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/48674881,keras.layers.Dense,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><machine-learning><neural-network><keras>,9,43138.96736,43139.19931,8,166,5153267,311,7561254,1.133333333,0,https://stackoverflow.com/questions/49008074,"I want to do multi label classification (20 distinct output labels), based on vectorized words using TfidfVectorizer. I have set of 39974 rows each one containing 2739 items (zeros or ones).

I would like to classify this data using Keras model which will contain 1 hidden layer (~20 nodes with activation='relu') and output layer equal 20 possible output values (with activation='softmax' to choose best fit).

Here's my code so far:

model = Sequential()
model.add(Dense(units=20, activation='relu', input_shape=tfidf_matrix.shape))
model.add(Dense(units=20, activation='softmax'))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(tfidf_matrix, train_data['cuisine_id'], epochs=10)


But got error: 


  ValueError: Error when checking input: expected dense_1_input to have
  3 dimensions, but got array with shape (39774, 2739)


How can I specify this NN to fit using this matrix?","The number of rows (number of training samples) is not the part of the input shape of the network because the training process feeds the network one sample per batch (or, more precisely, batch_size samples per batch).

So in your case, input shape of the network is (2739, ) and the right code should be like this:

model = Sequential()
# the shape of one training example is
input_shape = tfidf_matrix[0].shape
model.add(Dense(units=20, activation='relu', input_shape=input_shape))
model.add(Dense(units=20, activation='softmax'))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', 
metrics=['accuracy'])
model.fit(tfidf_matrix, train_data['cuisine_id'], epochs=10)",11/15/2016 4:19,4/15/2017 5:16,"Add a comment
                 |","1
            
        
        
            
                
                Thanks a lot for response! That input layer staff confused me. Works fine now!
                
                
– ldragicevic
                
                
                    Commented
                    Feb 8, 2018 at 21:34
                
            
        
    
    
        
            
            
        
        
            
                
                Saved me a bunch! thanks^^
                
                
– najmieh sadat safarabadi
                
                
                    Commented
                    Apr 23, 2023 at 15:51
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/48699954,keras.models.fit,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Train,Static,0,<python><tensorflow><keras><anaconda>,6,43140.26944,43140.29722,10,3726,7919597,73,6651056,1.547619048,0,https://stackoverflow.com/questions/49037211,"when triying to fit the model  i get this error

i'm using Keras and every time i try to fit my model

padded_model.fit(train_X, train_y, epochs=50, verbose=1)


i get this error :


  'int' object has no attribute 'ndim'","If train_x and train_y are normal Python lists, they don't have the attribute .ndim. Only Numpy arrays have this attribute representing the number of dimensions. 

(https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.ndarray.ndim.html)",11/20/2016 15:18,11/20/2016 16:25,"What is train_x and train_y? They need to be numpy arrays or list of numpy arrays.
                
                
– umutto
                
                
                    Commented
                    Feb 9, 2018 at 6:34
                
            
        
    
    
        
            
            
        
        
            
                
                @umutto i've got train_c and train y using train_test_split : 'train_X, test_X, train_y, 'test_y = train_test_split(padded_docs, y,                                                      train_size=0.7,                                                     test_size=0.3,                                                     random_state=123)'
                
                
– collin
                
                
                    Commented
                    Feb 9, 2018 at 6:43
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                Please include the types of train_X and train_y using the type() function in python.
                
                
– Dr. Snoopy
                
                
                    Commented
                    Feb 9, 2018 at 9:03
                
            
        
    

            
	    

        
                    Add a comment
                 |","So the Problem is that the function fit cannot treate lists ?
                
                
– collin
                
                
                    Commented
                    Feb 9, 2018 at 8:06
                
            
        
    
    
        
            
            
        
        
            
                
                Basically yes, but it is also not recommended to use lists. Numpy is a special package to speed up numerical calculations. Using pure Python lists is much slower and not recommended for this purpose. But it is easy to turn a list into a Numpy array if the module is installed. Just pass the list to the array constructor: x = np.array([1.0, 2.0, 3.0])
                
                
– Joe
                
                
                    Commented
                    Feb 9, 2018 at 8:25
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/48760472,keras.layers.LSTM,SAM,BET,IC-1,IC-1,Unacceptable Input Value,IF,Keras,Model Construction,Static,0,<python><keras>,12,43144.25625,43146.53681,14,56010,2097240,5275,4948889,1.192391706,0,https://stackoverflow.com/questions/49197916,"Here is my code fore training the complete model and saving it:   

num_units = 2
activation_function = 'sigmoid'
optimizer = 'adam'
loss_function = 'mean_squared_error'
batch_size = 10
num_epochs = 100

# Initialize the RNN
regressor = Sequential()

# Adding the input layer and the LSTM layer
regressor.add(LSTM(units = num_units, activation = activation_function, input_shape=(None, 1)))

# Adding the output layer
regressor.add(Dense(units = 1))

# Compiling the RNN
regressor.compile(optimizer = optimizer, loss = loss_function)

# Using the training set to train the model
regressor.fit(x_train, y_train, batch_size = batch_size, epochs = num_epochs)
regressor.save('model.h5')


After that I have seen that most of the time people our suggesting the test dataset for checking the prediction which I have attempted as well and got good result.     

But the problem is in the usage of the model that I have created. I want to have a forecast for next 30 days or every minute whatsoever. Now I have the trained model but I am not getting what I can do or what code do I use to use the model and forecast the prices for next 30 days or one minute.    

Please suggest me the way out. I am stuck at this problem since a week and not able to make any successful attempts.   

Here is the link of the repository where one can find the complete runnable code, the model, and the dataset:  My repository link","Well, you need a stateful=True model, so you can feed it one prediction after another to get the next and keep the model thinking that each input is not a new sequence, but a sequel to the previous. 

Fixing the code and training

I see in the code that there is an attempt to make your y be a shifte x (a good option for predicting the next steps). But there is also a big problem in the preprocessing here:

training_set = df_train.values
training_set = min_max_scaler.fit_transform(training_set)

x_train = training_set[0:len(training_set)-1]
y_train = training_set[1:len(training_set)]
x_train = np.reshape(x_train, (len(x_train), 1, 1))


Data for LSTM layers must be shaped as (number_of_sequences, number_of_steps,features).     

So, you're clearly creating sequences of 1 step only, meaning that your LSTM is not learning sequences at all.  (There is no sequence with only one step).     

Assuming that your data is a single unique sequence with 1 feature, it should definitely be shaped as (1, len(x_train), 1).

Naturally, y_train should also have the same shape.  

This, in its turn, will require that your LSTM layers be return_sequences=True - The only way to make y have a length in steps. Also, for having a good prediction, you may need a more complex model (because now it will be trully learning).    

This done, you train your model until you get a satisfactory result.



Predicting the future

For predicting the future, you will need stateful=True LSTM layers. 

Before anything, you reset the model's states: model.reset_states() - Necessary every time you're inputting a new sequence into a stateful model.

Then, first you predict the entire X_train (this is needed for the model to understand at which point of the sequence it is, in technical words: to create a state). 

predictions = model.predict(`X_train`) #this creates states


And finally you create a loop where you start with the last step of the previous prediction:

future = []
currentStep = predictions[:,-1:,:] #last step from the previous prediction

for i in range(future_pred_count):
    currentStep = model.predict(currentStep) #get the next step
    future.append(currentStep) #store the future steps    

#after processing a sequence, reset the states for safety
model.reset_states()




Example

This code does this with a 2-feature sequence, a shifted future step prediction, and a method that is a little different from this answer, but based on the same principle. 

I created two models (one stateful=False, for training without needing to reset states every time - never forget to reset states when you're starting a new sequence - and the other stateful=True, copying the weights from the trained model, for predicting the future)

https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb",11/24/2016 11:13,1/17/2017 22:24,"Is there someone who can help me with the issue? Please let me know.
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 13, 2018 at 7:35
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                I have doubt  regarding the dataset. You provides prices as input which explain how accurate you model is. There is more complexe models not able to predict evolution with such a good accuracy. I guess the second input is the date (not visible in the head (block 3) ?. I'll take a look when I have time but you can take a look at this video (youtube.com/watch?v=EqWm8A-dRYg). He is predicting BTC price with bidirectionnal LSTM and social networks sentiments. To finish, you can predict the days after (as you have a daily timestep). If you need mode days, you can create a loop.
                
                
– Nicolas M.
                
                
                    Commented
                    Feb 13, 2018 at 9:01
                
            
        
    
    
        
            
            
        
        
            
                
                @NicolasM.Sure.. thank you for your time and consideration. But have a look at the code so I can improve. Thanks.
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 13, 2018 at 9:12
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                Why downvote? I do not understand, why people not having any answer start downvoting the question or sometimes try to close it.
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 13, 2018 at 9:33
                
            
        
    
    
        
            
            
        
        
            
                
                I've slightly modified the dataframe compare to what you've done. Nevertheless, I've added a shifted column to predict the next price but the prediction is still to accurate. I don't know exactly what is wrong but I guess this is due to the fact that we have 1 batch of around 700 timesteps of 1 input and the model is able to understand that there is only a shift. Everytime I tried RNN, I used multiple batches compare to this exercice so I'm also stuck :s. I'll continue to take a look
                
                
– Nicolas M.
                
                
                    Commented
                    Feb 13, 2018 at 11:20
                
            
        
    

            
	    

        
                    
                 | 
            Show 3 more comments","1
            
        
        
            
                
                This is awesome.... Daniel. This is great atleast you have try to help me,..... I will try your suggestion and if it works, I will accept your answer too.
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 16, 2018 at 5:03
                
            
        
    
    
        
            
            
        
        
            
                
                Dear Daniel, is it possible for your make changes in my repository code here: github.com/JafferWilson/forecastbtc I guess that will help me a lot and make understanding.. I am trying to implement the way you have. But can try it out please.
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 16, 2018 at 5:08
                
            
        
    
    
        
            
            
        
        
            
                
                daniel had you checked my code... because afyer adding your suggestions I am getting lots of error...
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 16, 2018 at 9:20
                
            
        
    
    
        
            
            
        
        
            
                
                regressor.add(LSTM(units = num_units, activation = activation_function, input_shape=(None, 1),                     stateful=True,return_sequences=True, batch_input_shape=(1, len(x_train), 1))) I have made the code something like this with staeful=true
                
                
– Jaffer Wilson
                
                
                    Commented
                    Feb 16, 2018 at 11:26
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Basic shape mismatch error. LSTM layers expect 3D data, you're giving it a 2D data.
                
                
– Daniel Möller
                
                
                    Commented
                    Nov 8, 2019 at 11:49
                
            
        
    

            
	    

        
                    
                 | 
            Show 8 more comments"
https://stackoverflow.com/questions/49195189,"keras.models.add,keras.models.save",Hybrid,SAI,"F,IC-1",SAM (Level 3) ? AMO(Level 2),Missing Input value-Method order Dependency,IF,Keras,Load,Static,0,<python><tensorflow><raspberry-pi><keras>,6,43168.57361,43168.72917,13,678,9418446,664,3574571,1.222222222,0.5,https://stackoverflow.com/questions/49853303,"I have trained a keras sequential model in a linux 64 machine and saved to a .h5 file.

It this PC I can load the model and do predictions without problems.

Now I'm implementing the prediction in a Raspberry Pi 3 that have installed keras, tensorflow, h5py and python3.

when I load the model

from keras.models import load_model
model = load_model('model-0.6358.h5')


, I'm getting: 

usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
return f(*args, **kwds)

/usr/local/lib/python3.4/dist-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.
warnings.warn('Error in loading the saved optimizer '


But... it looks like it predicts right.

How can I avoid that warning message?","load_model first builds the saved model architecture with its saved weights and then tries to build the saved optimizer with its saved weights. 

However, you get an error message because there is a mismatch between the shape of the saved optimizer weights and the shape of the weights that the optimizer is expecting based on the architecture of the loaded model. 

I ran into this issue using Keras 2.1.4 when I tried to save and re-load a model that had inner submodels that were set to trainable=False. This information seems not to be preserved when you save the model, so after re-instatiating the inner submodel is set to trainable=True and the optimizer would expect more saved weights than were actually saved. If this might be the problem in your case, I described a workaround in this bug-report:


Set the trainability of all the inner model layers explicitly
Right before saving, the trainability flags of all the layers have to be set to the state that they had at model compile time


If you want to get rid of the warning and you do not need the optimizer after saving anyway, you can also save your model without the optimizer: use model.save(filename, include_optimizer=False)",12/20/2016 1:10,12/20/2016 11:03,"3
            
        
        
            
                
                Yes... it will predict right. The ""optimizer"" is only necessary for ""training"", and it's state will only help select the proper learning rate and weight updates. Even with a fresh new optimizer, it will be possible to train if you manually adjust its properties to reasonable amounts. A fresh new optimizer may have a hard time at the beginning of the training, but after a few epochs it will very probably find its way.
                
                
– Daniel Möller
                
                
                    Commented
                    Mar 9, 2018 at 13:52
                
            
        
    
    
        
            
            
        
        
            
                
                @DanielMöller that make sense, I mistrust when I get one warning message only in one platform, any way I will not train in the RPI. But do you know why I'm getting that message?
                
                
– Mquinteiro
                
                
                    Commented
                    Mar 9, 2018 at 14:14
                
            
        
    
    
        
            
            
        
        
            
                
                Unfortunately I don't know.... it's probably some kind of incompatibility. In my case it's worse, because I was never able to load a model this way even on the same machine.
                
                
– Daniel Möller
                
                
                    Commented
                    Mar 9, 2018 at 14:25
                
            
        
    
    
        
            
            
        
        
            
                
                @DanielMöller h5py problem? Keras problem? Did you try to load the file with f = h5py.File(""name.h5"",'r') for  name in f: print name?
                
                
– Mquinteiro
                
                
                    Commented
                    Mar 9, 2018 at 14:30
                
            
        
    
    
        
            
            
        
        
            
                
                For me it worked when I changed the version of keras to the same which I used to train the model.
                
                
– skt7
                
                
                    Commented
                    May 10, 2019 at 9:30
                
            
        
    

            
	    

        
                    Add a comment
                 |","Hi @KiraMichiru, ""A workaround is to explicitly set all the layers of the submodel to trainable=False, but I still consider this a bug."" I don't understand. When? At which point in the code we have to set the trainability of the layers for it to load correctly? model = tf.keras.models.load_model(path) is a single call, and inside it is where the attempted restoration of the optimizer happens and fails...
                
                
– WurmD
                
                
                    Commented
                    Jul 20, 2022 at 16:06
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/49785536,keras.backend,SAM,BET,IC-1,IC-1,Unacceptable Input Value,0,Keras,0,Static,0,<python><machine-learning><neural-network><keras>,15,43201.95556,43201.95833,26,10065,8112138,1521,3657151,2.207058824,0,https://stackoverflow.com/questions/50825936,"I cannot seem to get the value of learning rate. What I get is below. 

I've tried the model for 200 epochs and want to see/change the learning rate. Is this not the correct way?

>>> print(ig_cnn_model.optimizer.lr)
<tf.Variable 'lr_6:0' shape=() dtype=float32_ref>","Use eval() from keras.backend:

import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(1, input_shape=(1,)))
model.add(Dense(1))
model.compile(loss='mse', optimizer='adam')

print(K.eval(model.optimizer.lr))


Output:

0.001",12/28/2016 20:12,01-04-2017 14:36,"See this.
                
                
– Autonomous
                
                
                    Commented
                    Apr 11, 2018 at 22:59
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                @ParagS.Chandakkar Already saw that before I posted here. For them it returns a values, AFIK.
                
                
– user14492
                
                
                    Commented
                    Apr 11, 2018 at 23:03
                
            
        
    

            
	    

        
                    Add a comment
                 |","4
            
        
        
            
                
                What if I want to reset the learning rate?  e.g. something like model.optimizer.lr=10?
                
                
– Zach
                
                
                    Commented
                    Sep 18, 2018 at 21:32
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                Or model.optimizer.lr.numpy() in recent tensorflow versions. lr is just a variable, so assigning it works as usual: model.optimizer.lr.assign(0.1)
                
                
– hoefling
                
                
                    Commented
                    Nov 26, 2020 at 14:04
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/50304156,keras.preprocessing.image.ImageDataGenerator.data_generator.flow_from_directory,SAM,BET,IC-1,IC-1,Unacceptable Input Value,MOB,Keras,0,Static,0,<python><tensorflow><memory><keras-layer><resnet>,19,43232.33958,43297.74653,11,126,9804206,746,9044016,11,2,https://stackoverflow.com/questions/50920908,"Using ResNet50 pre-trained Weights I am trying to build a classifier. The code base is fully implemented in Keras high-level Tensorflow API. The complete code is posted in the below GitHub Link.
Source Code: Classification Using RestNet50 Architecture
The file size of the pre-trained model is 94.7mb.
I loaded the pre-trained file
new_model = Sequential()

new_model.add(ResNet50(include_top=False,
                pooling='avg',
                weights=resnet_weight_paths))

and fit the model
train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 12,
    class_mode = 'categorical'
    )

validation_generator = data_generator.flow_from_directory(
    'path_to_the_validation_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    class_mode = 'categorical'
    )

#compile the model

new_model.fit_generator(
    train_generator,
    steps_per_epoch = 3,
    validation_data = validation_generator,
    validation_steps = 1
)

and in the Training dataset, I have two folders dog and cat, each holder almost 10,000 images. When  I compiled the script, I get the following error

Epoch 1/1 2018-05-12 13:04:45.847298: W
tensorflow/core/framework/allocator.cc:101] Allocation of 38535168
exceeds 10% of system memory. 2018-05-12 13:04:46.845021: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:47.552176: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.199240: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.918930: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:49.274137: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:49.647061: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.028839: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.413735: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory.

Any ideas to optimize the way to load the pre-trained model (or) get rid of this warning message?
Thanks!","Try reducing batch_size attribute to a small number(like 1,2 or 3).
Example:

train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 2,
    class_mode = 'categorical'
    )",12/30/2016 16:30,12/30/2016 16:37,"1
            
        
        
            
                
                To clarify, does the model run after these messages?
                
                
– Allen Lavoie
                
                
                    Commented
                    May 15, 2018 at 16:42
                
            
        
    
    
        
            
            
        
        
            
                
                Yes it run.....
                
                
– Madhi
                
                
                    Commented
                    May 15, 2018 at 16:43
                
            
        
    
    
        
            
                    4
            
        
        
            
                
                In that case, take a look at stackoverflow.com/a/42121886/6824418 ? Unless there's some other reason you need to reduce the memory usage.
                
                
– Allen Lavoie
                
                
                    Commented
                    May 15, 2018 at 16:47
                
            
        
    

            
	    

        
                    Add a comment
                 |","How come reducing the batch_size fixes this? (Intuitively it'd mean the training/validation data takes up less memory but I don't see how!)
                
                
– Omrii
                
                
                    Commented
                    Apr 4, 2021 at 5:05
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                @Omrii It's a matter of caching. Most data loaders do not keep all of the data in memory at all times, and nearly all do not keep significant data in the GPU memory. By decreasing the batch_size, you decrease the amount of data needed at any step of the training, allowing for less to be loaded at a time and still maintain optimal throughput. So no, the data is not smaller, but yes, it does take up less RAM and video RAM. Tensorflow is being clever here to save you headache.
                
                
– Poik
                
                
                    Commented
                    Jun 9, 2021 at 0:10
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                Keep in mind that batch size is a hyper parameter which affects the gradient decent trajectory and might push you into different optima - therefore using a larger machine (in the cloud) would help
                
                
– Romeo Kienzler
                
                
                    Commented
                    Jul 11, 2022 at 10:18
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/50920908,keras.models.compile.metrics.confusion_matrix,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Evaluation,Static,0,<python><keras>,22,43270.20694,43270.25278,33,2153,4439753,1048,2725751,3.4,0,https://stackoverflow.com/questions/51632716,"This question already has answers here:
                                
                            
                    
                
            
                    
                        Multilabel-indicator is not supported for confusion matrix
                            
                                (4 answers)
                            
                    
                Closed 5 years ago.
        


    

I am building a multiclass model with Keras.

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training


Here is how my test data looks like (it's text data).

X_test
Out[25]: 
array([[621, 139, 549, ...,   0,   0,   0],
       [621, 139, 543, ...,   0,   0,   0]])

y_test
Out[26]: 
array([[0, 0, 1],
       [0, 1, 0]])


After generating predictions...

predictions = model.predict(X_test)
predictions
Out[27]: 
array([[ 0.29071924,  0.2483743 ,  0.46090645],
       [ 0.29566404,  0.45295066,  0.25138539]], dtype=float32)


I did the following to get the confusion matrix.

y_pred = (predictions > 0.5)

confusion_matrix(y_test, y_pred)
Traceback (most recent call last):

  File ""<ipython-input-38-430e012b2078>"", line 1, in <module>
    confusion_matrix(y_test, y_pred)

  File ""/Users/abrahammathew/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py"", line 252, in confusion_matrix
    raise ValueError(""%s is not supported"" % y_type)

ValueError: multilabel-indicator is not supported


However, I am getting the above error.

How can I get a confusion matrix when doing a multiclass neural network in Keras?","Your input to confusion_matrix must be an array of int not one hot encodings.

matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))",01-12-2017 19:11,1/13/2017 19:50,"Add a comment
                 |","35
            
        
        
            
                
                In case it's too subtle, this answer clarifies that the question was asked about sklearn.metrics.confusion_matrix(), not tensorflow.math.confusion_matrix(), which might be expected given the tag keras
                
                
– Jake Stevens-Haas
                
                
                    Commented
                    Jan 30, 2020 at 4:23
                
            
        
    
    
        
            
                    3
            
        
        
            
                
                How can I use it for the image dataset?
                
                
– Nur Bijoy
                
                
                    Commented
                    Mar 15, 2021 at 8:38
                
            
        
    
    
        
            
            
        
        
            
                
                @JakeStevens-Haas Thx: that is actually a material flaw in the answer: this answer should be rewritten
                
                
– WestCoastProjects
                
                
                    Commented
                    Jun 26, 2021 at 18:03
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/51421885,keras.models.predict,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Prediction,Static,0,<python><keras><shape><text-classification>,10,43300.49028,43300.93333,10,66658,1832058,175,9435410,3.25,0,https://stackoverflow.com/questions/52270177,"I am getting the following error while calling the model.predict function when running a text classification model in keras. I searched the everywhere but it isn't working for me.

ValueError: Error when checking input: expected dense_1_input to have shape (100,) but got array with shape (1,)


My data has 5 classes and has a total of 15 examples only. Below is the dataset

             query        tags
0               hi       intro
1      how are you       wellb
2            hello       intro
3        what's up       wellb
4       how's life       wellb
5              bye          gb
6    see you later          gb
7         good bye          gb
8           thanks   gratitude
9        thank you   gratitude
10  that's helpful   gratitude
11      I am great  revertfine
12            fine  revertfine
13       I am fine  revertfine
14            good  revertfine


This is the code of my model

from keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelBinarizer
from keras.models import Sequential
import pandas as pd
from keras.layers import Dense, Activation

data = pd.read_csv('text_class.csv')
train_text = data['query']
train_labels = data['tags']

tokenize = Tokenizer(num_words=100)
tokenize.fit_on_texts(train_text)

x_data = tokenize.texts_to_matrix(train_text)

encoder = LabelBinarizer()
encoder.fit(train_labels)
y_data = encoder.transform(train_labels)

model = Sequential()
model.add(Dense(512, input_shape=(100,)))
model.add(Activation('relu'))
model.add(Dense(5))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
model.fit(x_data, y_data, batch_size=8, epochs=10)

predictions = model.predict(x_data[0])
tag_labels = encoder.classes_
predicted_tags = tag_labels[np.argmax(predictions)]
print (predicted_tags)


I am not able to figure out where the problem lies and how to fix it.","x_data is 2-dimensional array with shape (15, 100)

  print(x_data.shape) 


but x_data[0] is 1-dimensional array with shape (100, )

  print(x_data[0].shape) 


and it makes problem.

Use slicing x_data[0:1] to get it as 2-dimensional array with shape (1, 100) 

 print(x_data[0:1].shape) 


and it will work

 predictions = model.predict(x_data[0:1])",1/29/2017 7:58,1/31/2017 1:02,"1
            
        
        
            
                
                What is the shape of x_data?
                
                
– lordingtar
                
                
                    Commented
                    Jul 19, 2018 at 12:00
                
            
        
    
    
        
            
            
        
        
            
                
                @lordingtar 15,100
                
                
– Bhavesh Laddagiri
                
                
                    Commented
                    Jul 19, 2018 at 13:34
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                completely unrelated question. How did you convert his text data into csv or dataframe?
                
                
– Bal Krishna Jha
                
                
                    Commented
                    Jul 20, 2018 at 9:59
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                @krishna I used pandas.read_table(filename, sep=""\s{2,}"") - and even io.StringIO(text) to create file in memory, not on disk.
                
                
– furas
                
                
                    Commented
                    Jul 20, 2018 at 12:46
                
            
        
    
    
        
            
            
        
        
            
                
                Thanks a lot ... ??
                
                
– Bhavesh Laddagiri
                
                
                    Commented
                    Jul 20, 2018 at 15:53
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/51632716," keras.layers.Concatenate,tf.keras.backend.concatenate",AMO,F,F,F,Missing Required State-specific Method Order,Crash,Keras,Model Construction,Static,0,<python><keras><keras-layer>,11,43313.49861,43313.50833,11,56010,2097240,255,9819585,1.192391706,0,https://stackoverflow.com/questions/52282108,"I just recently started playing around with Keras and got into making custom layers. However, I am rather confused by the many different types of layers with slightly different names but with the same functionality. 

For example, there are 3 different forms of the concatenate function from https://keras.io/layers/merge/ and https://www.tensorflow.org/api_docs/python/tf/keras/backend/concatenate

keras.layers.Concatenate(axis=-1)
keras.layers.concatenate(inputs, axis=-1)
tf.keras.backend.concatenate()


I know the 2nd one is used for functional API but what is the difference between the 3? The documentation seems a bit unclear on this.

Also, for the 3rd one, I have seen a code that does this below. Why must there be the line ._keras_shape after the concatenation?

# Concatenate the summed atom and bond features
atoms_bonds_features = K.concatenate([atoms, summed_bond_features], axis=-1)

# Compute fingerprint
atoms_bonds_features._keras_shape = (None, max_atoms, num_atom_features + num_bond_features)


Lastly, under keras.layers, there always seems to be 2 duplicates. For example, Add() and add(), and so on.","First, the backend: tf.keras.backend.concatenate()
Backend functions are supposed to be used ""inside"" layers. You'd only use this in Lambda layers, custom layers, custom loss functions, custom metrics, etc.
It works directly on ""tensors"".
It's not the choice if you're not going deep on customizing. (And it was a bad choice in your example code -- See details at the end).
If you dive deep into keras code, you will notice that the Concatenate layer uses this function internally:
import keras.backend as K
class Concatenate(_Merge):  
    #blablabla   
    def _merge_function(self, inputs):
        return K.concatenate(inputs, axis=self.axis)
    #blablabla


Then, the Layer: keras.layers.Concatenate(axis=-1)
As any other keras layers, you instantiate and call it on tensors.
Pretty straighforward:
#in a functional API model:
inputTensor1 = Input(shape) #or some tensor coming out of any other layer   
inputTensor2 = Input(shape2) #or some tensor coming out of any other layer

#first parentheses are creating an instance of the layer
#second parentheses are ""calling"" the layer on the input tensors
outputTensor = keras.layers.Concatenate(axis=someAxis)([inputTensor1, inputTensor2])

This is not suited for sequential models, unless the previous layer outputs a list (this is possible but not common).

Finally, the concatenate function from the layers module: keras.layers.concatenate(inputs, axis=-1)
This is not a layer. This is a function that will return the tensor produced by an internal Concatenate layer.
The code is simple:
def concatenate(inputs, axis=-1, **kwargs):
   #blablabla
   return Concatenate(axis=axis, **kwargs)(inputs)


Older functions
In Keras 1, people had functions that were meant to receive ""layers"" as input and return an output ""layer"". Their names were related to the merge word.
But since Keras 2 doesn't mention or document these, I'd probably avoid using them, and if old code is found, I'd probably update it to a proper Keras 2 code.

Why the _keras_shape word?
This backend function was not supposed to be used in high level codes. The coder should have used a Concatenate layer.
atoms_bonds_features = Concatenate(axis=-1)([atoms, summed_bond_features])   
#just this line is perfect

Keras layers add the _keras_shape property to all their output tensors, and Keras uses this property for infering the shapes of the entire model.
If you use any backend function ""outside"" a layer or loss/metric, your output tensor will lack this property and an error will appear telling _keras_shape doesn't exist.
The coder is creating a bad workaround by adding the property manually, when it should have been added by a proper keras layer. (This may work now, but in case of keras updates this code will break while proper codes will remain ok)",1/29/2017 14:04,03-05-2017 02:13,"Add a comment
                 |","Wow, thank you for the detailed explanation! If the lowercase concatenation is not a layer and is simply a function that calls on uppercase Concatenation, then why even have the lowercase concatenation option?
                
                
– Lim Kaizhuo
                
                
                    Commented
                    Aug 1, 2018 at 12:33
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Good question.... I think it's for historical reasons, maybe to create a better transition for those using Keras 1? Or maybe because people that only use Sequential models are not used to the ""instantiate and call"" standard?
                
                
– Daniel Möller
                
                
                    Commented
                    Aug 1, 2018 at 12:36
                
                        
                            
                        
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/51731207,0,0,0,0,0,0,0,Keras,0,0,General Clarification,<python><tensorflow><neural-network><keras>,12,43319.67361,43319.68472,15,1745,7699859,523,6875778,1.8,0,https://stackoverflow.com/questions/52352522,"I have been practicing building and comparing neural networks using Keras and Tensorflow in python, but when I look to plot the models for comparisons I am receiving an error:

TypeError: 'History' object is not subscriptable


Here is my code for the three models:

############################## Initiate model 1 ###############################
# Model 1 has no hidden layers
from keras.models import Sequential
model1 = Sequential()

# Get layers
from keras.layers import Dense
# Add first layer
n_cols = len(X.columns)
model1.add(Dense(units=n_cols, activation='relu', input_shape=(n_cols,)))
# Add output layer
model1.add(Dense(units=2, activation='softmax'))

# Compile the model
model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics= 
['accuracy']) 

# Define early_stopping_monitor
from keras.callbacks import EarlyStopping
early_stopping_monitor = EarlyStopping(patience=2)

# Fit model
model1.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)


############################## Initiate model 2 ###############################
# Model 2 has 1 hidden layer that has the mean number of nodes of input and output layer
model2 = Sequential()

# Add first layer
model2.add(Dense(units=n_cols, activation='relu', input_shape=(n_cols,)))
# Add hidden layer
import math
model2.add(Dense(units=math.ceil((n_cols+2)/2), activation='relu'))
# Add output layer
model2.add(Dense(units=2, activation='softmax'))

# Compile the model
model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics= 
['accuracy']) 

# Fit model
model2.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)

############################## Initiate model 3 ###############################
# Model 3 has 1 hidden layer that is 2/3 the size of the input layer plus the size of the output layer
model3 = Sequential()

# Add first layer
model3.add(Dense(units=n_cols, activation='relu', input_shape=(n_cols,)))
# Add hidden layer
model3.add(Dense(units=math.ceil((n_cols*(2/3))+2), activation='relu'))
# Add output layer
model3.add(Dense(units=2, activation='softmax'))

# Compile the model
model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics= 
['accuracy']) 

# Fit model
model3.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)


# Plot the models
plt.plot(model1.history['val_loss'], 'r', model2.history['val_loss'], 'b', 
model3.history['val_loss'], 'g')
plt.xlabel('Epochs')
plt.ylabel('Validation score')
plt.show()


I have no problems with running any of my models, getting predicted probabilities, plotting ROC curves, or plotting PR curves. However, when I attempt to plot the three curves together I am getting an error from this area of my code:

model1.history['val_loss']

TypeError: 'History' object is not subscriptable


Does anyone have experience with this type of error and can lead me to what I am doing wrong?

Thank you in advance.","Call to model.fit() returns a History object that has a member history, which is of type dict. 

So you can replace :

model2.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)


with 

history2 = model2.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)


Similarly for other models.

and then you can use :

plt.plot(history1.history['val_loss'], 'r', history2.history['val_loss'], 'b', 
history3.history['val_loss'], 'g')",1/29/2017 16:58,1/31/2017 20:32,"9
            
        
        
            
                
                try model1.history.history['val_loss'] and similarly for others.
                
                
– Bal Krishna Jha
                
                
                    Commented
                    Aug 7, 2018 at 16:18
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @krishna that worked great. Thank you
                
                
– Aaron England
                
                
                    Commented
                    Aug 7, 2018 at 17:38
                
            
        
    

            
	    

        
                    Add a comment
                 |","1
            
        
        
            
                
                Worked like a charm. Thanks for the insight!
                
                
– Aaron England
                
                
                    Commented
                    Aug 7, 2018 at 17:38
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/51806852," keras.models.save_weights,keras.models.save",AMO,F,F,F,Missing Required State-specific Method Order,Crash,Keras,Model Construction,Static,0,<python-3.x><tensorflow><keras>,13,43324.32847,43551.87361,19,1022,1123955,1210,3049753,8.75,-0.166666667,https://stackoverflow.com/questions/52553593,"Inspired by tf.keras.Model subclassing I created custom model.

I can train it and get successfull results, but I can't save it.

I use python3.6 with tensorflow v1.10 (or v1.9)  

Minimal complete code example here:

import tensorflow as tf
from tensorflow.keras.datasets import mnist


class Classifier(tf.keras.Model):
    def __init__(self):
        super().__init__(name=""custom_model"")

        self.batch_norm1 = tf.layers.BatchNormalization()
        self.conv1 = tf.layers.Conv2D(32, (7, 7))
        self.pool1 = tf.layers.MaxPooling2D((2, 2), (2, 2))

        self.batch_norm2 = tf.layers.BatchNormalization()
        self.conv2 = tf.layers.Conv2D(64, (5, 5))
        self.pool2 = tf.layers.MaxPooling2D((2, 2), (2, 2))

    def call(self, inputs, training=None, mask=None):
        x = self.batch_norm1(inputs)
        x = self.conv1(x)
        x = tf.nn.relu(x)
        x = self.pool1(x)

        x = self.batch_norm2(x)
        x = self.conv2(x)
        x = tf.nn.relu(x)
        x = self.pool2(x)

        return x


if __name__ == '__main__':
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape(*x_train.shape, 1)[:1000]
    y_train = y_train.reshape(*y_train.shape, 1)[:1000]

    x_test = x_test.reshape(*x_test.shape, 1)
    y_test = y_test.reshape(*y_test.shape, 1)

    y_train = tf.keras.utils.to_categorical(y_train)
    y_test = tf.keras.utils.to_categorical(y_test)

    model = Classifier()

    inputs = tf.keras.Input((28, 28, 1))

    x = model(inputs)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(10, activation=""sigmoid"")(x)

    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(optimizer=""adam"", loss=""binary_crossentropy"", metrics=[""accuracy""])
    model.fit(x_train, y_train, epochs=1, shuffle=True)

    model.save(""./my_model"")


Error message:  

1000/1000 [==============================] - 1s 1ms/step - loss: 4.6037 - acc: 0.7025
Traceback (most recent call last):
  File ""/home/user/Data/test/python/mnist/mnist_run.py"", line 62, in <module>
    model.save(""./my_model"")
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1278, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 101, in save_model
    'config': model.get_config()
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1049, in get_config
    layer_config = layer.get_config()
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1028, in get_config
    raise NotImplementedError
NotImplementedError

Process finished with exit code 1


I looked into the error line and found out that get_config method checks self._is_graph_network

Do anybody deal with this problem?

Thanks!

Update 1:

On the keras 2.2.2 (not tf.keras)

Found comment (for model saving)

file: keras/engine/network.py

Function: get_config  


  # Subclassed networks are not serializable

  # (unless serialization is implemented by

  # the author of the subclassed network).  


So, obviously it won't work...

I wonder, why don't they point it out in the documentation (Like: ""Use subclassing without ability to save!"")

Update 2:

Found in keras documentation:  


  In subclassed models, the model's topology is defined as Python code

  (rather than as a static graph of layers). That means the model's

  topology cannot be inspected or serialized. As a result, the following

  methods and attributes are not available for subclassed models:  
  
  model.inputs        and model.outputs.

  model.to_yaml()     and model.to_json()

  model.get_config()  and model.save().  


So, there is no way to save model by using subclassing.

It's possible to only use Model.save_weights()","TensorFlow 2.2

Thanks for @cal for noticing me that the new TensorFlow has supported saving the custom models!


  By using model.save to save the whole model and by using load_model to restore previously stored subclassed model. The following code snippets describe how to implement them.


class ThreeLayerMLP(keras.Model):

  def __init__(self, name=None):
    super(ThreeLayerMLP, self).__init__(name=name)
    self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')
    self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')
    self.pred_layer = layers.Dense(10, name='predictions')

  def call(self, inputs):
    x = self.dense_1(inputs)
    x = self.dense_2(x)
    return self.pred_layer(x)

def get_model():
  return ThreeLayerMLP(name='3_layer_mlp')

model = get_model()
# Save the model
model.save('path_to_my_model',save_format='tf')

# Recreate the exact same model purely from the file
new_model = keras.models.load_model('path_to_my_model')


See: Save and serialize models with Keras - Part II: Saving and Loading of Subclassed Models

TensorFlow 2.0

TL;DR:


do not use model.save() for custom subclass keras model;
use save_weights() and load_weights() instead.




With the help of the Tensorflow Team, it turns out the best practice of saving a Custom Sub-Class Keras Model is to save its weights and load it back when needed.

The reason that we can not simply save a Keras custom subclass model is that it contains custom codes, which can not be serialized safely. However, the weights can be saved/loaded when we have the same model structure and custom codes without any problem.

There has a great tutorial written by Francois Chollet who is the author of Keras, for how to save/load Sequential/Functional/Keras/Custom Sub-Class Models in Tensorflow 2.0 in Colab at here. In Saving Subclassed Models section, it said that:


  Sequential models and Functional models are datastructures that represent a DAG of layers. As such, they can be safely serialized and deserialized.
  
  A subclassed model differs in that it's not a datastructure, it's a
  piece of code. The architecture of the model is defined via the body
  of the call method. This means that the architecture of the model
  cannot be safely serialized. To load a model, you'll need to have
  access to the code that created it (the code of the model subclass).
  Alternatively, you could be serializing this code as bytecode (e.g.
  via pickling), but that's unsafe and generally not portable.",1/30/2017 14:08,1/30/2017 17:56,"2
            
        
        
            
                
                The reason why subclassed model can not be serializable is that keras need to trace the history of each tensor so as to determine the structure of graph, every tensor should a output of tf.keras.layers.Layer, however, subclassed model contains naive tensorflow operations like tf.nn.relu in its call method, as a result, it can not be serialized
                
                
– Jie.Zhou
                
                
                    Commented
                    Aug 12, 2018 at 8:58
                
            
        
    
    
        
            
            
        
        
            
                
                So, what if I use only tf.keras inside?   Answer: it doesn't work
                
                
– RedEyed
                
                
                    Commented
                    Aug 12, 2018 at 9:02
                
                        
                            
                        
            
        
    
    
        
            
                    4
            
        
        
            
                
                My suggestion is that, if you real want subclassed model, then forget Model.save, use Model.save_weights to save only weights of model, the load the weights with Model.load_weights, otherwise if you still want to save whole model instead of weights only, you must follow the functional api guide of keras
                
                
– Jie.Zhou
                
                
                    Commented
                    Aug 12, 2018 at 9:54
                
            
        
    
    
        
            
            
        
        
            
                
                Oh, it's really helpfull. Model.save_weights works fine. Thanks you a lot!
                
                
– RedEyed
                
                
                    Commented
                    Aug 12, 2018 at 10:02
                
            
        
    

            
	    

        
                    Add a comment
                 |","2
            
        
        
            
                
                Note that by saving weights and architecture apart from each other  you are losing compilation information (optimizer and losses) which are being saved using model.save()
                
                
– Daniel Braun
                
                
                    Commented
                    May 26, 2019 at 6:31
                
            
        
    
    
        
            
                    2
            
        
        
            
                
                It seems you can now save the full model even with custom code according to tensorflow's documentation. Your answer could be edited :)
                
                
– Cal
                
                
                    Commented
                    Mar 12, 2020 at 22:34
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Additional note for TensorFlow 2.2: You have to call model._set_inputs before this works.
                
                
– prouast
                
                
                    Commented
                    May 4, 2020 at 1:48
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                @Huan, I have saved with model.save(""NameOfModel"", save_format='tf'), but after loading the model with loaded_model = keras.models.load_model('./NameOfModel'), I get the ValueError Python inputs incompatible with input_signature:       inputs: (         Tensor(""IteratorGetNext:0"", shape=(None, 2), dtype=int32))       input_signature: (         TensorSpec(shape=(None, 2), dtype=tf.int64, name='input_1'))
                
                
– hanzgs
                
                
                    Commented
                    Jul 20, 2020 at 7:39
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                Thank you for this answer. But does it apply to RNN as well? As I cannot use save_model once I include self.cell = layers.SimpleRNNCell(10)    self.rnn = layers.RNN(self.cell, return_sequences=True, return_state=False) in my use case.
                
                
– Paco Wong
                
                
                    Commented
                    Oct 16, 2020 at 14:35
                
            
        
    

            
	    

        
                    
                 | 
            Show 1 more comment"
https://stackoverflow.com/questions/52553593,keras.models.add,SAM,BET,IC-1,IC-1,Unacceptable Input Value,Crash,Keras,Model Construction,Static,0,<python><keras>,11,43371.44514,43371.48681,12,41498,349130,131,10057087,0.820833961,0,https://stackoverflow.com/questions/54497130,"import tensorflow as tf 

mnist = tf.keras.datasets.mnist 

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))

model.compile(optimizer ='adam',
            loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])
model.fit(x_train, y_train, epochs=3)


When I tried to save the model

model.save('epic_num_reader.model')


I get a NotImplementedError:  

NotImplementedError                       Traceback (most recent call last)
<ipython-input-4-99efa4bdc06e> in <module>()
      1 
----> 2 model.save('epic_num_reader.model')

NotImplementedError: Currently `save` requires model to be a graph network. Consider using `save_weights`, in order to save the weights of the model.


So how can I save the model defined in the code?","You forgot the input_shape argument in the definition of the first layer, which makes the model undefined, and saving undefined models has not been implemented yet, which triggers the error.

model.add(tf.keras.layers.Flatten(input_shape = (my, input, shape)))


Just add the input_shape to the first layer and it should work fine.",2/15/2017 4:56,2/15/2017 9:02,"Ah I see you were also trying the tutorial at pythonprogramming.net/…. Glad to see you've already found a solution
                
                
– Anonymous Person
                
                
                    Commented
                    Jan 15, 2019 at 2:45
                
            
        
    

            
	    

        
                    Add a comment
                 |","This is a great answer.Thank you.
                
                
– Evan-Gao
                
                
                    Commented
                    Sep 29, 2018 at 9:28
                
            
        
    
    
        
            
            
        
        
            
                
                I follow the same sentdex tutorial and changed the code to model.add(tf.keras.layers.Flatten(input_shape=x_train.shape)) buI got expected flatten_1_input to have 4 dimensions, but got array with shape (60000, 28, 28) when run model.fit(). What should the code be exactly?
                
                
– hanaZ
                
                
                    Commented
                    Nov 4, 2018 at 19:40
                
                        
                            
                        
            
        
    
    
        
            
            
        
        
            
                
                @hanaZ Input shape should not contain the batch/samples dimension, and in the future remember that comments are to clarify questions and answers, not to introduce your own.
                
                
– Dr. Snoopy
                
                
                    Commented
                    Nov 4, 2018 at 20:48
                
                        
                            
                        
            
        
    
    
        
            
                    3
            
        
        
            
                
                @Matias. Thanks much for the reply. I tried with model.add(tf.keras.layers.Flatten(input_shape=x_train[0].shape)) , then it worked.
                
                
– hanaZ
                
                
                    Commented
                    Nov 13, 2018 at 23:10
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                It will work fine, but I found that the performance of my model dropped massively and asked a question about it (currently unexplained)
                
                
– Roman Starkov
                
                
                    Commented
                    Jan 6, 2019 at 22:55
                
                        
                            
                        
            
        
    

            
	    

        
                    
                 | 
            Show 2 more comments"
https://stackoverflow.com/questions/53153790,keras.layers.Flatten,SAM,DT,MT,MT,Unacceptable Input Type,Crash,Keras,Model Construction,Static,0,<python><tensorflow><keras><conv-neural-network><keras-layer>,8,43409.49028,43409.50486,14,18037,2099607,83,10607259,1.64761847,0,https://stackoverflow.com/questions/54652536,"I am fine-tuning a MobileNet with 14 new classes. When I add new layers by:

x=mobile.layers[-6].output
x=Flatten(x)
predictions = Dense(14, activation='softmax')(x)
model = Model(inputs=mobile.input, outputs=predictions)


I get the error:

'Tensor' object has no attribute 'lower'


Also using:

model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit_generator(train_batches, steps_per_epoch=18,
                validation_data=valid_batches, validation_steps=3, epochs=60, verbose=2)


I get the error:

Error when checking target: expected dense_1 to have 4 dimensions, but got array with shape (10, 14)


What does lower mean? I saw other fine-tuning scripts and there were no other arguments other than the name of the model which is x in this case.","The tensor must be passed to the layer when you are calling it, and not as an argument. Therefore it must be like this:

x = Flatten()(x)  # first the layer is constructed and then it is called on x


To make it more clear, it is equivalent to this:

flatten_layer = Flatten()  # instantiate the layer
x = flatten_layer(x)       # call it on the given tensor",2/20/2017 12:02,2/20/2017 12:08,"Add a comment
                 |","I transferred the code to jupyter notebook and found the errors. I was using vs code and the errors was too vague. thanks sir.
                
                
– Shiro Mier
                
                
                    Commented
                    Nov 5, 2018 at 12:37
                
            
        
    

            
	    

        
                    Add a comment
                 |"
https://stackoverflow.com/questions/54891464,keras.preprocessing.text.Tokenizer,AMO,G,G,G,Missing Required Method Order,IF,Keras,Train,Static,0,<python><keras><tokenize>,10,43522.74722,43522.78125,9,56010,2097240,515,9399165,1.192391706,0,https://stackoverflow.com/questions/58279628,"I have a dataframe with text columns. I separated them into x_train and x_test.

My question is if its better to do Keras's Tokenizer.fit_on_text() on the entire x data set or just x_train?

Like this:

tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_data)


or

tokenizer.fit_on_texts(x_train)        # <- fixed typo
tokenizer.texts_to_sequences(x_train)


Does it matter? I'd also have to tokenize x_test later too, so can I just use the same tokenizer?","Although the information in this question is good, indeed, there are more important things that you need to notice:

You MUST use the same tokenizer in training and test data

Otherwise, there will be different tokens for each dataset. Each tokenizer has an internal dictionary that is created with fit_on_texts.
It's not guaranteed that train and test data will have the same words with same frequencies, so each dataset will create a different dictionary, and all results from test data will be wrong.
This also means that you cannot fit_on_texts, train and then fit_on_texts again: this will change the internal dictionary.
It's possible to fit on the entire data. But it's probably a better idea to reserve a token for ""unknown"" words (oov_token=True), for the cases when you find new test data with words your model has never seen (this requires that you replace rare words in training data with this token too).
As @Fernando H metioned, it is probably be better to fit the tokenizer only with train data (even though, you must reserve an oov token even in training data (the model must learn what to do with the oov).

Testing the tokenizer with unknown words:
The following test shows that the tokenizer completely ignores unknown words when oov_token is not set. This might not be a good idea. Unknown words may be key words in sentences and simply ignoring them might be worse than knowing there is something unknown there.
import numpy as np
from keras.layers import *
from keras.models import Model
from keras.preprocessing.text import Tokenizer

training = ['hey you there', 'how are you', 'i am fine thanks', 'hello there']
test = ['he is fine', 'i am fine too']

tokenizer = Tokenizer()
tokenizer.fit_on_texts(training)

print(tokenizer.texts_to_sequences(training))
print(tokenizer.texts_to_sequences(test))

Outputs:
[[3, 1, 2], [4, 5, 1], [6, 7, 8, 9], [10, 2]]
[[8], [6, 7, 8]]

Now, this shows that the tokenizer will attibute index 1 to all unknown words:
tokenizer2 = Tokenizer(oov_token = True)
tokenizer2.fit_on_texts(training)
print(tokenizer2.texts_to_sequences(training))
print(tokenizer2.texts_to_sequences(test))

Outputs:
[[4, 2, 3], [5, 6, 2], [7, 8, 9, 10], [11, 3]]
[[1, 1, 9], [7, 8, 9, 1]]

But it might be interesting to have a group of rare words in training data replaced with 1 too, so your model has a notion of how to deal with unknown words.",03-06-2017 02:32,03-06-2017 02:57,"Add a comment
                 |","1
            
        
        
            
                
                Thank you very much for this answer! I was having nightmares with this issue, nobody that I talked seemed to see a problem. I am really glad that someone with a high score on SO has answered it. Doing fit_on_texts on the whole dataset seemed like a bit of data leakage to me. Even keras official blog did the fit on all the dataset, which made things even more confusing. Just one question, do you know any research papers addressing this problem of creating a training vocabulary using all the dataset?
                
                
– xicocaio
                
                
                    Commented
                    May 16, 2019 at 21:25
                
                        
                            
                        
            
        
    
    
        
            
                    4
            
        
        
            
                
                @xicocaio The main idea of dividing your dataset into train and test is to evaluate your model for future unkown situations in a objetive way. That's said, if you fit your tokenizer on whole dataset you are somehow biasing your model. For a good evaluation of your model, you have to take in account the UNK tokens. So as any other kind of ""feature extraction"" the best practices are to only fit on train and apply to all.
                
                
– OSainz
                
                
                    Commented
                    Jul 8, 2019 at 5:49
                
                        
                            
                        
            
        
    
    
        
            
                    1
            
        
        
            
                
                @Hiyam One tokenization only, you must keep the tokenizer for future data, the model must see the same tokens consistently.
                
                
– Daniel Möller
                
                
                    Commented
                    Nov 21, 2019 at 9:49
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                fit_on_ text, must be used only one, a single time on the entire data (so before the split). Notice the comment in the answer about rare words (it will be important). You cannot fit_on_text more than once. Later you can split the data the way you want (you can split it already tokenized or not, no problem when you call text_to_sequences)
                
                
– Daniel Möller
                
                
                    Commented
                    Nov 21, 2019 at 10:08
                
            
        
    
    
        
            
                    1
            
        
        
            
                
                Dont you guys think fitting on the entire dataset will result on 'better than real' results on the validation and test sets? I mean, there is a probability of your model finding unknown words on the prediction phase, so eliminating this chance on the evaluation stage will create results that does not reflects the real prediction performance. I think tokenizing just the training data using oov_token is a better way to describe future results performance on eval and test phases. Am I wrong?
                
                
– Fernando H'.'
                
                
                    Commented
                    Mar 28, 2020 at 12:08
                
            
        
    

            
	    

        
                    
                 | 
            Show 8 more comments"
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
